{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T15:09:23.777156Z",
     "start_time": "2019-11-21T15:09:22.568774Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import featuretools as ft\n",
    "import catboost\n",
    "from tqdm import tqdm_notebook\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy.stats import skew  # for some statistics\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "import scipy.optimize\n",
    "from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import sklearn.linear_model as linear_model\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, f1_score\n",
    "from sklearn.externals import joblib\n",
    "from category_encoders import CatBoostEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料讀取與處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T15:09:57.756831Z",
     "start_time": "2019-11-21T15:09:24.226818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1461618, 115)\n",
      "(421665, 114)\n",
      "(421665, 22)\n"
     ]
    }
   ],
   "source": [
    "train_raw = pd.read_csv(\"train_pre_31.csv\")\n",
    "X_test = pd.read_csv(\"test_pre_31.csv\")\n",
    "train_raw = train_raw.drop_duplicates(subset=train_raw.columns[~train_raw.columns.isin([\n",
    "        'acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "       'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm', 'locdt', 'mcc',\n",
    "       'mchno', 'ovrlt', 'scity', 'stocn', 'stscd', \"loctm_min\", \n",
    "       'conam_mean_by_loctm_min', \n",
    "       'conam_mean_by_loctm_sec', \n",
    "       'bacno_cano_no_recent_10_conam_pos_trend',\n",
    "       'bacno_cano_no_recent_7_loctm_diff', \n",
    "       'bacno_cano_loctm_uique_count', \n",
    "       'bacno_cano_14_rolling_times_prime_time_fraud_cnt',\n",
    "       'bacno_cano_10_times_sleep_time_cnt',\n",
    "       'bacno_cano_10_sleep_time_conam_lower_3000_cnt',\n",
    "       'bacno_cano_10_prime_time_fraud_conam_lower_3000_cnt', \n",
    "       'bacno_cano_no_recent_10_ecfg_sum', 'fraud_ind', \"locdt\",\n",
    "        'flbmk_value_counts', 'hcefg_value_counts', 'flg_3dsmk_value_counts'\n",
    "         'ecfg_value_counts', 'ovrlt_value_counts', 'insfg_value_counts'\n",
    "        \"conam_mean_by_train_test_acqic\", \"loctm_value_counts\",\n",
    "        \"loctm_hour_value_counts\", 'loctm_sec'])], keep='last')\n",
    "test_raw = pd.read_csv(\"test.csv\")\n",
    "\n",
    "X = train_raw.drop('fraud_ind', axis=1)\n",
    "y = train_raw['fraud_ind']\n",
    "\n",
    "print(train_raw.shape)\n",
    "print(X_test.shape)\n",
    "print(test_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T15:10:04.883446Z",
     "start_time": "2019-11-21T15:09:58.916753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1461618, 103)\n",
      "(421665, 103)\n"
     ]
    }
   ],
   "source": [
    "all_features = pd.concat([X, X_test], axis=0)\n",
    "all_features.tail()\n",
    "\n",
    "del all_features[\"locdt\"]\n",
    "del all_features['flg_3dsmk_value_counts']\n",
    "del all_features['flbmk_value_counts']\n",
    "del all_features['hcefg_value_counts']\n",
    "del all_features['ecfg_value_counts']\n",
    "del all_features['ovrlt_value_counts']\n",
    "del all_features['insfg_value_counts']\n",
    "del all_features[\"conam_mean_by_train_test_acqic\"]\n",
    "\n",
    "del all_features[\"loctm_value_counts\"]\n",
    "del all_features[\"loctm_hour_value_counts\"]\n",
    "del all_features['loctm_sec']\n",
    "\n",
    "X = all_features[:-len(X_test)]\n",
    "X_test = all_features[-len(X_test):]\n",
    "print(X.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義資料type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T15:10:06.709749Z",
     "start_time": "2019-11-21T15:10:06.680086Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_colnames = [\"bacno\", \"cano\", \"mchno\", \"acqic\", \"mcc\", \"ecfg\", \"insfg\", \"contp\", 'etymd',\n",
    "                \"stocn\", \"scity\", \"stscd\", \"ovrlt\", \"flbmk\", \"hcefg\", \"csmcu\", \"flg_3dsmk\"] # + high_cacol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T15:10:14.635078Z",
     "start_time": "2019-11-21T15:10:08.374192Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bacno\n",
      "category\n",
      "cano\n",
      "category\n",
      "mchno\n",
      "category\n",
      "acqic\n",
      "category\n",
      "mcc\n",
      "category\n",
      "ecfg\n",
      "category\n",
      "insfg\n",
      "category\n",
      "contp\n",
      "category\n",
      "etymd\n",
      "category\n",
      "stocn\n",
      "category\n",
      "scity\n",
      "category\n",
      "stscd\n",
      "category\n",
      "ovrlt\n",
      "category\n",
      "flbmk\n",
      "category\n",
      "hcefg\n",
      "category\n",
      "csmcu\n",
      "category\n",
      "flg_3dsmk\n",
      "category\n"
     ]
    }
   ],
   "source": [
    "# Your data contains string(?) values. You should set categorical columns in pandas to treat them properly. \n",
    "# After that there is no need to pass them again in categorical_feature param. \n",
    "model_type = 'lgb'\n",
    "for col in cat_colnames:\n",
    "    # for catboost\n",
    "    if model_type == 'cat':\n",
    "        try:\n",
    "            X[col] = X[col].astype('str')\n",
    "            X_test[col] = X_test[col].astype('str')\n",
    "            print(col)\n",
    "            print(X[col].dtype)\n",
    "        except Exception:\n",
    "            print(\"No \", col, \" exists\")\n",
    "    else:\n",
    "        try:\n",
    "            X[col] = X[col].astype('category')\n",
    "            X_test[col] = X_test[col].astype('category')\n",
    "            print(col)\n",
    "            print(X[col].dtype)\n",
    "        except Exception:\n",
    "            print(\"No \", col, \" exists\")\n",
    "# X['city_town'] = X['city_town'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T15:10:20.824802Z",
     "start_time": "2019-11-21T15:10:16.463475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [ 357931  357948  358029 ... 1461615 1461616 1461617] Test: [     0      1      2 ... 365485 365486 365487]\n",
      "Train: [      0       1       2 ... 1461615 1461616 1461617] Test: [357931 357948 358029 ... 730833 730834 730835]\n",
      "Train: [      0       1       2 ... 1461615 1461616 1461617] Test: [ 728957  729015  729104 ... 1096284 1096285 1096286]\n",
      "Train: [      0       1       2 ... 1096284 1096285 1096286] Test: [1089685 1089726 1089803 ... 1461615 1461616 1461617]\n",
      "(1096214, 103)\n",
      "(1096214,)\n",
      "(365404, 103)\n",
      "(365404,)\n"
     ]
    }
   ],
   "source": [
    "perm = np.random.permutation(len(X)) \n",
    "X = X.iloc[perm].reset_index(drop=True) \n",
    "y = y.iloc[perm].reset_index(drop=True)\n",
    "\n",
    "sss = StratifiedKFold(n_splits=4, random_state=111, shuffle=False)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不同模型適配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型調參"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T16:18:53.815772Z",
     "start_time": "2019-11-20T16:18:53.806929Z"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK\n",
    "\n",
    "# Domain Space\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "# Optimization Algorithm\n",
    "from hyperopt import tpe, fmin, Trials\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Domain Space (parameter space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T16:23:52.206449Z",
     "start_time": "2019-11-20T16:23:52.198964Z"
    }
   },
   "outputs": [],
   "source": [
    "# 需對每個要調的參數創造空間\n",
    "\n",
    "\"\"\"\n",
    "Demo\n",
    "\"\"\"\n",
    "# learning rate: \n",
    "# learning_rate = {'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.2))}\n",
    "# learning_rate_dist = []\n",
    "# Draw 10000 samples from the learning rate domain\n",
    "# for _ in range(10000):\n",
    "#     learning_rate_dist.append(sample(learning_rate)['learning_rate'])  # 檢視抽樣空間結果\n",
    "\n",
    "# num_leaves\n",
    "# num_leaves = {'num_leaves': hp.quniform('num_leaves', 30, 150, 1)}\n",
    "# num_leaves_dist = []\n",
    "\n",
    "# boosting type domain\n",
    "# boosting_type = {'boosting_type':\n",
    "#                      hp.choice('boosting_type',\n",
    "#                                [{'boosting_type': 'gbdt', 'subsample': hp.uniform('subsample', 0.5, 1)},\n",
    "#                                 {'boosting_type': 'dart', 'subsample': hp.uniform('subsample', 0.5, 1)},\n",
    "#                                 {'boosting_type': 'goss', 'subsample': 1.0}]\n",
    "#                                )\n",
    "#                  }\n",
    "# Draw a sample\n",
    "# params_set = sample(boosting_type)\n",
    "\n",
    "space = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    # 'n_estimators': hp.quniform('num_leaves', 100, 5000, 1),\n",
    "#     'class_weight': hp.choice('class_weight', [None, 'balanced']),\n",
    "#     'boosting_type': hp.choice('boosting_type',\n",
    "#                                [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)},\n",
    "#                                 {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n",
    "#                                 {'boosting_type': 'goss', 'subsample': 1.0}]),\n",
    "    'num_leaves': hp.quniform('num_leaves', 20, 80, 1),\n",
    "    # 'learning_rate': hp.loguniform('learning_rate', np.log(0.0005), np.log(0.2)),\n",
    "    'learning_rate': 0.1,\n",
    "    # 'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n",
    "    # 'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "    # 'reg_alpha': hp.uniform('reg_alpha', 0.0, 3.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 3.0),\n",
    "    'feature_fraction': hp.uniform('feature_fraction', 0.6, 1.0),\n",
    "    'bagging_fraction': hp.uniform('bagging_fraction', 0.6, 1.0),\n",
    "    'min_child_weight': hp.uniform('min_child_weight', 0.0, 30),\n",
    "    # 'max_drop': hp.quniform('max_drop', 30, 60, 5),\n",
    "    # 'skip_drop': hp.uniform('skip_drop', 0.5, 0.8),\n",
    "    # 'drop_rate': hp.uniform('drop_rate', 0.1, 0.5),\n",
    "    'scale_pos_weight': hp.quniform('scale_pos_weight', 1, 10, 0.3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T16:23:56.377624Z",
     "start_time": "2019-11-20T16:23:56.372037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.9781067413963053,\n",
       " 'boosting_type': 'gbdt',\n",
       " 'feature_fraction': 0.6289145448268182,\n",
       " 'learning_rate': 0.1,\n",
       " 'min_child_weight': 20.233194332365073,\n",
       " 'num_leaves': 25.0,\n",
       " 'objective': 'binary',\n",
       " 'reg_lambda': 0.6369540662445169,\n",
       " 'scale_pos_weight': 9.6}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample from the full space\n",
    "example = sample(space)\n",
    "\n",
    "# Dictionary get method with default\n",
    "# subsample = example['boosting_type'].get('subsample', 1.0)\n",
    "\n",
    "# Assign top-level keys\n",
    "# example['boosting_type'] = example['boosting_type']['boosting_type']\n",
    "# example['subsample'] = subsample\n",
    "\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T16:24:00.646871Z",
     "start_time": "2019-11-20T16:24:00.636884Z"
    }
   },
   "outputs": [],
   "source": [
    "# N_FOLDS = 5\n",
    "\n",
    "# Create the dataset\n",
    "# train_set = lgb.Dataset(X_train_hyper, y_train_hyper)\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    # 放那些只吃整數的參數\n",
    "    for parameter_name in ['num_leaves']: # , 'max_drop'\n",
    "        params[parameter_name] = int(params[parameter_name])\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    # Perform n_fold cross validation with hyperparameters\n",
    "    # Use early stopping and evalute based on ROC AUC\n",
    "#     cv_results = lgb.cv(params, \n",
    "#                         train_set, \n",
    "#                         nfold=n_folds, \n",
    "#                         num_boost_round=100, # 先放1000去看~\n",
    "#                         # early_stopping_rounds=100,  # not work for dart\n",
    "#                         metrics='rmse', \n",
    "#                         # seed=50,\n",
    "#                         stratified=False)\n",
    "    \n",
    "    model_gbdt = LGBMClassifier(**params)\n",
    "    \n",
    "    \n",
    "#     model_dart = LGBMClassifier(objective='binary', # regression, L2 loss, aliases: regression_l2, l2, mean_squared_error, mse, l2_root, root_mean_squared_error, rmse\n",
    "#                              boosting_type='dart',\n",
    "#                              num_leaves=params['num_leaves'],  # < 2^max_depth\n",
    "#                              reg_lambda=params['reg_lambda'],\n",
    "#                              # reg_alpha=params['reg_alpha'],\n",
    "#                              learning_rate=0.1, # 先設一樣的learning rate去調\n",
    "#                              n_estimators=1000,\n",
    "#                              bagging_fraction=params['bagging_fraction'],\n",
    "#                              feature_fraction=params['feature_fraction'],\n",
    "#                              drop_rate=params['drop_rate'],  # default=0.1\n",
    "#                              max_drop=params['max_drop'], # 10  # default=50, 每iteration最多丟掉幾顆樹\n",
    "#                              skip_drop=params['skip_drop'], # 0.6 # default=0.5 每iteration決定dropout的機率\n",
    "#                              # xgboost_dart_mode=True,  # default=False, 是否要用xgb的dart模式\n",
    "#                              # uniform_drop=True,  # set this to true, if you want to use uniform drop\n",
    "#                              # drop_seed=11,  # random seed to choose dropping models\n",
    "#                              # early_stopping_rounds=100\n",
    "#                              # feature_fraction_seed=7\n",
    "#                          )\n",
    "\n",
    "    print_callback = lgb.print_evaluation(period=1000)\n",
    "\n",
    "    fit_result = model_gbdt.fit(X_train, y_train, \n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  eval_metric='auc',\n",
    "                  # categorical_feature=categorical_feature_idx,\n",
    "                  # early_stopping_rounds=100, # 在dart不能用，因為每一輪整個模型參數都會被更新\n",
    "                  verbose=0,\n",
    "                  callbacks=[print_callback]\n",
    "                 )\n",
    "    \n",
    "    \n",
    "    # print(cv_results)\n",
    "    # stratify works only with classification problems. So to work with regression, you need to make it False.\n",
    "    \n",
    "    run_time = timer() - start\n",
    "    \n",
    "    # Extract the best score\n",
    "    loss = fit_result.best_score_['valid_0']['binary_logloss']\n",
    "    \n",
    "    # Boosting rounds that returned the highest cv score\n",
    "    # n_estimators = int(np.argmax(cv_results['rmse-mean']) + 1)\n",
    "    \n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([loss, params, run_time])\n",
    "    of_connection.close()\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    # loss = 1 - best_score\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'train_time': run_time, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T16:24:03.592435Z",
     "start_time": "2019-11-20T16:24:03.576832Z"
    }
   },
   "outputs": [],
   "source": [
    "# Algorithm\n",
    "tpe_algorithm = tpe.suggest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.0.4  Result History\n",
    "\n",
    "Keeping track of the results is not strictly necessary as Hyperopt will do this internally for the algorithm. However, if we want to find out what is going on behind the scenes, we can use a Trials object which will store basic training information and also the dictionary returned from the objective function (which includes the loss and params). Making a trials object is one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T16:24:05.204327Z",
     "start_time": "2019-11-20T16:24:05.193080Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keep track of results\n",
    "bayes_trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T16:24:37.341625Z",
     "start_time": "2019-11-20T16:24:37.328244Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# File to save first results\n",
    "out_file = 'lgb_gbdt_trials_pre_31_lr_0_1_fix.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write the headers to the file\n",
    "writer.writerow(['loss', 'params', 'train_time'])\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T19:55:45.651849Z",
     "start_time": "2019-11-20T16:24:44.246880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [3:31:01<00:00, 43.28s/it, best loss: 0.015268564481717449]   \n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin\n",
    "\n",
    "MAX_EVALS = 200\n",
    "\n",
    "# Optimize\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest,\n",
    "            max_evals=MAX_EVALS, trials=bayes_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T19:55:47.248289Z",
     "start_time": "2019-11-20T19:55:47.244369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'loss': 0.015268564481717449, 'params': {'bagging_fraction': 0.7795345629576638, 'boosting_type': 'gbdt', 'feature_fraction': 0.793185302484529, 'learning_rate': 0.1, 'min_child_weight': 8.788416143799257, 'num_leaves': 79, 'objective': 'binary', 'reg_lambda': 0.9988765862420731, 'scale_pos_weight': 0.8999999999999999}, 'train_time': 40.911826491355896, 'status': 'ok'}, {'loss': 0.015305418125836759, 'params': {'bagging_fraction': 0.62306323285472, 'boosting_type': 'gbdt', 'feature_fraction': 0.8607417975146786, 'learning_rate': 0.1, 'min_child_weight': 11.764607875190393, 'num_leaves': 77, 'objective': 'binary', 'reg_lambda': 0.46657449147625035, 'scale_pos_weight': 0.8999999999999999}, 'train_time': 39.90269403811544, 'status': 'ok'}]\n"
     ]
    }
   ],
   "source": [
    "# lr固定(0.1)、樹數量固定(1000)的情況下的結果\n",
    "bayes_trials_results = sorted(bayes_trials.results, key=lambda x: x['loss'])\n",
    "print(bayes_trials_results[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### retrain the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T17:53:07.094747Z",
     "start_time": "2019-11-21T16:27:21.540911Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lightGBM (dart)\n",
    "{'bagging_fraction': 0.9292807078881113, 'boosting_type': 'gbdt', 'feature_fraction': 0.8601841783747871, \n",
    " 'learning_rate': 0.06303137412922537, 'num_leaves': 61, 'objective': 'binary', 'reg_alpha': 2.1700796673636176,\n",
    " 'reg_lambda': 1.256856046624738, 'scale_pos_weight': 0.8999999999999999}\n",
    "\n",
    "model_gbdt_tuned = LGBMClassifier(objective='binary', # regression, L2 loss, aliases: regression_l2, l2, mean_squared_error, mse, l2_root, root_mean_squared_error, rmse\n",
    "                             boosting_type='gbdt',\n",
    "                             num_leaves=50,  # < 2^max_depth\n",
    "                             reg_lambda=1.25,\n",
    "                             # reg_alpha=2.17,\n",
    "                             learning_rate=0.06, # 0.1\n",
    "                             n_estimators=3500,\n",
    "                             # max_bin=200, # default=255\n",
    "                             bagging_fraction=0.75,\n",
    "                             # bagging_freq=5, \n",
    "                             # bagging_seed=7,\n",
    "                             feature_fraction=0.75,\n",
    "                             # drop_rate=0.1184,  # default=0.1\n",
    "                             # max_drop=50, # 10  # default=50, 每iteration最多丟掉幾顆樹\n",
    "                             # skip_drop=0.7283, # 0.6 # default=0.5 每iteration決定dropout的機率\n",
    "                             scale_pos_weight=1.8,\n",
    "                             min_child_weight= 8.788416143799257,\n",
    "                             # xgboost_dart_mode=True,  # default=False, 是否要用xgb的dart模式\n",
    "                             # uniform_drop=True,  # set this to true, if you want to use uniform drop\n",
    "                             # drop_seed=11,  # random seed to choose dropping models\n",
    "                             # early_stopping_rounds=100\n",
    "                             # feature_fraction_seed=7\n",
    "                         )\n",
    "\n",
    "print_callback = lgb.print_evaluation(period=250)\n",
    "\n",
    "result = model_gbdt_tuned.fit(X, y, \n",
    "              # eval_set=[(X_val, y_val)],\n",
    "              # eval_metric='auc',\n",
    "              # categorical_feature=categorical_feature_idx,\n",
    "              # early_stopping_rounds=100, # 在dart不能用，因為每一輪整個模型參數都會被更新\n",
    "              verbose=0,\n",
    "              callbacks=[print_callback]\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-19T16:23:59.109Z"
    }
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "joblib.dump(model_gbdt_tuned, 'model_gbdt_tuned_pre_31_final.pkl')\n",
    "# load model\n",
    "# gbm_pickle = joblib.load('lgb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T17:55:06.012380Z",
     "start_time": "2019-11-21T17:53:09.968329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5931"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model_gbdt_tuned.predict(X_test)\n",
    "test_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-21T17:55:10.105238Z",
     "start_time": "2019-11-21T17:55:08.535460Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.concat([test_raw[\"txkey\"], pd.Series(test_pred, name='fraud_ind')], axis=1)\n",
    "submission.to_csv(\"submission_78_model_gbdt_tuned_full_min_child.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
