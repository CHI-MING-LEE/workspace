{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T15:44:06.750872Z",
     "start_time": "2019-03-24T15:44:06.739442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on 20190316\\n\\nDQN practice\\n\\nRef: https://keon.io/deep-q-learning/\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on 20190316\n",
    "\n",
    "DQN practice\n",
    "\n",
    "Ref: \n",
    "1. https://keon.io/deep-q-learning/\n",
    "2. https://zhuanlan.zhihu.com/p/26985029\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](DQN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T08:47:51.613537Z",
     "start_time": "2019-03-31T08:47:50.127964Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T17:30:02.161168Z",
     "start_time": "2019-03-17T17:30:02.156579Z"
    }
   },
   "source": [
    "# Build an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T08:47:52.504739Z",
     "start_time": "2019-03-31T08:47:52.488805Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        # 滿了，最老的會被pop掉，補上新的@@\n",
    "        self.memory = deque(maxlen=2000)  # like a list and shines at 'appendleft' and 'popleft'\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep Q Learning\n",
    "        # Sequential() creates the foundation of the layers.\n",
    "        model = Sequential()\n",
    "        # 'Dense' is the basic form of a neural network layer\n",
    "        # Input Layer of state size(4) and Hidden Layer with 24 nodes\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        # Hidden layer with 24 nodes\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        # Output Layer with # of actions: 2 nodes (left, right)\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        # Create the model based on the information above\n",
    "        model.compile(loss='mse',\n",
    "                  optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        # Sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        # Extract informations from each memory\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # if done, make our target reward\n",
    "            target_q = reward\n",
    "            if not done:\n",
    "              # predict the future discounted reward\n",
    "              target_q = reward + self.gamma * \\\n",
    "                       np.amax(self.model.predict(next_state)[0])\n",
    "            # make the agent to approximately map\n",
    "            # the current state to future discounted reward\n",
    "            # We'll call that current_q\n",
    "            current_q = self.model.predict(state)\n",
    "            current_q[0][action] = target_q\n",
    "            # Train the Neural Net with the state and current_q\n",
    "            self.model.fit(state, current_q, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-28T16:36:33.166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1000, score: 20, e: 1.0\n",
      "episode: 1/1000, score: 20, e: 0.99\n",
      "episode: 2/1000, score: 11, e: 0.98\n",
      "episode: 3/1000, score: 20, e: 0.96\n",
      "episode: 4/1000, score: 30, e: 0.93\n",
      "episode: 5/1000, score: 25, e: 0.91\n",
      "episode: 6/1000, score: 23, e: 0.89\n",
      "episode: 7/1000, score: 11, e: 0.88\n",
      "episode: 8/1000, score: 27, e: 0.86\n",
      "episode: 9/1000, score: 10, e: 0.85\n",
      "episode: 10/1000, score: 35, e: 0.82\n",
      "episode: 11/1000, score: 33, e: 0.79\n",
      "episode: 12/1000, score: 21, e: 0.77\n",
      "episode: 13/1000, score: 19, e: 0.76\n",
      "episode: 14/1000, score: 11, e: 0.75\n",
      "episode: 15/1000, score: 13, e: 0.74\n",
      "episode: 16/1000, score: 23, e: 0.73\n",
      "episode: 17/1000, score: 38, e: 0.7\n",
      "episode: 18/1000, score: 36, e: 0.67\n",
      "episode: 19/1000, score: 15, e: 0.66\n",
      "episode: 20/1000, score: 31, e: 0.64\n",
      "episode: 21/1000, score: 33, e: 0.62\n",
      "episode: 22/1000, score: 26, e: 0.61\n",
      "episode: 23/1000, score: 83, e: 0.56\n",
      "episode: 24/1000, score: 32, e: 0.54\n",
      "episode: 25/1000, score: 109, e: 0.48\n",
      "episode: 26/1000, score: 141, e: 0.42\n",
      "episode: 27/1000, score: 120, e: 0.37\n",
      "episode: 28/1000, score: 129, e: 0.33\n",
      "episode: 29/1000, score: 133, e: 0.29\n",
      "episode: 30/1000, score: 112, e: 0.26\n",
      "episode: 31/1000, score: 160, e: 0.22\n",
      "episode: 32/1000, score: 124, e: 0.19\n",
      "episode: 33/1000, score: 141, e: 0.17\n",
      "episode: 34/1000, score: 191, e: 0.14\n",
      "episode: 35/1000, score: 175, e: 0.12\n",
      "episode: 36/1000, score: 166, e: 0.099\n",
      "episode: 37/1000, score: 268, e: 0.075\n",
      "episode: 38/1000, score: 154, e: 0.065\n",
      "episode: 39/1000, score: 138, e: 0.056\n",
      "episode: 40/1000, score: 184, e: 0.047\n",
      "episode: 41/1000, score: 119, e: 0.042\n",
      "episode: 42/1000, score: 47, e: 0.04\n",
      "episode: 43/1000, score: 10, e: 0.039\n",
      "episode: 44/1000, score: 144, e: 0.034\n",
      "episode: 45/1000, score: 71, e: 0.032\n",
      "episode: 46/1000, score: 138, e: 0.028\n",
      "episode: 47/1000, score: 175, e: 0.023\n",
      "episode: 48/1000, score: 264, e: 0.018\n",
      "episode: 49/1000, score: 194, e: 0.015\n",
      "episode: 50/1000, score: 248, e: 0.011\n",
      "episode: 51/1000, score: 210, e: 0.01\n",
      "episode: 52/1000, score: 159, e: 0.01\n",
      "episode: 53/1000, score: 149, e: 0.01\n",
      "episode: 54/1000, score: 155, e: 0.01\n",
      "episode: 55/1000, score: 177, e: 0.01\n",
      "episode: 56/1000, score: 182, e: 0.01\n",
      "episode: 57/1000, score: 149, e: 0.01\n",
      "episode: 58/1000, score: 130, e: 0.01\n",
      "episode: 59/1000, score: 146, e: 0.01\n",
      "episode: 60/1000, score: 13, e: 0.01\n",
      "episode: 61/1000, score: 226, e: 0.01\n",
      "episode: 62/1000, score: 499, e: 0.01\n",
      "episode: 63/1000, score: 182, e: 0.01\n",
      "episode: 64/1000, score: 11, e: 0.01\n",
      "episode: 65/1000, score: 184, e: 0.01\n",
      "episode: 66/1000, score: 158, e: 0.01\n",
      "episode: 67/1000, score: 11, e: 0.01\n",
      "episode: 68/1000, score: 257, e: 0.01\n",
      "episode: 69/1000, score: 122, e: 0.01\n",
      "episode: 70/1000, score: 124, e: 0.01\n",
      "episode: 71/1000, score: 145, e: 0.01\n",
      "episode: 72/1000, score: 128, e: 0.01\n",
      "episode: 73/1000, score: 499, e: 0.01\n",
      "episode: 74/1000, score: 467, e: 0.01\n",
      "episode: 75/1000, score: 158, e: 0.01\n",
      "episode: 76/1000, score: 217, e: 0.01\n",
      "episode: 77/1000, score: 52, e: 0.01\n",
      "episode: 78/1000, score: 13, e: 0.01\n",
      "episode: 79/1000, score: 8, e: 0.01\n",
      "episode: 80/1000, score: 9, e: 0.01\n",
      "episode: 81/1000, score: 21, e: 0.01\n",
      "episode: 82/1000, score: 107, e: 0.01\n",
      "episode: 83/1000, score: 203, e: 0.01\n",
      "episode: 84/1000, score: 163, e: 0.01\n",
      "episode: 85/1000, score: 124, e: 0.01\n",
      "episode: 86/1000, score: 210, e: 0.01\n",
      "episode: 87/1000, score: 180, e: 0.01\n",
      "episode: 88/1000, score: 183, e: 0.01\n",
      "episode: 89/1000, score: 334, e: 0.01\n",
      "episode: 90/1000, score: 257, e: 0.01\n",
      "episode: 91/1000, score: 121, e: 0.01\n",
      "episode: 92/1000, score: 433, e: 0.01\n",
      "episode: 93/1000, score: 124, e: 0.01\n",
      "episode: 94/1000, score: 337, e: 0.01\n",
      "episode: 95/1000, score: 21, e: 0.01\n",
      "episode: 96/1000, score: 226, e: 0.01\n",
      "episode: 97/1000, score: 183, e: 0.01\n",
      "episode: 98/1000, score: 245, e: 0.01\n",
      "episode: 99/1000, score: 377, e: 0.01\n",
      "episode: 100/1000, score: 11, e: 0.01\n",
      "episode: 101/1000, score: 493, e: 0.01\n",
      "episode: 102/1000, score: 499, e: 0.01\n",
      "episode: 103/1000, score: 363, e: 0.01\n",
      "episode: 104/1000, score: 20, e: 0.01\n",
      "episode: 105/1000, score: 58, e: 0.01\n",
      "episode: 106/1000, score: 329, e: 0.01\n",
      "episode: 107/1000, score: 161, e: 0.01\n",
      "episode: 108/1000, score: 221, e: 0.01\n",
      "episode: 109/1000, score: 499, e: 0.01\n",
      "episode: 110/1000, score: 147, e: 0.01\n",
      "episode: 111/1000, score: 148, e: 0.01\n",
      "episode: 112/1000, score: 126, e: 0.01\n",
      "episode: 113/1000, score: 10, e: 0.01\n",
      "episode: 114/1000, score: 371, e: 0.01\n",
      "episode: 115/1000, score: 78, e: 0.01\n",
      "episode: 116/1000, score: 166, e: 0.01\n",
      "episode: 117/1000, score: 88, e: 0.01\n",
      "episode: 118/1000, score: 101, e: 0.01\n",
      "episode: 119/1000, score: 119, e: 0.01\n",
      "episode: 120/1000, score: 120, e: 0.01\n",
      "episode: 121/1000, score: 38, e: 0.01\n",
      "episode: 122/1000, score: 140, e: 0.01\n",
      "episode: 123/1000, score: 220, e: 0.01\n",
      "episode: 124/1000, score: 116, e: 0.01\n",
      "episode: 125/1000, score: 134, e: 0.01\n",
      "episode: 126/1000, score: 219, e: 0.01\n",
      "episode: 127/1000, score: 171, e: 0.01\n",
      "episode: 128/1000, score: 20, e: 0.01\n",
      "episode: 129/1000, score: 11, e: 0.01\n",
      "episode: 130/1000, score: 333, e: 0.01\n",
      "episode: 131/1000, score: 499, e: 0.01\n",
      "episode: 132/1000, score: 217, e: 0.01\n",
      "episode: 133/1000, score: 85, e: 0.01\n",
      "episode: 134/1000, score: 227, e: 0.01\n",
      "episode: 135/1000, score: 461, e: 0.01\n",
      "episode: 136/1000, score: 293, e: 0.01\n",
      "episode: 137/1000, score: 146, e: 0.01\n",
      "episode: 138/1000, score: 380, e: 0.01\n",
      "episode: 139/1000, score: 499, e: 0.01\n",
      "episode: 140/1000, score: 439, e: 0.01\n",
      "episode: 141/1000, score: 15, e: 0.01\n",
      "episode: 142/1000, score: 9, e: 0.01\n",
      "episode: 143/1000, score: 8, e: 0.01\n",
      "episode: 144/1000, score: 8, e: 0.01\n",
      "episode: 145/1000, score: 9, e: 0.01\n",
      "episode: 146/1000, score: 8, e: 0.01\n",
      "episode: 147/1000, score: 9, e: 0.01\n",
      "episode: 148/1000, score: 8, e: 0.01\n",
      "episode: 149/1000, score: 8, e: 0.01\n",
      "episode: 150/1000, score: 8, e: 0.01\n",
      "episode: 151/1000, score: 8, e: 0.01\n",
      "episode: 152/1000, score: 8, e: 0.01\n",
      "episode: 153/1000, score: 8, e: 0.01\n",
      "episode: 154/1000, score: 9, e: 0.01\n",
      "episode: 155/1000, score: 9, e: 0.01\n",
      "episode: 156/1000, score: 9, e: 0.01\n",
      "episode: 157/1000, score: 8, e: 0.01\n",
      "episode: 158/1000, score: 9, e: 0.01\n",
      "episode: 159/1000, score: 11, e: 0.01\n",
      "episode: 160/1000, score: 8, e: 0.01\n",
      "episode: 161/1000, score: 8, e: 0.01\n",
      "episode: 162/1000, score: 10, e: 0.01\n",
      "episode: 163/1000, score: 9, e: 0.01\n",
      "episode: 164/1000, score: 11, e: 0.01\n",
      "episode: 165/1000, score: 16, e: 0.01\n",
      "episode: 166/1000, score: 49, e: 0.01\n",
      "episode: 167/1000, score: 60, e: 0.01\n",
      "episode: 168/1000, score: 61, e: 0.01\n",
      "episode: 169/1000, score: 247, e: 0.01\n",
      "episode: 170/1000, score: 60, e: 0.01\n",
      "episode: 171/1000, score: 9, e: 0.01\n",
      "episode: 172/1000, score: 9, e: 0.01\n",
      "episode: 173/1000, score: 9, e: 0.01\n",
      "episode: 174/1000, score: 9, e: 0.01\n",
      "episode: 175/1000, score: 8, e: 0.01\n",
      "episode: 176/1000, score: 8, e: 0.01\n",
      "episode: 177/1000, score: 9, e: 0.01\n",
      "episode: 178/1000, score: 9, e: 0.01\n",
      "episode: 179/1000, score: 9, e: 0.01\n",
      "episode: 180/1000, score: 8, e: 0.01\n",
      "episode: 181/1000, score: 8, e: 0.01\n",
      "episode: 182/1000, score: 9, e: 0.01\n",
      "episode: 183/1000, score: 9, e: 0.01\n",
      "episode: 184/1000, score: 9, e: 0.01\n",
      "episode: 185/1000, score: 8, e: 0.01\n",
      "episode: 186/1000, score: 8, e: 0.01\n",
      "episode: 187/1000, score: 8, e: 0.01\n",
      "episode: 188/1000, score: 9, e: 0.01\n",
      "episode: 189/1000, score: 9, e: 0.01\n",
      "episode: 190/1000, score: 8, e: 0.01\n",
      "episode: 191/1000, score: 9, e: 0.01\n",
      "episode: 192/1000, score: 7, e: 0.01\n",
      "episode: 193/1000, score: 10, e: 0.01\n",
      "episode: 194/1000, score: 28, e: 0.01\n",
      "episode: 195/1000, score: 39, e: 0.01\n",
      "episode: 196/1000, score: 52, e: 0.01\n",
      "episode: 197/1000, score: 84, e: 0.01\n",
      "episode: 198/1000, score: 124, e: 0.01\n",
      "episode: 199/1000, score: 86, e: 0.01\n",
      "episode: 200/1000, score: 53, e: 0.01\n",
      "episode: 201/1000, score: 150, e: 0.01\n",
      "episode: 202/1000, score: 281, e: 0.01\n",
      "episode: 203/1000, score: 108, e: 0.01\n",
      "episode: 204/1000, score: 111, e: 0.01\n",
      "episode: 205/1000, score: 104, e: 0.01\n",
      "episode: 206/1000, score: 99, e: 0.01\n",
      "episode: 207/1000, score: 147, e: 0.01\n",
      "episode: 208/1000, score: 138, e: 0.01\n",
      "episode: 209/1000, score: 133, e: 0.01\n",
      "episode: 210/1000, score: 128, e: 0.01\n",
      "episode: 211/1000, score: 131, e: 0.01\n",
      "episode: 212/1000, score: 105, e: 0.01\n",
      "episode: 213/1000, score: 70, e: 0.01\n",
      "episode: 214/1000, score: 149, e: 0.01\n",
      "episode: 215/1000, score: 122, e: 0.01\n",
      "episode: 216/1000, score: 21, e: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 217/1000, score: 94, e: 0.01\n",
      "episode: 218/1000, score: 102, e: 0.01\n",
      "episode: 219/1000, score: 88, e: 0.01\n",
      "episode: 220/1000, score: 107, e: 0.01\n",
      "episode: 221/1000, score: 112, e: 0.01\n",
      "episode: 222/1000, score: 153, e: 0.01\n",
      "episode: 223/1000, score: 8, e: 0.01\n",
      "episode: 224/1000, score: 9, e: 0.01\n",
      "episode: 225/1000, score: 10, e: 0.01\n",
      "episode: 226/1000, score: 17, e: 0.01\n",
      "episode: 227/1000, score: 9, e: 0.01\n",
      "episode: 228/1000, score: 8, e: 0.01\n",
      "episode: 229/1000, score: 47, e: 0.01\n",
      "episode: 230/1000, score: 123, e: 0.01\n",
      "episode: 231/1000, score: 120, e: 0.01\n",
      "episode: 232/1000, score: 111, e: 0.01\n",
      "episode: 233/1000, score: 85, e: 0.01\n",
      "episode: 234/1000, score: 62, e: 0.01\n",
      "episode: 235/1000, score: 114, e: 0.01\n",
      "episode: 236/1000, score: 109, e: 0.01\n",
      "episode: 237/1000, score: 25, e: 0.01\n",
      "episode: 238/1000, score: 107, e: 0.01\n",
      "episode: 239/1000, score: 100, e: 0.01\n",
      "episode: 240/1000, score: 46, e: 0.01\n",
      "episode: 241/1000, score: 131, e: 0.01\n",
      "episode: 242/1000, score: 46, e: 0.01\n",
      "episode: 243/1000, score: 36, e: 0.01\n",
      "episode: 244/1000, score: 31, e: 0.01\n",
      "episode: 245/1000, score: 111, e: 0.01\n",
      "episode: 246/1000, score: 74, e: 0.01\n",
      "episode: 247/1000, score: 59, e: 0.01\n",
      "episode: 248/1000, score: 103, e: 0.01\n",
      "episode: 249/1000, score: 94, e: 0.01\n",
      "episode: 250/1000, score: 102, e: 0.01\n",
      "episode: 251/1000, score: 234, e: 0.01\n",
      "episode: 252/1000, score: 192, e: 0.01\n",
      "episode: 253/1000, score: 179, e: 0.01\n",
      "episode: 254/1000, score: 164, e: 0.01\n",
      "episode: 255/1000, score: 163, e: 0.01\n",
      "episode: 256/1000, score: 147, e: 0.01\n",
      "episode: 257/1000, score: 254, e: 0.01\n",
      "episode: 258/1000, score: 319, e: 0.01\n",
      "episode: 259/1000, score: 126, e: 0.01\n",
      "episode: 260/1000, score: 169, e: 0.01\n",
      "episode: 261/1000, score: 173, e: 0.01\n",
      "episode: 262/1000, score: 146, e: 0.01\n",
      "episode: 263/1000, score: 206, e: 0.01\n",
      "episode: 264/1000, score: 17, e: 0.01\n",
      "episode: 265/1000, score: 136, e: 0.01\n",
      "episode: 266/1000, score: 95, e: 0.01\n",
      "episode: 267/1000, score: 9, e: 0.01\n",
      "episode: 268/1000, score: 144, e: 0.01\n",
      "episode: 269/1000, score: 207, e: 0.01\n",
      "episode: 270/1000, score: 194, e: 0.01\n",
      "episode: 271/1000, score: 174, e: 0.01\n",
      "episode: 272/1000, score: 88, e: 0.01\n",
      "episode: 273/1000, score: 18, e: 0.01\n",
      "episode: 274/1000, score: 12, e: 0.01\n",
      "episode: 275/1000, score: 140, e: 0.01\n",
      "episode: 276/1000, score: 200, e: 0.01\n",
      "episode: 277/1000, score: 310, e: 0.01\n",
      "episode: 278/1000, score: 169, e: 0.01\n",
      "episode: 279/1000, score: 202, e: 0.01\n",
      "episode: 280/1000, score: 221, e: 0.01\n",
      "episode: 281/1000, score: 416, e: 0.01\n",
      "episode: 282/1000, score: 499, e: 0.01\n",
      "episode: 283/1000, score: 499, e: 0.01\n",
      "episode: 284/1000, score: 156, e: 0.01\n",
      "episode: 285/1000, score: 9, e: 0.01\n",
      "episode: 286/1000, score: 8, e: 0.01\n",
      "episode: 287/1000, score: 128, e: 0.01\n",
      "episode: 288/1000, score: 110, e: 0.01\n",
      "episode: 289/1000, score: 269, e: 0.01\n",
      "episode: 290/1000, score: 277, e: 0.01\n",
      "episode: 291/1000, score: 230, e: 0.01\n",
      "episode: 292/1000, score: 193, e: 0.01\n",
      "episode: 293/1000, score: 158, e: 0.01\n",
      "episode: 294/1000, score: 54, e: 0.01\n",
      "episode: 295/1000, score: 200, e: 0.01\n",
      "episode: 296/1000, score: 170, e: 0.01\n",
      "episode: 297/1000, score: 185, e: 0.01\n",
      "episode: 298/1000, score: 178, e: 0.01\n",
      "episode: 299/1000, score: 189, e: 0.01\n",
      "episode: 300/1000, score: 80, e: 0.01\n",
      "episode: 301/1000, score: 127, e: 0.01\n",
      "episode: 302/1000, score: 141, e: 0.01\n",
      "episode: 303/1000, score: 143, e: 0.01\n",
      "episode: 304/1000, score: 201, e: 0.01\n",
      "episode: 305/1000, score: 151, e: 0.01\n",
      "episode: 306/1000, score: 141, e: 0.01\n",
      "episode: 307/1000, score: 227, e: 0.01\n",
      "episode: 308/1000, score: 125, e: 0.01\n",
      "episode: 309/1000, score: 166, e: 0.01\n",
      "episode: 310/1000, score: 139, e: 0.01\n",
      "episode: 311/1000, score: 133, e: 0.01\n",
      "episode: 312/1000, score: 191, e: 0.01\n",
      "episode: 313/1000, score: 193, e: 0.01\n",
      "episode: 314/1000, score: 21, e: 0.01\n",
      "episode: 315/1000, score: 13, e: 0.01\n",
      "episode: 316/1000, score: 21, e: 0.01\n",
      "episode: 317/1000, score: 260, e: 0.01\n",
      "episode: 318/1000, score: 194, e: 0.01\n",
      "episode: 319/1000, score: 499, e: 0.01\n",
      "episode: 320/1000, score: 216, e: 0.01\n",
      "episode: 321/1000, score: 499, e: 0.01\n",
      "episode: 322/1000, score: 499, e: 0.01\n",
      "episode: 323/1000, score: 408, e: 0.01\n",
      "episode: 324/1000, score: 83, e: 0.01\n",
      "episode: 325/1000, score: 10, e: 0.01\n",
      "episode: 326/1000, score: 10, e: 0.01\n",
      "episode: 327/1000, score: 185, e: 0.01\n",
      "episode: 328/1000, score: 288, e: 0.01\n",
      "episode: 329/1000, score: 499, e: 0.01\n",
      "episode: 330/1000, score: 363, e: 0.01\n",
      "episode: 331/1000, score: 171, e: 0.01\n",
      "episode: 332/1000, score: 186, e: 0.01\n",
      "episode: 333/1000, score: 159, e: 0.01\n",
      "episode: 334/1000, score: 187, e: 0.01\n",
      "episode: 335/1000, score: 102, e: 0.01\n",
      "episode: 336/1000, score: 290, e: 0.01\n",
      "episode: 337/1000, score: 277, e: 0.01\n",
      "episode: 338/1000, score: 10, e: 0.01\n",
      "episode: 339/1000, score: 453, e: 0.01\n",
      "episode: 340/1000, score: 499, e: 0.01\n",
      "episode: 341/1000, score: 243, e: 0.01\n",
      "episode: 342/1000, score: 481, e: 0.01\n",
      "episode: 343/1000, score: 328, e: 0.01\n",
      "episode: 344/1000, score: 224, e: 0.01\n",
      "episode: 345/1000, score: 232, e: 0.01\n",
      "episode: 346/1000, score: 177, e: 0.01\n",
      "episode: 347/1000, score: 216, e: 0.01\n",
      "episode: 348/1000, score: 36, e: 0.01\n",
      "episode: 349/1000, score: 7, e: 0.01\n",
      "episode: 350/1000, score: 8, e: 0.01\n",
      "episode: 351/1000, score: 26, e: 0.01\n",
      "episode: 352/1000, score: 9, e: 0.01\n",
      "episode: 353/1000, score: 8, e: 0.01\n",
      "episode: 354/1000, score: 499, e: 0.01\n",
      "episode: 355/1000, score: 288, e: 0.01\n",
      "episode: 356/1000, score: 269, e: 0.01\n",
      "episode: 357/1000, score: 499, e: 0.01\n",
      "episode: 358/1000, score: 157, e: 0.01\n",
      "episode: 359/1000, score: 242, e: 0.01\n",
      "episode: 360/1000, score: 499, e: 0.01\n",
      "episode: 361/1000, score: 499, e: 0.01\n",
      "episode: 362/1000, score: 134, e: 0.01\n",
      "episode: 363/1000, score: 65, e: 0.01\n",
      "episode: 364/1000, score: 29, e: 0.01\n",
      "episode: 365/1000, score: 93, e: 0.01\n",
      "episode: 366/1000, score: 101, e: 0.01\n",
      "episode: 367/1000, score: 9, e: 0.01\n",
      "episode: 368/1000, score: 13, e: 0.01\n",
      "episode: 369/1000, score: 11, e: 0.01\n",
      "episode: 370/1000, score: 10, e: 0.01\n",
      "episode: 371/1000, score: 14, e: 0.01\n",
      "episode: 372/1000, score: 13, e: 0.01\n",
      "episode: 373/1000, score: 15, e: 0.01\n",
      "episode: 374/1000, score: 52, e: 0.01\n",
      "episode: 375/1000, score: 51, e: 0.01\n",
      "episode: 376/1000, score: 198, e: 0.01\n",
      "episode: 377/1000, score: 118, e: 0.01\n",
      "episode: 378/1000, score: 368, e: 0.01\n",
      "episode: 379/1000, score: 143, e: 0.01\n",
      "episode: 380/1000, score: 499, e: 0.01\n",
      "episode: 381/1000, score: 317, e: 0.01\n",
      "episode: 382/1000, score: 417, e: 0.01\n",
      "episode: 383/1000, score: 123, e: 0.01\n",
      "episode: 384/1000, score: 499, e: 0.01\n",
      "episode: 385/1000, score: 179, e: 0.01\n",
      "episode: 386/1000, score: 499, e: 0.01\n",
      "episode: 387/1000, score: 499, e: 0.01\n",
      "episode: 388/1000, score: 499, e: 0.01\n",
      "episode: 389/1000, score: 125, e: 0.01\n",
      "episode: 390/1000, score: 241, e: 0.01\n",
      "episode: 391/1000, score: 312, e: 0.01\n",
      "episode: 392/1000, score: 499, e: 0.01\n",
      "episode: 393/1000, score: 478, e: 0.01\n",
      "episode: 394/1000, score: 499, e: 0.01\n",
      "episode: 395/1000, score: 430, e: 0.01\n",
      "episode: 396/1000, score: 499, e: 0.01\n",
      "episode: 397/1000, score: 499, e: 0.01\n",
      "episode: 398/1000, score: 54, e: 0.01\n",
      "episode: 399/1000, score: 132, e: 0.01\n",
      "episode: 400/1000, score: 127, e: 0.01\n",
      "episode: 401/1000, score: 199, e: 0.01\n",
      "episode: 402/1000, score: 499, e: 0.01\n",
      "episode: 403/1000, score: 221, e: 0.01\n",
      "episode: 404/1000, score: 103, e: 0.01\n",
      "episode: 405/1000, score: 66, e: 0.01\n",
      "episode: 406/1000, score: 9, e: 0.01\n",
      "episode: 407/1000, score: 171, e: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Agent interacts with Env, so the class doesn't need to contain the class of env\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "done = False\n",
    "batch_size = 32\n",
    "EPISODES = 1000\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()  # initial state\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    for t in range(2000):\n",
    "        # env.render() 還沒安裝相關pkg，不能秀\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else -10  # done就是輸\n",
    "        next_state = np.expand_dims(next_state, axis=0)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                      .format(e, EPISODES, t, agent.epsilon))\n",
    "            break\n",
    "        # start experience replaying after batch_size\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T16:47:26.058908Z",
     "start_time": "2019-03-24T16:47:26.053447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "2\n",
      "(-inf, inf)\n"
     ]
    }
   ],
   "source": [
    "# initialize gym environment and the agent\n",
    "env = gym.make('CartPole-v0')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "print(state_size)\n",
    "print(action_size)\n",
    "print(env.reward_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T16:34:10.316696Z",
     "start_time": "2019-03-20T16:34:10.311341Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.16507484  0.28030365 -0.39574315 -1.69611162] 0.0 True\n",
      "(array([ 0.17068091,  0.09125663, -0.42966538, -1.5477934 ]), 0.0, True, {})\n"
     ]
    }
   ],
   "source": [
    "# env.reset()\n",
    "next_state, reward, done, _ = env.step(1)\n",
    "print(next_state, reward, done)\n",
    "print(env.step(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
