{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T15:18:23.183335Z",
     "start_time": "2019-08-27T15:18:23.179870Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/erhwenkuo/deep-learning-with-keras-notebooks/blob/master/1.8-seq2seq-introduction.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./10minutes_seq2seq.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T16:24:26.930217Z",
     "start_time": "2019-09-04T16:24:25.157006Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T16:24:26.952100Z",
     "start_time": "2019-09-04T16:24:26.943365Z"
    }
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    \"\"\"\n",
    "    給予一組的字符:\n",
    "    + 將這些字符使用one-hot編碼成數字表示\n",
    "    + 解碼one-hot編碼數字表示成為原本的字符\n",
    "    + 解碼字符機率的向量以回覆最有可能的字符\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"初始化字符表\n",
    "        \n",
    "        # 參數:\n",
    "            chars: 會出現在輸入的可能字符集\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        \n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"對輸入的字串進行one-hot編碼\n",
    "        \n",
    "        # 參數:\n",
    "            C: 要被編碼的字符\n",
    "            num_rows: one-hot編碼後要回傳的最大行數。這是用來確保每一個輸入都會得到\n",
    "            相同行數的輸出\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        \"\"\"對輸入的編碼(向量)進行解碼\n",
    "        \n",
    "        # 參數:\n",
    "            x: 要被解碼的字符向量或字符編碼\n",
    "            calc_argmax: 是否要用argmax算符找出機率最大的字符編碼\n",
    "        \"\"\"\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)\n",
    "    \n",
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相關的參數與產生訓練用的資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T16:24:27.743773Z",
     "start_time": "2019-09-04T16:24:27.737678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n"
     ]
    }
   ],
   "source": [
    "# 模型與資料集的參數\n",
    "TRAINING_SIZE = 50000 # 訓練資料集的samples數\n",
    "DIGITS = 3            # 加數或被加數的字符數\n",
    "INVERT = True \n",
    "\n",
    "# 輸入的最大長度 'int + int' (比如, '345+678')\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "# 所有要用到的字符(包括數字、加號及空格)\n",
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars) # 創建CharacterTable的instance\n",
    "\n",
    "questions = [] # 訓練用的句子 \"xxx+yyy\"\n",
    "expected = []  # 訓練用的標籤\n",
    "seen = set()\n",
    "\n",
    "print('Generating data...') # 產生訓練資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T16:24:28.661398Z",
     "start_time": "2019-09-04T16:24:28.646611Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                           for i in range(np.random.randint(1, DIGITS+1))))\n",
    "f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T16:24:35.691971Z",
     "start_time": "2019-09-04T16:24:29.746435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total addition questions: 50000\n"
     ]
    }
   ],
   "source": [
    "while len(questions) < TRAINING_SIZE:\n",
    "    # 數字產生器 (3個字符)\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                           for i in range(np.random.randint(1, DIGITS+1))))\n",
    "    a, b = f(), f()\n",
    "    # 跳過己經看過的題目以及x+Y = Y+x這樣的題目\n",
    "    key = tuple(sorted((a, b)))  # 排序a, b\n",
    "    if key in seen:\n",
    "        continue    \n",
    "    seen.add(key)\n",
    "    \n",
    "    # 當數字不足MAXLEN則填補空白\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    \n",
    "    # 答案的最大的字符長度為DIGITS + 1 (3位數+3位數 <= 4位數)\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    if INVERT:\n",
    "        # 故意轉的~~~~\n",
    "        # 調轉問題字符的方向, 比如. '12+345'變成'543+21'\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "    \n",
    "print('Total addition questions:', len(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料的前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T16:36:31.630910Z",
     "start_time": "2019-09-04T16:36:31.620904Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "50000\n",
      "7\n",
      "4\n",
      "12\n",
      "(50000, 4, 12)\n",
      "[[[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]]\n"
     ]
    }
   ],
   "source": [
    "# 把資料做適當的轉換, LSTM預期的資料結構 -> [samples, timesteps, features]\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool) # 初始一個3維的numpy ndarray (特徵資料)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool) # 初始一個3維的numpy ndarray (標籤資料)\n",
    "print(len(questions))\n",
    "print(MAXLEN)\n",
    "print(DIGITS + 1)  # 答案最大四位數\n",
    "print(len(chars))\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T16:36:33.768996Z",
     "start_time": "2019-09-04T16:36:33.422260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature data:  (50000, 7, 12)\n",
      "Label data:  (50000, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "# 將\"特徵資料\"轉換成LSTM預期的資料結構 -> [samples, timesteps, features]\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)      # <--- 要了解為什麼要這樣整理資料\n",
    "\n",
    "print(\"Feature data: \", x.shape)\n",
    "\n",
    "# 將\"標籤資料\"轉換成LSTM預期的資料結構 -> [samples, timesteps, features]\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)  # <--- 要了解為什麼要這樣整理資料\n",
    "\n",
    "print(\"Label data: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T16:36:35.896344Z",
     "start_time": "2019-09-04T16:36:35.878526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "(45000, 7, 12)\n",
      "(45000, 4, 12)\n",
      "Validation Data:\n",
      "(5000, 7, 12)\n",
      "(5000, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "# 打散 Shuffle(x, y)\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# 保留10%的資料來做為驗證\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 構建網絡架構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T16:36:49.102364Z",
     "start_time": "2019-09-04T16:36:48.491958Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 128)               72192     \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 4, 12)             1548      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 12)             0         \n",
      "=================================================================\n",
      "Total params: 205,324\n",
      "Trainable params: 205,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 可以試著替代其它種的rnn units, 比如,GRU或SimpleRNN\n",
    "LSTM = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# ===== 編碼 (encoder) ====\n",
    "\n",
    "# 使用RNN“編碼”輸入序列，產生HIDDEN_SIZE的輸出。\n",
    "# 注意：在輸入序列長度可變的情況下，使用input_shape =（None，num_features）\n",
    "model.add(LSTM(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars)))) # MAXLEN代表是timesteps, 而len(chars)是one-hot編碼的features\n",
    "\n",
    "# 將每一步RNN的timestep從 1 vec -> 4 vecs，這樣才可以跟output match \n",
    "# 作為解碼器RNN的輸入，重複提供每個時間步的RNN的最後一個隱藏狀態。\n",
    "# 重複“DIGITS + 1”次，因為這是最大輸出長度，例如當DIGITS = 3時，最大輸出是999 + 999 = 1998（長度為4)。\n",
    "model.add(layers.RepeatVector(DIGITS + 1))  # 決定time step，也就是輸出幾個字\n",
    "\n",
    "# 因為沒有return_sequence所以會把timestep全部滾完，輸出最後一個state (已repeat 4次) <-- 重要!!\n",
    "\n",
    "# ==== 解碼 (decoder) ====\n",
    "# 解碼器RNN可以是多層堆疊或單層。\n",
    "for _ in range(LAYERS):\n",
    "    # 通過將return_sequences設置為True，不僅返回最後一個輸出，而且還以（num_samples，timesteps，output_dim）\n",
    "    # 的形式返回所有輸出。這是必要的，因為下面的TimeDistributed需要第一個維度是時間步長。\n",
    "    model.add(LSTM(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# 對輸入的每個時間片推送到密集層來對於輸出序列的每一時間步，決定選擇哪個字符。(如果只有dense就只會輸出1個time step而不是每個time step)\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars))))  # 每個time step都要連，每個字的one hot維度\n",
    "\n",
    "model.add(layers.Activation('softmax'))  # 每一個one hot都給一個softmax\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練模型/驗證評估\n",
    "我們將進行50次的訓練，並且在每次訓練之後就進行檢查。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T16:36:51.540189Z",
     "start_time": "2019-09-04T16:36:51.535371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 7, 12)\n",
      "(45000, 4, 12)\n",
      "(5000, 7, 12)\n",
      "(5000, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T16:44:03.513043Z",
     "start_time": "2019-09-04T16:40:46.059339Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 8s 185us/step - loss: 1.8910 - acc: 0.3202 - val_loss: 1.7940 - val_acc: 0.3417\n",
      "Q 501+5   T 506  \u001b[91m☒\u001b[0m 11  \n",
      "Q 862+5   T 867  \u001b[91m☒\u001b[0m 101 \n",
      "Q 38+386  T 424  \u001b[91m☒\u001b[0m 108 \n",
      "Q 223+469 T 692  \u001b[91m☒\u001b[0m 103 \n",
      "Q 841+6   T 847  \u001b[91m☒\u001b[0m 10  \n",
      "Q 153+804 T 957  \u001b[91m☒\u001b[0m 108 \n",
      "Q 11+741  T 752  \u001b[91m☒\u001b[0m 101 \n",
      "Q 533+195 T 728  \u001b[91m☒\u001b[0m 108 \n",
      "Q 9+851   T 860  \u001b[91m☒\u001b[0m 101 \n",
      "Q 155+976 T 1131 \u001b[91m☒\u001b[0m 111 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.7381 - acc: 0.3585 - val_loss: 1.6695 - val_acc: 0.3754\n",
      "Q 589+35  T 624  \u001b[91m☒\u001b[0m 900 \n",
      "Q 964+14  T 978  \u001b[91m☒\u001b[0m 104 \n",
      "Q 1+51    T 52   \u001b[91m☒\u001b[0m 11  \n",
      "Q 909+2   T 911  \u001b[91m☒\u001b[0m 100 \n",
      "Q 82+172  T 254  \u001b[91m☒\u001b[0m 332 \n",
      "Q 577+408 T 985  \u001b[91m☒\u001b[0m 104 \n",
      "Q 832+12  T 844  \u001b[91m☒\u001b[0m 332 \n",
      "Q 3+385   T 388  \u001b[91m☒\u001b[0m 332 \n",
      "Q 286+5   T 291  \u001b[91m☒\u001b[0m 660 \n",
      "Q 0+682   T 682  \u001b[91m☒\u001b[0m 172 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 147us/step - loss: 1.5994 - acc: 0.3999 - val_loss: 1.5129 - val_acc: 0.4286\n",
      "Q 71+418  T 489  \u001b[91m☒\u001b[0m 244 \n",
      "Q 738+815 T 1553 \u001b[91m☒\u001b[0m 1419\n",
      "Q 200+51  T 251  \u001b[91m☒\u001b[0m 224 \n",
      "Q 871+250 T 1121 \u001b[91m☒\u001b[0m 1109\n",
      "Q 530+44  T 574  \u001b[91m☒\u001b[0m 544 \n",
      "Q 30+672  T 702  \u001b[91m☒\u001b[0m 700 \n",
      "Q 235+5   T 240  \u001b[91m☒\u001b[0m 244 \n",
      "Q 421+628 T 1049 \u001b[91m☒\u001b[0m 100 \n",
      "Q 40+619  T 659  \u001b[91m☒\u001b[0m 560 \n",
      "Q 866+506 T 1372 \u001b[91m☒\u001b[0m 1219\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 147us/step - loss: 1.4262 - acc: 0.4657 - val_loss: 1.3483 - val_acc: 0.4895\n",
      "Q 22+900  T 922  \u001b[91m☒\u001b[0m 990 \n",
      "Q 696+180 T 876  \u001b[91m☒\u001b[0m 700 \n",
      "Q 93+944  T 1037 \u001b[91m☒\u001b[0m 1032\n",
      "Q 49+172  T 221  \u001b[91m☒\u001b[0m 239 \n",
      "Q 28+775  T 803  \u001b[91m☒\u001b[0m 886 \n",
      "Q 97+517  T 614  \u001b[91m☒\u001b[0m 660 \n",
      "Q 66+30   T 96   \u001b[91m☒\u001b[0m 11  \n",
      "Q 57+968  T 1025 \u001b[91m☒\u001b[0m 105 \n",
      "Q 833+87  T 920  \u001b[91m☒\u001b[0m 832 \n",
      "Q 505+43  T 548  \u001b[91m☒\u001b[0m 560 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 1.2799 - acc: 0.5226 - val_loss: 1.2204 - val_acc: 0.5461\n",
      "Q 896+7   T 903  \u001b[91m☒\u001b[0m 990 \n",
      "Q 469+820 T 1289 \u001b[91m☒\u001b[0m 1299\n",
      "Q 772+39  T 811  \u001b[91m☒\u001b[0m 890 \n",
      "Q 244+58  T 302  \u001b[91m☒\u001b[0m 390 \n",
      "Q 356+79  T 435  \u001b[91m☒\u001b[0m 425 \n",
      "Q 858+74  T 932  \u001b[91m☒\u001b[0m 952 \n",
      "Q 735+2   T 737  \u001b[91m☒\u001b[0m 734 \n",
      "Q 6+378   T 384  \u001b[91m☒\u001b[0m 382 \n",
      "Q 544+45  T 589  \u001b[91m☒\u001b[0m 590 \n",
      "Q 73+63   T 136  \u001b[91m☒\u001b[0m 131 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 147us/step - loss: 1.1564 - acc: 0.5726 - val_loss: 1.0986 - val_acc: 0.5936\n",
      "Q 28+775  T 803  \u001b[91m☒\u001b[0m 894 \n",
      "Q 87+99   T 186  \u001b[91m☒\u001b[0m 183 \n",
      "Q 70+690  T 760  \u001b[91m☒\u001b[0m 744 \n",
      "Q 691+3   T 694  \u001b[91m☒\u001b[0m 690 \n",
      "Q 25+348  T 373  \u001b[91m☒\u001b[0m 385 \n",
      "Q 107+369 T 476  \u001b[91m☒\u001b[0m 512 \n",
      "Q 314+170 T 484  \u001b[91m☒\u001b[0m 485 \n",
      "Q 868+380 T 1248 \u001b[91m☒\u001b[0m 1258\n",
      "Q 475+890 T 1365 \u001b[91m☒\u001b[0m 1355\n",
      "Q 7+422   T 429  \u001b[91m☒\u001b[0m 432 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 147us/step - loss: 1.0501 - acc: 0.6145 - val_loss: 1.0334 - val_acc: 0.6105\n",
      "Q 82+998  T 1080 \u001b[91m☒\u001b[0m 1063\n",
      "Q 362+82  T 444  \u001b[91m☒\u001b[0m 438 \n",
      "Q 91+712  T 803  \u001b[91m☒\u001b[0m 798 \n",
      "Q 32+49   T 81   \u001b[91m☒\u001b[0m 84  \n",
      "Q 730+769 T 1499 \u001b[91m☒\u001b[0m 1490\n",
      "Q 845+5   T 850  \u001b[91m☒\u001b[0m 858 \n",
      "Q 146+18  T 164  \u001b[91m☒\u001b[0m 158 \n",
      "Q 266+9   T 275  \u001b[91m☒\u001b[0m 274 \n",
      "Q 476+2   T 478  \u001b[91m☒\u001b[0m 474 \n",
      "Q 741+977 T 1718 \u001b[91m☒\u001b[0m 1619\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 147us/step - loss: 0.9674 - acc: 0.6497 - val_loss: 0.9375 - val_acc: 0.6600\n",
      "Q 982+88  T 1070 \u001b[91m☒\u001b[0m 1062\n",
      "Q 129+87  T 216  \u001b[92m☑\u001b[0m 216 \n",
      "Q 121+391 T 512  \u001b[91m☒\u001b[0m 513 \n",
      "Q 13+356  T 369  \u001b[92m☑\u001b[0m 369 \n",
      "Q 683+0   T 683  \u001b[91m☒\u001b[0m 689 \n",
      "Q 954+865 T 1819 \u001b[91m☒\u001b[0m 1743\n",
      "Q 96+290  T 386  \u001b[91m☒\u001b[0m 380 \n",
      "Q 827+441 T 1268 \u001b[91m☒\u001b[0m 1265\n",
      "Q 36+47   T 83   \u001b[91m☒\u001b[0m 80  \n",
      "Q 91+991  T 1082 \u001b[92m☑\u001b[0m 1082\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 149us/step - loss: 0.8898 - acc: 0.6807 - val_loss: 0.8630 - val_acc: 0.6872\n",
      "Q 486+728 T 1214 \u001b[91m☒\u001b[0m 1212\n",
      "Q 526+820 T 1346 \u001b[91m☒\u001b[0m 1354\n",
      "Q 27+660  T 687  \u001b[91m☒\u001b[0m 670 \n",
      "Q 549+5   T 554  \u001b[91m☒\u001b[0m 552 \n",
      "Q 283+743 T 1026 \u001b[91m☒\u001b[0m 1040\n",
      "Q 217+9   T 226  \u001b[91m☒\u001b[0m 224 \n",
      "Q 759+197 T 956  \u001b[91m☒\u001b[0m 925 \n",
      "Q 609+16  T 625  \u001b[91m☒\u001b[0m 626 \n",
      "Q 864+94  T 958  \u001b[91m☒\u001b[0m 962 \n",
      "Q 382+59  T 441  \u001b[91m☒\u001b[0m 444 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 149us/step - loss: 0.8190 - acc: 0.7071 - val_loss: 0.7862 - val_acc: 0.7233\n",
      "Q 3+66    T 69   \u001b[91m☒\u001b[0m 68  \n",
      "Q 24+293  T 317  \u001b[91m☒\u001b[0m 314 \n",
      "Q 125+720 T 845  \u001b[91m☒\u001b[0m 842 \n",
      "Q 341+3   T 344  \u001b[91m☒\u001b[0m 345 \n",
      "Q 62+809  T 871  \u001b[91m☒\u001b[0m 872 \n",
      "Q 653+901 T 1554 \u001b[91m☒\u001b[0m 1565\n",
      "Q 820+984 T 1804 \u001b[91m☒\u001b[0m 1812\n",
      "Q 26+46   T 72   \u001b[91m☒\u001b[0m 78  \n",
      "Q 330+48  T 378  \u001b[92m☑\u001b[0m 378 \n",
      "Q 332+85  T 417  \u001b[91m☒\u001b[0m 410 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 11\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 147us/step - loss: 0.7498 - acc: 0.7357 - val_loss: 0.7242 - val_acc: 0.7428\n",
      "Q 553+8   T 561  \u001b[92m☑\u001b[0m 561 \n",
      "Q 742+74  T 816  \u001b[92m☑\u001b[0m 816 \n",
      "Q 438+12  T 450  \u001b[91m☒\u001b[0m 448 \n",
      "Q 39+607  T 646  \u001b[92m☑\u001b[0m 646 \n",
      "Q 200+25  T 225  \u001b[91m☒\u001b[0m 228 \n",
      "Q 3+385   T 388  \u001b[92m☑\u001b[0m 388 \n",
      "Q 709+166 T 875  \u001b[91m☒\u001b[0m 878 \n",
      "Q 796+9   T 805  \u001b[91m☒\u001b[0m 806 \n",
      "Q 150+93  T 243  \u001b[91m☒\u001b[0m 241 \n",
      "Q 61+59   T 120  \u001b[91m☒\u001b[0m 111 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 12\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 147us/step - loss: 0.6748 - acc: 0.7620 - val_loss: 0.6393 - val_acc: 0.7658\n",
      "Q 15+686  T 701  \u001b[91m☒\u001b[0m 700 \n",
      "Q 21+879  T 900  \u001b[91m☒\u001b[0m 898 \n",
      "Q 231+716 T 947  \u001b[91m☒\u001b[0m 945 \n",
      "Q 9+908   T 917  \u001b[92m☑\u001b[0m 917 \n",
      "Q 462+35  T 497  \u001b[91m☒\u001b[0m 596 \n",
      "Q 310+37  T 347  \u001b[91m☒\u001b[0m 344 \n",
      "Q 26+711  T 737  \u001b[91m☒\u001b[0m 736 \n",
      "Q 44+234  T 278  \u001b[91m☒\u001b[0m 277 \n",
      "Q 164+81  T 245  \u001b[92m☑\u001b[0m 245 \n",
      "Q 881+605 T 1486 \u001b[91m☒\u001b[0m 1485\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 13\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 146us/step - loss: 0.5262 - acc: 0.8116 - val_loss: 0.4348 - val_acc: 0.8452\n",
      "Q 194+540 T 734  \u001b[92m☑\u001b[0m 734 \n",
      "Q 443+6   T 449  \u001b[91m☒\u001b[0m 459 \n",
      "Q 613+99  T 712  \u001b[91m☒\u001b[0m 711 \n",
      "Q 106+449 T 555  \u001b[91m☒\u001b[0m 553 \n",
      "Q 826+336 T 1162 \u001b[91m☒\u001b[0m 1161\n",
      "Q 410+8   T 418  \u001b[92m☑\u001b[0m 418 \n",
      "Q 507+22  T 529  \u001b[92m☑\u001b[0m 529 \n",
      "Q 85+774  T 859  \u001b[91m☒\u001b[0m 860 \n",
      "Q 740+14  T 754  \u001b[92m☑\u001b[0m 754 \n",
      "Q 269+23  T 292  \u001b[92m☑\u001b[0m 292 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 147us/step - loss: 0.3588 - acc: 0.8887 - val_loss: 0.2983 - val_acc: 0.9185\n",
      "Q 1+47    T 48   \u001b[91m☒\u001b[0m 47  \n",
      "Q 128+34  T 162  \u001b[92m☑\u001b[0m 162 \n",
      "Q 579+794 T 1373 \u001b[91m☒\u001b[0m 1372\n",
      "Q 29+35   T 64   \u001b[91m☒\u001b[0m 63  \n",
      "Q 643+60  T 703  \u001b[92m☑\u001b[0m 703 \n",
      "Q 150+93  T 243  \u001b[92m☑\u001b[0m 243 \n",
      "Q 47+35   T 82   \u001b[92m☑\u001b[0m 82  \n",
      "Q 591+140 T 731  \u001b[91m☒\u001b[0m 732 \n",
      "Q 215+344 T 559  \u001b[91m☒\u001b[0m 569 \n",
      "Q 944+8   T 952  \u001b[92m☑\u001b[0m 952 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 15\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 7s 150us/step - loss: 0.2509 - acc: 0.9393 - val_loss: 0.2201 - val_acc: 0.9477\n",
      "Q 700+435 T 1135 \u001b[91m☒\u001b[0m 1145\n",
      "Q 41+491  T 532  \u001b[92m☑\u001b[0m 532 \n",
      "Q 6+367   T 373  \u001b[92m☑\u001b[0m 373 \n",
      "Q 870+7   T 877  \u001b[92m☑\u001b[0m 877 \n",
      "Q 173+178 T 351  \u001b[91m☒\u001b[0m 341 \n",
      "Q 522+359 T 881  \u001b[92m☑\u001b[0m 881 \n",
      "Q 215+344 T 559  \u001b[91m☒\u001b[0m 569 \n",
      "Q 609+16  T 625  \u001b[92m☑\u001b[0m 625 \n",
      "Q 447+924 T 1371 \u001b[92m☑\u001b[0m 1371\n",
      "Q 545+63  T 608  \u001b[92m☑\u001b[0m 608 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 16\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 0.1781 - acc: 0.9644 - val_loss: 0.1681 - val_acc: 0.9597\n",
      "Q 114+943 T 1057 \u001b[91m☒\u001b[0m 1056\n",
      "Q 37+575  T 612  \u001b[92m☑\u001b[0m 612 \n",
      "Q 73+621  T 694  \u001b[92m☑\u001b[0m 694 \n",
      "Q 571+786 T 1357 \u001b[92m☑\u001b[0m 1357\n",
      "Q 908+52  T 960  \u001b[92m☑\u001b[0m 960 \n",
      "Q 672+51  T 723  \u001b[92m☑\u001b[0m 723 \n",
      "Q 59+542  T 601  \u001b[91m☒\u001b[0m 501 \n",
      "Q 741+42  T 783  \u001b[92m☑\u001b[0m 783 \n",
      "Q 60+377  T 437  \u001b[92m☑\u001b[0m 437 \n",
      "Q 747+23  T 770  \u001b[92m☑\u001b[0m 770 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 17\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 147us/step - loss: 0.1310 - acc: 0.9759 - val_loss: 0.1229 - val_acc: 0.9746\n",
      "Q 51+526  T 577  \u001b[92m☑\u001b[0m 577 \n",
      "Q 989+764 T 1753 \u001b[92m☑\u001b[0m 1753\n",
      "Q 3+562   T 565  \u001b[92m☑\u001b[0m 565 \n",
      "Q 486+939 T 1425 \u001b[92m☑\u001b[0m 1425\n",
      "Q 62+860  T 922  \u001b[92m☑\u001b[0m 922 \n",
      "Q 18+9    T 27   \u001b[91m☒\u001b[0m 28  \n",
      "Q 31+34   T 65   \u001b[92m☑\u001b[0m 65  \n",
      "Q 645+663 T 1308 \u001b[92m☑\u001b[0m 1308\n",
      "Q 632+29  T 661  \u001b[92m☑\u001b[0m 661 \n",
      "Q 848+38  T 886  \u001b[92m☑\u001b[0m 886 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 18\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 0.0976 - acc: 0.9838 - val_loss: 0.0921 - val_acc: 0.9831\n",
      "Q 77+333  T 410  \u001b[92m☑\u001b[0m 410 \n",
      "Q 354+48  T 402  \u001b[92m☑\u001b[0m 402 \n",
      "Q 29+35   T 64   \u001b[92m☑\u001b[0m 64  \n",
      "Q 375+115 T 490  \u001b[92m☑\u001b[0m 490 \n",
      "Q 49+11   T 60   \u001b[91m☒\u001b[0m 51  \n",
      "Q 40+619  T 659  \u001b[92m☑\u001b[0m 659 \n",
      "Q 87+167  T 254  \u001b[92m☑\u001b[0m 254 \n",
      "Q 5+492   T 497  \u001b[92m☑\u001b[0m 497 \n",
      "Q 53+92   T 145  \u001b[92m☑\u001b[0m 145 \n",
      "Q 218+836 T 1054 \u001b[92m☑\u001b[0m 1054\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 19\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 0.0872 - acc: 0.9833 - val_loss: 0.0706 - val_acc: 0.9889\n",
      "Q 620+38  T 658  \u001b[92m☑\u001b[0m 658 \n",
      "Q 411+27  T 438  \u001b[92m☑\u001b[0m 438 \n",
      "Q 71+97   T 168  \u001b[92m☑\u001b[0m 168 \n",
      "Q 292+31  T 323  \u001b[92m☑\u001b[0m 323 \n",
      "Q 391+528 T 919  \u001b[92m☑\u001b[0m 919 \n",
      "Q 520+893 T 1413 \u001b[92m☑\u001b[0m 1413\n",
      "Q 680+883 T 1563 \u001b[92m☑\u001b[0m 1563\n",
      "Q 26+410  T 436  \u001b[92m☑\u001b[0m 436 \n",
      "Q 96+157  T 253  \u001b[92m☑\u001b[0m 253 \n",
      "Q 70+178  T 248  \u001b[92m☑\u001b[0m 248 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 20\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 0.0615 - acc: 0.9908 - val_loss: 0.0709 - val_acc: 0.9847\n",
      "Q 307+3   T 310  \u001b[92m☑\u001b[0m 310 \n",
      "Q 859+879 T 1738 \u001b[92m☑\u001b[0m 1738\n",
      "Q 778+845 T 1623 \u001b[92m☑\u001b[0m 1623\n",
      "Q 998+733 T 1731 \u001b[92m☑\u001b[0m 1731\n",
      "Q 114+593 T 707  \u001b[92m☑\u001b[0m 707 \n",
      "Q 594+50  T 644  \u001b[92m☑\u001b[0m 644 \n",
      "Q 56+955  T 1011 \u001b[92m☑\u001b[0m 1011\n",
      "Q 0+381   T 381  \u001b[92m☑\u001b[0m 381 \n",
      "Q 67+152  T 219  \u001b[92m☑\u001b[0m 219 \n",
      "Q 187+874 T 1061 \u001b[92m☑\u001b[0m 1061\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 21\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 0.0588 - acc: 0.9896 - val_loss: 0.0500 - val_acc: 0.9916\n",
      "Q 2+616   T 618  \u001b[92m☑\u001b[0m 618 \n",
      "Q 17+687  T 704  \u001b[92m☑\u001b[0m 704 \n",
      "Q 60+939  T 999  \u001b[91m☒\u001b[0m 1999\n",
      "Q 63+6    T 69   \u001b[91m☒\u001b[0m 79  \n",
      "Q 99+457  T 556  \u001b[92m☑\u001b[0m 556 \n",
      "Q 878+542 T 1420 \u001b[92m☑\u001b[0m 1420\n",
      "Q 445+84  T 529  \u001b[92m☑\u001b[0m 529 \n",
      "Q 756+217 T 973  \u001b[92m☑\u001b[0m 973 \n",
      "Q 890+32  T 922  \u001b[92m☑\u001b[0m 922 \n",
      "Q 731+21  T 752  \u001b[92m☑\u001b[0m 752 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 22\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 149us/step - loss: 0.0504 - acc: 0.9907 - val_loss: 0.1124 - val_acc: 0.9634\n",
      "Q 77+333  T 410  \u001b[91m☒\u001b[0m 400 \n",
      "Q 1+817   T 818  \u001b[92m☑\u001b[0m 818 \n",
      "Q 848+39  T 887  \u001b[92m☑\u001b[0m 887 \n",
      "Q 4+892   T 896  \u001b[92m☑\u001b[0m 896 \n",
      "Q 2+986   T 988  \u001b[92m☑\u001b[0m 988 \n",
      "Q 38+555  T 593  \u001b[92m☑\u001b[0m 593 \n",
      "Q 76+67   T 143  \u001b[92m☑\u001b[0m 143 \n",
      "Q 944+96  T 1040 \u001b[92m☑\u001b[0m 1040\n",
      "Q 172+32  T 204  \u001b[92m☑\u001b[0m 204 \n",
      "Q 889+62  T 951  \u001b[92m☑\u001b[0m 951 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 23\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 150us/step - loss: 0.0413 - acc: 0.9928 - val_loss: 0.0385 - val_acc: 0.9924\n",
      "Q 51+519  T 570  \u001b[92m☑\u001b[0m 570 \n",
      "Q 496+20  T 516  \u001b[92m☑\u001b[0m 516 \n",
      "Q 647+472 T 1119 \u001b[92m☑\u001b[0m 1119\n",
      "Q 32+752  T 784  \u001b[92m☑\u001b[0m 784 \n",
      "Q 147+55  T 202  \u001b[92m☑\u001b[0m 202 \n",
      "Q 881+3   T 884  \u001b[92m☑\u001b[0m 884 \n",
      "Q 55+906  T 961  \u001b[92m☑\u001b[0m 961 \n",
      "Q 357+251 T 608  \u001b[92m☑\u001b[0m 608 \n",
      "Q 741+609 T 1350 \u001b[92m☑\u001b[0m 1350\n",
      "Q 53+739  T 792  \u001b[92m☑\u001b[0m 792 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 24\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 0.0525 - acc: 0.9875 - val_loss: 0.0425 - val_acc: 0.9907\n",
      "Q 153+619 T 772  \u001b[92m☑\u001b[0m 772 \n",
      "Q 247+559 T 806  \u001b[92m☑\u001b[0m 806 \n",
      "Q 5+724   T 729  \u001b[92m☑\u001b[0m 729 \n",
      "Q 2+349   T 351  \u001b[92m☑\u001b[0m 351 \n",
      "Q 43+91   T 134  \u001b[92m☑\u001b[0m 134 \n",
      "Q 3+67    T 70   \u001b[92m☑\u001b[0m 70  \n",
      "Q 26+291  T 317  \u001b[92m☑\u001b[0m 317 \n",
      "Q 55+380  T 435  \u001b[92m☑\u001b[0m 435 \n",
      "Q 73+63   T 136  \u001b[92m☑\u001b[0m 136 \n",
      "Q 78+175  T 253  \u001b[92m☑\u001b[0m 253 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 25\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 0.0248 - acc: 0.9974 - val_loss: 0.0268 - val_acc: 0.9954\n",
      "Q 167+179 T 346  \u001b[92m☑\u001b[0m 346 \n",
      "Q 482+37  T 519  \u001b[92m☑\u001b[0m 519 \n",
      "Q 253+20  T 273  \u001b[92m☑\u001b[0m 273 \n",
      "Q 592+526 T 1118 \u001b[92m☑\u001b[0m 1118\n",
      "Q 22+693  T 715  \u001b[92m☑\u001b[0m 715 \n",
      "Q 67+538  T 605  \u001b[92m☑\u001b[0m 605 \n",
      "Q 988+383 T 1371 \u001b[92m☑\u001b[0m 1371\n",
      "Q 218+89  T 307  \u001b[92m☑\u001b[0m 307 \n",
      "Q 609+289 T 898  \u001b[92m☑\u001b[0m 898 \n",
      "Q 245+69  T 314  \u001b[92m☑\u001b[0m 314 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 26\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 150us/step - loss: 0.0209 - acc: 0.9976 - val_loss: 0.0256 - val_acc: 0.9951\n",
      "Q 947+8   T 955  \u001b[92m☑\u001b[0m 955 \n",
      "Q 78+206  T 284  \u001b[92m☑\u001b[0m 284 \n",
      "Q 4+676   T 680  \u001b[92m☑\u001b[0m 680 \n",
      "Q 221+75  T 296  \u001b[92m☑\u001b[0m 296 \n",
      "Q 533+0   T 533  \u001b[92m☑\u001b[0m 533 \n",
      "Q 62+98   T 160  \u001b[92m☑\u001b[0m 160 \n",
      "Q 74+266  T 340  \u001b[92m☑\u001b[0m 340 \n",
      "Q 676+500 T 1176 \u001b[92m☑\u001b[0m 1176\n",
      "Q 881+3   T 884  \u001b[92m☑\u001b[0m 884 \n",
      "Q 8+842   T 850  \u001b[92m☑\u001b[0m 850 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 27\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 0.0358 - acc: 0.9917 - val_loss: 0.0244 - val_acc: 0.9952\n",
      "Q 136+2   T 138  \u001b[92m☑\u001b[0m 138 \n",
      "Q 56+388  T 444  \u001b[92m☑\u001b[0m 444 \n",
      "Q 82+976  T 1058 \u001b[92m☑\u001b[0m 1058\n",
      "Q 0+78    T 78   \u001b[92m☑\u001b[0m 78  \n",
      "Q 351+1   T 352  \u001b[92m☑\u001b[0m 352 \n",
      "Q 918+0   T 918  \u001b[92m☑\u001b[0m 918 \n",
      "Q 357+534 T 891  \u001b[92m☑\u001b[0m 891 \n",
      "Q 99+143  T 242  \u001b[92m☑\u001b[0m 242 \n",
      "Q 234+505 T 739  \u001b[92m☑\u001b[0m 739 \n",
      "Q 68+736  T 804  \u001b[92m☑\u001b[0m 804 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 28\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 148us/step - loss: 0.0149 - acc: 0.9988 - val_loss: 0.0240 - val_acc: 0.9944\n",
      "Q 52+270  T 322  \u001b[92m☑\u001b[0m 322 \n",
      "Q 62+283  T 345  \u001b[92m☑\u001b[0m 345 \n",
      "Q 870+7   T 877  \u001b[92m☑\u001b[0m 877 \n",
      "Q 309+113 T 422  \u001b[92m☑\u001b[0m 422 \n",
      "Q 175+5   T 180  \u001b[92m☑\u001b[0m 180 \n",
      "Q 91+33   T 124  \u001b[92m☑\u001b[0m 124 \n",
      "Q 712+66  T 778  \u001b[92m☑\u001b[0m 778 \n",
      "Q 213+482 T 695  \u001b[92m☑\u001b[0m 695 \n",
      "Q 362+3   T 365  \u001b[92m☑\u001b[0m 365 \n",
      "Q 514+468 T 982  \u001b[92m☑\u001b[0m 982 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 29\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 147us/step - loss: 0.0197 - acc: 0.9966 - val_loss: 0.0527 - val_acc: 0.9832\n",
      "Q 746+51  T 797  \u001b[92m☑\u001b[0m 797 \n",
      "Q 174+740 T 914  \u001b[92m☑\u001b[0m 914 \n",
      "Q 51+16   T 67   \u001b[92m☑\u001b[0m 67  \n",
      "Q 28+382  T 410  \u001b[92m☑\u001b[0m 410 \n",
      "Q 8+786   T 794  \u001b[92m☑\u001b[0m 794 \n",
      "Q 362+752 T 1114 \u001b[92m☑\u001b[0m 1114\n",
      "Q 908+778 T 1686 \u001b[92m☑\u001b[0m 1686\n",
      "Q 2+532   T 534  \u001b[92m☑\u001b[0m 534 \n",
      "Q 120+98  T 218  \u001b[92m☑\u001b[0m 218 \n",
      "Q 137+80  T 217  \u001b[92m☑\u001b[0m 217 \n"
     ]
    }
   ],
   "source": [
    "for iteration in range(1, 30):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(x_train, y_train,\n",
    "             batch_size=BATCH_SIZE,\n",
    "             epochs=1,\n",
    "             validation_data=(x_val, y_val))\n",
    "    \n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict_classes(rowx, verbose=0)\n",
    "        \n",
    "        q = ctable.decode(rowx[0]) # question: x\n",
    "        correct = ctable.decode(rowy[0]) # answer: y\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print('Q:', q[::-1] if INVERT else q, end=' ')  # 印出來的時候要倒回來\n",
    "        print('A:', correct, end=' ')\n",
    "        if correct == guess:\n",
    "            print(colors.ok + '☑' + colors.close, end=' ')\n",
    "        else:\n",
    "            print(colors.fail + '☒' + colors.close, end=' ')\n",
    "        print(guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們可以看到在30次的訓練循環之後,我們己經可以在驗證準確性上達到99.8%的程度。\n",
    "\n",
    "以上方法的一個先行條件是它假設:給定固定長度的序列當輸入[... t]有可能生成固定長度的目標[...t]序列。\n",
    "\n",
    "這在某些情況下可行，但不適用於大多數使用情境。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一般情境：序列到序列(seq-to-seq)的典型範例\n",
    "在一般情況下，輸入序列和輸出序列具有不同的長度（例如機器翻譯），並且為了開始預測目標，需要整個輸入序列。這需要更高級的設置，這是人們在沒有更多的上下文的情況下提到“序列到序列模型”時經常提到的。這是如何工作的：\n",
    "\n",
    "1. RNN層（或多個RNN層的堆疊）作為“編碼器(encoder)”：它處理輸入序列並返回其自身的內部狀態。請注意，我們丟棄編碼器RNN的輸出，只保留它的內部狀態。這個狀態將作為下一步解碼器的“上下文”或“條件”。\n",
    "2. 另一個RNN層（或多個RNN層的堆疊）充當“解碼器(decoder)”：對給定的目標序列的先前字符進行訓練，以預測目標序列的下一個字符。具體而言，訓練是將目標序列轉換成相同的序列偏移(offset)一個步驟的過程，這種情況稱為“教師強制(teacher forcing)”的訓練過程。重要的是，解碼器(decoder)使用來自編碼器(encoder)的狀態向量作為初始狀態，這是解碼器如何獲得關於它應該產生何種產出的關鍵資訊。實際上，解碼器學習以輸入序列為條件生成給定目標[... t]的目標[t + 1 ...]。\n",
    "\n",
    "![title](./seq2se1_1.PNG)\n",
    "\n",
    "在預測模式下，當我們想要解碼(decode)未知的輸入序列時，我們經歷一個稍微不同的過程：\n",
    "\n",
    "1. 將輸入序列編碼成狀態向量(hidden state vector)。\n",
    "2. 從大小為1的目標序列開始（只是開始序列字符）。\n",
    "3. 將狀態向量和1-char目標序列饋送給解碼器以產生下一個字符的預測。\n",
    "4. 使用這些預測對下一個字符進行採樣（我們簡單地使用argmax）。\n",
    "5. 將採樣的字符附加到目標序列。\n",
    "6. 重複，直到我們拿到生成序列結束字符或我們達到字符限制。\n",
    "\n",
    "![title](./seq2se1_2.PNG)\n",
    "\n",
    "\n",
    "也可以使用相同的過程來訓練Seq2Seq網絡，而不需要“教師強制”，即通過將解碼器的預測重新輸入到解碼器中。\n",
    "\n",
    "讓我們用實際的程式碼來說明這些想法。\n",
    "\n",
    "為了實現我們的範例，我們將使用英語句子對應的中文語句翻譯的數據集，您可以從<a href=\"http://www.manythings.org/anki/\">[manythings.org/anki]下載這些數據集。</a> 要下載的文件被稱為cmn-eng.zip(簡中對應到英文)。為了更貼近學習的效果, 我己經把簡中轉成了繁中的版本（cmn-tw.txt），可以從<a href=\"https://github.com/erhwenkuo/deep-learning-with-keras-notebooks/blob/master/assets/data/cmn-tw.txt\">Github</a>上取得這個資料檔。我們將實現一個字符級(character-level)的序列到序列模型，逐個字符地處理輸入，並逐個字符地產生輸出。另一個選擇是一個字級(word-level)模型，這個模型往往是機器翻譯更常見的。在這篇文章的最後，你會發現一些關於使用嵌入圖層(embedding layers)將我們的模型轉換為字級模型的參考連結。\n",
    "\n",
    "### 資料準備\n",
    "從<a href=\"https://github.com/erhwenkuo/deep-learning-with-keras-notebooks/blob/master/assets/data/cmn-tw.txt\">Github</a>下載cmn-tw.txt檔案。\n",
    "在這個Jupyter Notebook所在的目錄下產生一個新的子目錄\"data\"。\n",
    "把下載的資料檔複製到\"data\"的目錄裡頭。\n",
    "最後你的目錄結構看起來像這樣:\n",
    "\n",
    "\n",
    "xxx.ipynb\n",
    "data/   \n",
    "└── cmn-tw.txt\n",
    "\n",
    "\n",
    "以下是我們的流程總結：\n",
    "\n",
    "1. 將句子(sentence)轉換為3個Numpy數組, encoder_input_data, decoder_input_data, decoder_target_data：\n",
    "\n",
    " - encoder_input_data是包含英文句子的one-hot向量化的三維形狀數組（num_pairs, max_english_sentence_length, num_english_characters）。\n",
    " - decoder_input_data是包含中文句子的one-hot向量化的三維形狀數組（num_pairs, max_chinese_sentence_length, num_chinese_characters）。\n",
    " - decoder_target_data與decoder_input_data相同，但是偏移了一個時間步長。 decoder_target_data [:,t,：]將與decoder_input_data [：,t+1,：]相同。\n",
    "2. 訓練一個基本的基於LSTM的Seq2Seq模型來預測給出encoder_input_data和decoder_input_data的decoder_target_data。我們的模型使用教師強制(teacher forcing)的手法。\n",
    "\n",
    "3. 解碼一些句子以檢查模型是否正常工作（將來自encoder_input_data的樣本轉換為來自decoder_target_data的對應樣本）。\n",
    "\n",
    "整個網絡的架構構建可以參考以下的圖示:\n",
    "\n",
    "![title](./seq2se1_MT.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引入相關的函數庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:08:05.170403Z",
     "start_time": "2019-09-08T15:08:03.399043Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 專案的根目錄路徑\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# 置放訓練資料的目錄\n",
    "DATA_PATH = os.path.join(ROOT_DIR, \"data\")\n",
    "\n",
    "# 訓練資料檔\n",
    "DATA_FILE = os.path.join(DATA_PATH, \"cmn-tw.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:45:05.375160Z",
     "start_time": "2019-09-05T16:45:05.370706Z"
    }
   },
   "source": [
    "###  相關的參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:08:17.717998Z",
     "start_time": "2019-09-08T15:08:17.713311Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64 # 訓練時的批次數量\n",
    "epochs = 100 # 訓練循環數\n",
    "latent_dim = 256 # 編碼後的潛在空間的維度(dimensions of latent space)\n",
    "num_samples = 10000 # 用來訓練的樣本數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料的前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:10:43.968887Z",
     "start_time": "2019-09-08T15:10:43.964099Z"
    }
   },
   "outputs": [],
   "source": [
    "# 資料向量化\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set() # 英文字符集\n",
    "target_characters = set() # 中文字符集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:10:45.995564Z",
     "start_time": "2019-09-08T15:10:45.968469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi.\\t嗨。',\n",
       " 'Hi.\\t你好。',\n",
       " 'Run.\\t你用跑的。',\n",
       " 'Wait!\\t等等！',\n",
       " 'Hello!\\t你好。',\n",
       " 'I try.\\t讓我來。',\n",
       " 'I won!\\t我贏了。',\n",
       " 'Oh no!\\t不會吧。',\n",
       " 'Cheers!\\t乾杯!',\n",
       " 'He ran.\\t他跑了。',\n",
       " 'Hop in.\\t跳進來。',\n",
       " 'I lost.\\t我迷失了。',\n",
       " 'I quit.\\t我退出。',\n",
       " \"I'm OK.\\t我沒事。\",\n",
       " 'Listen.\\t聽著。',\n",
       " 'No way!\\t不可能！',\n",
       " 'No way!\\t沒門！',\n",
       " 'Really?\\t你確定？',\n",
       " 'Try it.\\t試試吧。',\n",
       " 'We try.\\t我們來試試。',\n",
       " 'Why me?\\t為什麼是我？',\n",
       " 'Ask Tom.\\t去問湯姆。',\n",
       " 'Be calm.\\t冷靜點。',\n",
       " 'Be fair.\\t公平點。',\n",
       " 'Be kind.\\t友善點。',\n",
       " 'Be nice.\\t和氣點。',\n",
       " 'Call me.\\t聯繫我。',\n",
       " 'Call us.\\t聯繫我們。',\n",
       " 'Come in.\\t進來。',\n",
       " 'Get Tom.\\t找到湯姆。',\n",
       " 'Get out!\\t滾出去！',\n",
       " 'Go away!\\t走開！',\n",
       " 'Go away!\\t滾！',\n",
       " 'Go away.\\t走開！',\n",
       " 'Goodbye!\\t再見！',\n",
       " 'Goodbye!\\t告辭！',\n",
       " 'Hang on!\\t等一下！',\n",
       " 'He came.\\t他來了。',\n",
       " 'He runs.\\t他跑。',\n",
       " 'Help me.\\t幫我一下。',\n",
       " 'Hold on.\\t堅持。',\n",
       " 'Hug Tom.\\t抱抱湯姆！',\n",
       " 'I agree.\\t我同意。',\n",
       " \"I'm ill.\\t我生病了。\",\n",
       " \"I'm old.\\t我老了。\",\n",
       " \"It's OK.\\t沒關係。\",\n",
       " \"It's me.\\t是我。\",\n",
       " 'Join us.\\t來加入我們吧。',\n",
       " 'Keep it.\\t留著吧。',\n",
       " 'Kiss me.\\t吻我。',\n",
       " 'Perfect!\\t完美！',\n",
       " 'See you.\\t再見！',\n",
       " 'Shut up!\\t閉嘴！',\n",
       " 'Skip it.\\t不管它。',\n",
       " 'Take it.\\t拿走吧。',\n",
       " 'Wake up!\\t醒醒！',\n",
       " 'Wash up.\\t去清洗一下。',\n",
       " 'We know.\\t我們知道。',\n",
       " 'Welcome.\\t歡迎。',\n",
       " 'Who won?\\t誰贏了？',\n",
       " 'Why not?\\t為什麼不？',\n",
       " 'You run.\\t你跑。',\n",
       " 'Back off.\\t往後退點。',\n",
       " 'Be still.\\t靜靜的，別動。',\n",
       " 'Cuff him.\\t把他銬上。',\n",
       " 'Drive on.\\t往前開。',\n",
       " 'Get away!\\t走開！',\n",
       " 'Get away!\\t滾！',\n",
       " 'Get down!\\t趴下！',\n",
       " 'Get lost!\\t滾！',\n",
       " 'Get real.\\t醒醒吧。',\n",
       " 'Grab Tom.\\t抓住湯姆。',\n",
       " 'Grab him.\\t抓住他。',\n",
       " 'Have fun.\\t玩得開心。',\n",
       " 'He tries.\\t他來試試。',\n",
       " 'Humor me.\\t你就隨了我的意吧。',\n",
       " 'Hurry up.\\t趕快!',\n",
       " 'Hurry up.\\t快點！',\n",
       " 'I forgot.\\t我忘了。',\n",
       " 'I resign.\\t我放棄。',\n",
       " \"I'll pay.\\t我來付錢。\",\n",
       " \"I'm busy.\\t我很忙。\",\n",
       " \"I'm cold.\\t我冷。\",\n",
       " \"I'm fine.\\t我很好。\",\n",
       " \"I'm full.\\t我吃飽了。\",\n",
       " \"I'm sick.\\t我生病了。\",\n",
       " \"I'm sick.\\t我病了。\",\n",
       " 'Leave me.\\t讓我一個人呆會兒。',\n",
       " \"Let's go!\\t走吧。\",\n",
       " \"Let's go!\\t我們開始吧！\",\n",
       " \"Let's go!\\t我們走吧!\",\n",
       " 'Look out!\\t當心！',\n",
       " 'She runs.\\t她跑。',\n",
       " 'Stand up.\\t起立。',\n",
       " 'They won.\\t他們贏了。',\n",
       " 'Tom died.\\t湯姆去世了。',\n",
       " 'Tom quit.\\t湯姆不干了。',\n",
       " 'Tom swam.\\t湯姆游泳了。',\n",
       " 'Trust me.\\t相信我。',\n",
       " 'Try hard.\\t努力。',\n",
       " 'Try some.\\t試試吧。',\n",
       " 'Who died?\\t誰死了？',\n",
       " 'Birds fly.\\t鳥類飛行。',\n",
       " 'Call home!\\t打電話回家！',\n",
       " 'Catch him.\\t抓住他。',\n",
       " 'Come home.\\t回家吧。',\n",
       " 'Do it now.\\t現在就做。',\n",
       " 'Dogs bark.\\t狗會叫。',\n",
       " \"Don't cry.\\t別哭。\",\n",
       " 'Excuse me.\\t對不起。',\n",
       " 'Feel this.\\t來感受一下這個。',\n",
       " 'Follow me.\\t請跟我來。',\n",
       " 'Follow us.\\t請跟著我們。',\n",
       " 'Good luck.\\t祝你好運。',\n",
       " 'Grab that.\\t抓住那個。',\n",
       " 'Grab this.\\t抓住這個。',\n",
       " 'Hands off.\\t手舉起來。',\n",
       " \"He's a DJ.\\t他是一個 DJ 。\",\n",
       " \"He's lazy.\\t他很懶。\",\n",
       " 'Hold fire.\\t停火。',\n",
       " 'Hold this.\\t我住這個。',\n",
       " 'How awful!\\t太可怕了。',\n",
       " 'I am cold.\\t我冷。',\n",
       " 'I am okay.\\t我沒事。',\n",
       " 'I am sick.\\t我生病了。',\n",
       " 'I get you.\\t我了解你。',\n",
       " 'I hope so.\\t我希望如此。',\n",
       " 'I laughed.\\t我笑了。',\n",
       " 'I promise.\\t我向你保證。',\n",
       " \"I'm a man.\\t我是個男人。\",\n",
       " \"I'm right.\\t我是對的。\",\n",
       " \"I'm sorry.\\t對不起。\",\n",
       " \"I'm sorry.\\t我很抱歉。\",\n",
       " \"I'm young.\\t我還年輕。\",\n",
       " 'It snowed.\\t下雪了。',\n",
       " \"It's 3:30.\\t3點半了。\",\n",
       " \"It's cold.\\t天很冷。\",\n",
       " \"It's free.\\t它是免費的。\",\n",
       " \"It's late.\\t很晚了。\",\n",
       " \"It's true.\\t這是真的。\",\n",
       " 'Let me in.\\t讓我進去。',\n",
       " 'Lie still.\\t躺著不動。',\n",
       " 'Look back!\\t回頭看！',\n",
       " 'Move over.\\t騰一下地方。',\n",
       " 'Of course!\\t當然!',\n",
       " 'Of course.\\t當然。',\n",
       " 'Oh please!\\t噢拜託了！',\n",
       " 'Open fire!\\t開火！',\n",
       " 'Read this.\\t念這個。',\n",
       " 'See above.\\t參見上文。',\n",
       " 'She cried.\\t她哭了。',\n",
       " 'She tried.\\t她試過了。',\n",
       " 'She walks.\\t她在行走。',\n",
       " 'Sit tight.\\t耐心等著。',\n",
       " 'Slow down.\\t慢一點。',\n",
       " 'Stay calm.\\t保持冷靜。',\n",
       " 'Stay down!\\t趴著！',\n",
       " 'Stop that!\\t住手。',\n",
       " 'Take care!\\t照顧好自己。',\n",
       " 'Take care.\\t照顧好自己。',\n",
       " 'Tom swims.\\tTom游泳。',\n",
       " 'Tom tried.\\t湯姆累了。',\n",
       " 'Tom waved.\\t湯姆揮手了。',\n",
       " 'Turn left.\\t向左轉。',\n",
       " 'Wait here.\\t在這等著。',\n",
       " 'Well done!\\t做得好！',\n",
       " 'Who cares?\\t愛誰誰。',\n",
       " 'Wonderful!\\t很棒！',\n",
       " 'You idiot!\\t蠢貨！',\n",
       " 'All aboard!\\t請上船！',\n",
       " 'Am I wrong?\\t我錯了嗎？',\n",
       " 'Birds sing.\\t鳥兒歌唱。',\n",
       " 'Can I help?\\t我可以幫忙嗎?',\n",
       " 'Come along.\\t快點。',\n",
       " 'Definitely!\\t必須的！',\n",
       " \"Don't move.\\t不要動。\",\n",
       " 'Fill it up.\\t把它填滿。',\n",
       " 'Follow him.\\t跟著他走。',\n",
       " 'God exists.\\t上帝存在。',\n",
       " 'Good night.\\t晚安。',\n",
       " 'He gave in.\\t他讓步了.',\n",
       " 'He is mean.\\t他很兇。',\n",
       " 'He is poor.\\t他很窮。',\n",
       " 'He is tall.\\t他高。',\n",
       " '\"Hello, Tom.\"\\t你好，湯姆。',\n",
       " '\"Hey, relax.\"\\t嘿，放鬆點。',\n",
       " 'How lovely!\\t多可愛啊！',\n",
       " 'Hurry home.\\t趕快回家。',\n",
       " 'I am a man.\\t我是個男人。',\n",
       " 'I am short.\\t我個頭矮。',\n",
       " 'I can cook.\\t我會做飯。',\n",
       " 'I can swim.\\t我會游泳。',\n",
       " 'I eat here.\\t我在這裡吃。',\n",
       " 'I envy her.\\t我嫉妒她。',\n",
       " 'I envy him.\\t我羨慕他。',\n",
       " 'I hate you.\\t我恨你。',\n",
       " 'I know Tom.\\t我認識湯姆。',\n",
       " 'I know him.\\t我認識他。',\n",
       " 'I like tea.\\t我喜歡茶。',\n",
       " 'I like you.\\t我喜歡你。',\n",
       " 'I like you.\\t我喜歡你。',\n",
       " 'I love her.\\t我愛她。',\n",
       " 'I love you.\\t我愛您。',\n",
       " 'I miss you.\\t我想念你。',\n",
       " 'I need you.\\t我需要你。',\n",
       " 'I think so.\\t我想是這樣的。',\n",
       " 'I use this.\\t我使用這個。',\n",
       " \"I'll do it.\\t我會做的。\",\n",
       " \"I'm a hero.\\t我是個英雄。\",\n",
       " \"I'm humble.\\t我是謙虛的。\",\n",
       " \"I'm single.\\t我單身。\",\n",
       " \"I'm sleepy.\\t我困了。\",\n",
       " \"I'm so fat.\\t我好胖哦。\",\n",
       " 'Ignore Tom.\\t別理湯姆。',\n",
       " 'Is it love?\\t那是愛嗎？',\n",
       " \"It's great.\\t真是太好了。\",\n",
       " \"It's night.\\t是晚上了。\",\n",
       " 'Just relax.\\t放鬆點吧。',\n",
       " 'Keep quiet!\\t保持安靜！',\n",
       " 'Let him in.\\t讓他進來。',\n",
       " 'Let me die.\\t讓我去死。',\n",
       " 'Let me see.\\t讓我想一想。',\n",
       " \"Let's walk.\\t我們走走吧。\",\n",
       " 'Look there.\\t看那裡。',\n",
       " 'No comment.\\t禁止評論。',\n",
       " 'No problem!\\t沒問題！',\n",
       " 'No problem.\\t沒關係。',\n",
       " 'No problem.\\t沒問題。',\n",
       " 'Once again.\\t再一次。',\n",
       " 'She smiled.\\t她笑了。',\n",
       " 'Stand back!\\t往後站！',\n",
       " 'Stay sharp.\\t保持警惕。',\n",
       " 'Stay there.\\t留在這裡別動。',\n",
       " 'Step aside.\\t讓開。',\n",
       " 'Study hard.\\t好好學習。',\n",
       " 'That hurts.\\t真疼。',\n",
       " 'Time flies.\\t時光飛逝。',\n",
       " 'Tom is shy.\\t湯姆害羞。',\n",
       " 'Tom smiled.\\t湯姆笑了。',\n",
       " 'Tom yawned.\\t湯姆打哈欠了。',\n",
       " 'Turn right.\\t向右轉。',\n",
       " 'We laughed.\\t我們笑了。',\n",
       " 'We lost it.\\t我們失去了它。',\n",
       " 'What is it?\\t這是什麼啊？',\n",
       " \"What's new?\\t有什麼新鮮事嗎？\",\n",
       " 'Where am I?\\t我在哪裡？',\n",
       " 'Where am I?\\t這是什麼地方?',\n",
       " 'Who are we?\\t我們是誰？',\n",
       " 'Wood burns.\\t木材燃燒。',\n",
       " 'Are we done?\\t我們完成了嗎？',\n",
       " 'Are you Tom?\\t你是湯姆嗎？',\n",
       " 'Are you mad?\\t您生氣了嗎？',\n",
       " 'Are you mad?\\t你生氣了嗎？',\n",
       " 'Are you sad?\\t你傷心嗎？',\n",
       " 'Be friendly.\\t友好點。',\n",
       " 'Break it up!\\t停手！',\n",
       " 'Can you see?\\t你能看嗎？',\n",
       " 'Count me in.\\t算我一個.',\n",
       " 'Do as I say.\\t按我說的做。',\n",
       " \"Don't leave!\\t別走！\",\n",
       " \"Don't panic!\\t不要恐慌！\",\n",
       " \"Don't shout.\\t不許大叫。\",\n",
       " \"Don't worry.\\t別擔心。\",\n",
       " \"Don't worry.\\t別擔心。\",\n",
       " 'Get serious.\\t認真點。',\n",
       " 'He is alone.\\t他獨自一人。',\n",
       " 'He is drunk.\\t他醉了。',\n",
       " \"He's strong.\\t他很強壯。\",\n",
       " 'Here we are!\\t我們到了！',\n",
       " 'How are you?\\t你們好嗎？',\n",
       " 'How curious!\\t多怪啊！',\n",
       " 'How strange!\\t真奇怪。',\n",
       " 'I am coming.\\t我來了。',\n",
       " 'I buy tapes.\\t我買磁帶。',\n",
       " \"I can't say.\\t我不能說。\",\n",
       " \"I don't lie.\\t我不說謊。\",\n",
       " 'I eat bread.\\t我吃麵包。',\n",
       " 'I feel cold.\\t我覺得冷。',\n",
       " 'I feel fine.\\t我感覺很好。',\n",
       " 'I feel weak.\\t我感到虛弱。',\n",
       " 'I have time.\\t我有時間。',\n",
       " 'I heard you.\\t我聽你的。',\n",
       " 'I like fish.\\t我喜歡魚。',\n",
       " 'I like jazz.\\t我喜歡爵士樂。',\n",
       " 'I live here.\\t我住在這。',\n",
       " 'I need glue.\\t我需要膠水。',\n",
       " 'I overslept.\\t我睡過頭了。',\n",
       " 'I said that.\\t我是那樣說的。',\n",
       " 'I smell gas.\\t我聞到瓦斯味。',\n",
       " 'I trust him.\\t我信任他。',\n",
       " 'I trust you.\\t我信任你。',\n",
       " 'I want more.\\t我想要更多。',\n",
       " 'I was wrong.\\t我搞錯了。',\n",
       " 'I will obey.\\t我會聽從指示。',\n",
       " 'I will wait.\\t我會等。',\n",
       " 'I work here.\\t我在這裡工作。',\n",
       " \"I'll attend.\\t我會參加。\",\n",
       " \"I'll get in.\\t我會進去。\",\n",
       " '\"I\\'m 17, too.\"\\t我也是17歲。',\n",
       " \"I'm Finnish.\\t我是芬蘭人。\",\n",
       " \"I'm Italian.\\t我是義大利人。\",\n",
       " \"I'm a loser.\\t我是個不成器的人。\",\n",
       " \"I'm at home.\\t我在家。\",\n",
       " \"I'm curious.\\t我很好奇。\",\n",
       " \"I'm married.\\t我結婚了。\",\n",
       " \"I'm married.\\t我已婚。\",\n",
       " \"I'm reading.\\t我在讀書。\",\n",
       " \"I'm serious.\\t我是認真的。\",\n",
       " \"I'm thirsty.\\t我渴了。\",\n",
       " 'Is Tom well?\\t湯姆還好吧？',\n",
       " 'Is it yours?\\t這是你的嗎？',\n",
       " \"It can't be!\\t這不可能！\",\n",
       " \"It can't be!\\t不可能！\",\n",
       " \"It's my job.\\t這是我份內的事。\",\n",
       " 'Just say no.\\t只要說不。',\n",
       " 'Keep trying.\\t繼續努力。',\n",
       " 'Let me know.\\t讓我知道。',\n",
       " \"Let's begin.\\t讓我們開始吧。\",\n",
       " \"Let's leave.\\t走吧。\",\n",
       " \"Let's start!\\t讓我們開始吧。\",\n",
       " 'Life is fun.\\t人生是有趣的。',\n",
       " 'Look around.\\t四處看看。',\n",
       " 'Lunch is on.\\t午餐送到了。',\n",
       " 'Many thanks.\\t非常感謝！',\n",
       " 'Money talks.\\t金錢萬能。',\n",
       " 'Nice timing.\\t好時機。',\n",
       " 'No one came.\\t沒有人來了。',\n",
       " 'OK. I agree.\\t好。我同意。',\n",
       " 'Please come.\\t請來吧。',\n",
       " 'Please sing.\\t請唱歌。',\n",
       " 'Release him.\\t放開他。',\n",
       " 'She hit him.\\t她打了他。',\n",
       " 'Stand aside.\\t一邊站著。',\n",
       " 'Step inside.\\t進來。',\n",
       " \"That's life.\\t這就是生活。\",\n",
       " \"That's life.\\t人生就是如此。\",\n",
       " \"That's true.\\t這是真的。\",\n",
       " 'They hugged.\\t他們擁抱。',\n",
       " 'They kissed.\\t他們親吻了。',\n",
       " 'This is ice.\\t這是冰塊。',\n",
       " 'Tom blushed.\\t湯姆臉紅了。',\n",
       " 'Tom frowned.\\t湯姆皺著眉頭。',\n",
       " 'Tom got fat.\\t湯姆變胖了。',\n",
       " \"Tom'll wait.\\t湯姆會等。\",\n",
       " \"Tom's wrong.\\t湯姆錯了。\",\n",
       " 'Turn around.\\t轉過來。',\n",
       " 'Was I wrong?\\t我錯了嗎？',\n",
       " 'We are boys.\\t我們是男孩。',\n",
       " 'We are late.\\t我們遲到了。',\n",
       " 'We both won.\\t我們都贏了。',\n",
       " 'We can rest.\\t我們可以休息。',\n",
       " 'We know him.\\t我們認識他。',\n",
       " 'We love Tom.\\t我們愛湯姆。',\n",
       " 'We want Tom.\\t我們想要湯姆。',\n",
       " 'What a pity!\\t太可惜了！',\n",
       " 'What a pity!\\t多遺憾啊！',\n",
       " 'What is new?\\t有什麼新鮮事嗎？',\n",
       " \"What's that?\\t那是什麼？\",\n",
       " \"What's this?\\t這是什麼啊？\",\n",
       " \"What's this?\\t那是什麼？\",\n",
       " 'Who are you?\\t你是誰？',\n",
       " 'Who drew it?\\t誰畫的？',\n",
       " 'Who is that?\\t那是誰？',\n",
       " 'Whose is it?\\t這是誰的？',\n",
       " 'Wood floats.\\t木頭會漂浮。',\n",
       " 'Work slowly.\\t幹活慢點。',\n",
       " \"You're sick!\\t你有病！\",\n",
       " 'Anything new?\\t有什麼新鮮事嗎？',\n",
       " 'Are you busy?\\t你忙嗎？',\n",
       " 'Are you lost?\\t您迷路了嗎？',\n",
       " 'Are you lost?\\t你迷路了嗎？',\n",
       " 'Are you sure?\\t你確定嗎？',\n",
       " 'Be realistic!\\t現實點！',\n",
       " 'Bring him in.\\t帶他進來。',\n",
       " 'Call Tom now.\\t現在給湯姆打電話。',\n",
       " 'Can we do it?\\t我們能做到嗎？',\n",
       " 'Can you come?\\t你能來嗎？',\n",
       " 'Can you read?\\t你能閱讀嗎？',\n",
       " 'Can you swim?\\t你會游泳嗎?',\n",
       " 'Did you call?\\t你打過電話了嗎？',\n",
       " 'Do you smoke?\\t你吸煙嗎?',\n",
       " 'Do your best.\\t盡力而為。',\n",
       " \"Don't be shy.\\t不要害羞。\",\n",
       " \"Don't bother.\\t不用麻煩了。\",\n",
       " \"Don't bug me.\\t別來煩我。\",\n",
       " 'Drive safely.\\t安全地駕駛。',\n",
       " 'Drive slowly.\\t開車慢點。',\n",
       " 'Ghosts exist.\\t鬼魂是存在的。',\n",
       " 'Give it back.\\t還回去',\n",
       " 'Good evening.\\t晚上好。',\n",
       " 'Good morning.\\t早上好！',\n",
       " 'He got angry.\\t他生氣了。',\n",
       " 'He likes tea.\\t他喜歡茶。',\n",
       " 'He loves her.\\t他愛她。',\n",
       " 'He was brave.\\t他很勇敢。',\n",
       " 'How annoying!\\t真煩人。',\n",
       " 'How dare you!\\t你敢！',\n",
       " 'I admire you.\\t我欣賞你。',\n",
       " 'I almost won.\\t我幾乎贏了。',\n",
       " 'I am Italian.\\t我是義大利人。',\n",
       " 'I am at home.\\t我在家。',\n",
       " 'I am curious.\\t我很好奇。',\n",
       " 'I am married.\\t我結婚了。',\n",
       " 'I am praying.\\t我在祈禱。',\n",
       " 'I am thirsty.\\t我渴了。',\n",
       " 'I ate caviar.\\t我吃了魚子醬。',\n",
       " 'I called Tom.\\t我給湯姆打了電話。',\n",
       " \"I don't care.\\t我無所謂。\",\n",
       " 'I have a car.\\t我有一輛車。',\n",
       " 'I have a dog.\\t我有一條狗。',\n",
       " 'I have a map.\\t我有一張地圖。',\n",
       " 'I like these.\\t我喜歡這些。',\n",
       " 'I love music.\\t我愛音樂。',\n",
       " 'I missed you.\\t我想你。',\n",
       " 'I see a book.\\t我看到一本書。',\n",
       " 'I understand.\\t我明白了。',\n",
       " 'I want a dog.\\t我想要一隻狗。',\n",
       " \"I'll sue you.\\t我要告你。\",\n",
       " \"I'm all ears.\\t我洗耳恭聽。\",\n",
       " \"I'm free now.\\t我現在有空了。\",\n",
       " \"I'm innocent.\\t我是清白的。\",\n",
       " \"I'm not sure.\\t我不確定。\",\n",
       " \"I'm pregnant.\\t我懷孕了。\",\n",
       " \"I'm so happy.\\t我好高興。\",\n",
       " \"I'm so tired.\\t我好累。\",\n",
       " \"I'm starving!\\t我餓死了！\",\n",
       " \"I'm starving!\\t我餓死了！\",\n",
       " \"I'm the best.\\t我是最好的。\",\n",
       " \"I'm thrilled.\\t我激動不已。\",\n",
       " \"I've decided.\\t我決定了。\",\n",
       " 'Is that love?\\t那是愛嗎？',\n",
       " 'Is that true?\\t真的？',\n",
       " \"It's amazing.\\t太神奇了。\",\n",
       " \"It's raining.\\t下雨了。\",\n",
       " \"It's raining.\\t在下雨。\",\n",
       " \"It's snowing.\\t正在下雪。\",\n",
       " \"It's so hard.\\t太難了。\",\n",
       " \"It's too big.\\t它太大了。\",\n",
       " 'Keep looking.\\t繼續看！',\n",
       " 'Keep reading.\\t繼續看。',\n",
       " 'Keep working.\\t繼續工作！',\n",
       " 'Let me think.\\t讓我想一想。',\n",
       " \"Let's go now.\\t我們現在去吧。\",\n",
       " 'Life goes on.\\t人生會繼續。',\n",
       " 'Mary came in.\\t瑪麗進來了。',\n",
       " 'Mary is tall.\\t瑪麗很高。',\n",
       " 'May I go now?\\t我現在能去了嗎？',\n",
       " 'Move quietly.\\t輕輕地移動。',\n",
       " 'My eyes hurt.\\t我的眼睛痛。',\n",
       " 'No one knows.\\t沒有人知道。',\n",
       " 'Nobody asked.\\t沒人問過。',\n",
       " 'Open the box.\\t打開箱子。',\n",
       " 'Please hurry.\\t請抓緊時間。',\n",
       " 'Please leave.\\t請你離開。',\n",
       " 'See you soon!\\t一會兒見！',\n",
       " 'See you then.\\t到時候見。',\n",
       " 'Shame on you!\\t你真丟臉！',\n",
       " 'Shame on you.\\t你真丟臉！',\n",
       " 'She is quiet.\\t她很安靜。',\n",
       " 'She is upset.\\t她心情不好。',\n",
       " 'She may come.\\t她可以來。',\n",
       " 'She may come.\\t她也許來。',\n",
       " 'Someone came.\\t有人來了。',\n",
       " 'Stop gawking.\\t別再傻看著。',\n",
       " 'Stop reading.\\t別再念了。',\n",
       " 'Stop staring.\\t別再盯著了。',\n",
       " 'Stop talking.\\t不要說話了。',\n",
       " 'Stop whining.\\t別再埋怨了。',\n",
       " 'Stop yelling!\\t別再喊叫了！',\n",
       " 'Sweet dreams!\\t祝你好夢。',\n",
       " 'Take it easy.\\t放輕鬆。',\n",
       " 'Take it easy.\\t別緊張。',\n",
       " 'Take me home.\\t帶我回家。',\n",
       " 'Tell us more.\\t告訴我們多一點。',\n",
       " \"That's crazy.\\t那是瘋狂的。\",\n",
       " \"That's great.\\t那真是太好了。\",\n",
       " \"That's right.\\t對！\",\n",
       " 'They refused.\\t他們拒絕了。',\n",
       " 'This is mine.\\t這是我的。',\n",
       " 'This is true.\\t這是真的。',\n",
       " 'Tom followed.\\t湯姆跟著。',\n",
       " 'Tom knows me.\\tTom認識我。',\n",
       " 'Tom sat down.\\t湯姆坐下了。',\n",
       " 'Tom told him.\\t湯姆告訴了他。',\n",
       " 'Tom was full.\\t湯姆飽了。',\n",
       " '\"Tom, wake up.\"\\t湯姆，快起來！',\n",
       " 'Try it again.\\t再試一次。',\n",
       " 'Unbelievable!\\t難以置信！',\n",
       " 'Vote for Tom.\\t投票給湯姆。',\n",
       " 'We are happy.\\t我們很快樂。',\n",
       " 'We can begin.\\t我們能開始。',\n",
       " 'We need more.\\t我們需要更多。',\n",
       " 'We succeeded.\\t我們成功了。',\n",
       " 'We surrender.\\t我們投降。',\n",
       " \"We're hiding.\\t我們在藏著。\",\n",
       " 'Welcome back.\\t歡迎回來。',\n",
       " 'What is love?\\t愛是什麼？',\n",
       " 'What is that?\\t那是什麼？',\n",
       " 'What is this?\\t這是什麼啊？',\n",
       " 'Who built it?\\t這是誰建的？',\n",
       " 'Who was here?\\t誰來了這裡？',\n",
       " 'Years passed.\\t多少年過去了。',\n",
       " 'Anything else?\\t還有別的嗎？',\n",
       " 'Are you angry?\\t您生氣了嗎？',\n",
       " 'Are you angry?\\t你生氣了嗎？',\n",
       " 'Are you happy?\\t你快樂嗎？',\n",
       " 'Are you ready?\\t您準備好了嗎？',\n",
       " 'Are you ready?\\t你們準備好了嗎？',\n",
       " 'Are you ready?\\t你準備好了嗎？',\n",
       " 'Are you tired?\\t你累了嗎?',\n",
       " 'Bring the key.\\t帶鑰匙來。',\n",
       " 'Call security!\\t叫保安來！',\n",
       " 'Can you skate?\\t你會溜冰嗎？',\n",
       " '\"Check, please.\"\\t請結帳。',\n",
       " \"Don't give up!\\t不要放棄!\",\n",
       " 'Draw a circle.\\t畫一個圈。',\n",
       " 'Flowers bloom.\\t鮮花盛開。',\n",
       " \"He can't swim.\\t他不會游泳。\",\n",
       " 'He is a thief.\\t這是一個小偷。',\n",
       " 'He is at home.\\t他在家裡。',\n",
       " 'He is my type!\\t他是我的菜!',\n",
       " 'He is no fool.\\t他不是傻子。',\n",
       " 'He is no fool.\\t他沒瘋。',\n",
       " \"He isn't home.\\t他不在家。\",\n",
       " 'He lied to us.\\t他欺騙了我們。',\n",
       " 'He sells cars.\\t他賣車子。',\n",
       " 'He shot at me.\\t他槍擊了我。',\n",
       " 'He tries hard.\\t他努力地嘗試。',\n",
       " 'He wants more.\\t他想要更多。',\n",
       " \"He's not home.\\t他不在家。\",\n",
       " 'How about you?\\t你怎麼樣？',\n",
       " 'How about you?\\t那你呢?',\n",
       " 'How about you?\\t你們呢？',\n",
       " 'How about you?\\t您呢？',\n",
       " 'How beautiful!\\t多美啊！',\n",
       " 'I am divorced.\\t我離婚了。',\n",
       " 'I am very sad.\\t我很難過。',\n",
       " 'I approved it.\\t我同意它。',\n",
       " 'I believe you.\\t我相信你。',\n",
       " 'I believe you.\\t我信過你。',\n",
       " 'I can see Tom.\\t我看得見湯姆。',\n",
       " \"I can't do it.\\t我無法做到這一點。\",\n",
       " 'I despise you.\\t我鄙視你。',\n",
       " 'I did my best.\\t我做得好到不能再好了。',\n",
       " 'I did see him.\\t我確實看見他了。',\n",
       " \"I didn't pass.\\t我沒通過。\",\n",
       " 'I feel lonely.\\t我覺得很孤獨。',\n",
       " 'I got it free.\\t我是免費得到的。',\n",
       " 'I have a home.\\t我有一個家。',\n",
       " 'I have an egg.\\t我有一隻蛋。',\n",
       " 'I have to win.\\t我必須贏。',\n",
       " 'I like sports.\\t我喜歡運動。',\n",
       " 'I like sweets.\\t我喜歡甜食。',\n",
       " 'I like to eat.\\t我喜歡吃。',\n",
       " 'I like to run.\\t我喜歡跑步。',\n",
       " 'I love coffee.\\t我愛咖啡。',\n",
       " 'I love movies.\\t我愛電影。',\n",
       " 'I love nature.\\t我愛大自然。',\n",
       " 'I love sports.\\t我喜歡運動。',\n",
       " 'I miss Boston.\\t我想念波士頓。',\n",
       " 'I must go now.\\t我現在必須走了。',\n",
       " 'I need to try.\\t我需要嘗試。',\n",
       " 'I respect Tom.\\t我尊敬湯姆。',\n",
       " 'I think I can.\\t我想我可以。',\n",
       " 'I use Twitter.\\t我用Twitter。',\n",
       " 'I want to cry.\\t我想哭。',\n",
       " 'I was at home.\\t我剛才在家。',\n",
       " 'I was so cold.\\t我很冷。',\n",
       " \"I'll buy this.\\t我要買這個。\",\n",
       " \"I'll call you.\\t我會打電話給你。\",\n",
       " \"I'll eat here.\\t我會在這裡吃飯。\",\n",
       " \"I'll take him.\\t我會去接他的。\",\n",
       " \"I'm a teacher.\\t我是個老師。\",\n",
       " \"I'm exhausted.\\t我累死了。\",\n",
       " \"I'm on a diet.\\t我在節食。\",\n",
       " \"I'm on my way.\\t我這就上路。\",\n",
       " \"I'm so stupid.\\t我真蠢。\",\n",
       " \"I'm very busy.\\t我很忙。\",\n",
       " \"I've got time.\\t我有時間。\",\n",
       " \"I've got wine.\\t我有酒。\",\n",
       " 'Is it serious?\\t嚴重嗎？',\n",
       " 'It is raining.\\t下雨了。',\n",
       " 'It is raining.\\t在下雨。',\n",
       " 'It looks good.\\t看起來不錯。',\n",
       " 'It might rain.\\t可能會下雨。',\n",
       " 'It might rain.\\t可能要下雨了。',\n",
       " \"It's a secret.\\t它是個秘密。\",\n",
       " \"It's business.\\t公事公辦。\",\n",
       " \"It's freezing.\\t天氣好冷。\",\n",
       " \"It's improved.\\t它有改善了。\",\n",
       " \"It's my treat.\\t我請客。\",\n",
       " \"It's not mine.\\t不是我的。\",\n",
       " \"It's too long.\\t它太長了。\",\n",
       " \"It's too loud.\\t它太大聲。\",\n",
       " \"It's very big.\\t它很大。\",\n",
       " 'Keep Tom safe.\\t確保湯姆安全。',\n",
       " 'Keep it quiet.\\t保持安靜',\n",
       " 'Let me see it.\\t讓我看看。',\n",
       " 'Lock the gate.\\t鎖大門。',\n",
       " 'May I come in?\\t我可以進來嗎?',\n",
       " 'Meet me there.\\t在那裡見我。',\n",
       " 'My foot hurts.\\t我的腳痛。',\n",
       " 'My pen is new.\\t我的鋼筆是新的。',\n",
       " 'Never give up.\\t決不放棄。',\n",
       " 'Pace yourself.\\t量力而行。',\n",
       " 'Quit gambling.\\t戒掉賭博吧。',\n",
       " 'Read this now.\\t現在讀這個。',\n",
       " 'Save yourself.\\t拯救你自己。',\n",
       " 'See you again.\\t再見！',\n",
       " 'See you later.\\t再見。',\n",
       " 'Shake my hand.\\t和我握手。',\n",
       " 'Shall we walk?\\t我們該步行嗎？',\n",
       " 'She felt blue.\\t她感到悶悶不樂的。',\n",
       " 'She hated him.\\t她恨他。',\n",
       " 'She loves Tom.\\t她愛湯姆。',\n",
       " \"She's dieting.\\t她在節食中。\",\n",
       " \"She's my type.\\t她是我的菜。\",\n",
       " 'Shut the door.\\t不要把門開著。',\n",
       " 'Shut the door.\\t關門。',\n",
       " 'Sit beside me.\\t坐我旁邊。',\n",
       " 'Speak clearly.\\t講清楚。',\n",
       " 'Stir the soup.\\t攪一下湯。',\n",
       " 'Stop meddling.\\t別再插手。',\n",
       " 'Stop shooting.\\t停止射擊。',\n",
       " 'Straighten up.\\t改邪歸正。',\n",
       " 'Stuff happens.\\t事情難免會發生。',\n",
       " 'Tell me again.\\t重新告訴我。',\n",
       " 'Thanks anyway.\\t還是要說謝謝的。',\n",
       " \"That's a book.\\t那是一本書。\",\n",
       " 'The bell rang.\\t鈴響了。',\n",
       " \"They're armed.\\t他們有帶武器。\",\n",
       " 'This is a wig.\\t這是假髮。',\n",
       " 'Time is money.\\t時間就是金錢。',\n",
       " 'Today was fun.\\t今天很有趣。',\n",
       " \"Tom didn't go.\\t湯姆沒走。\",\n",
       " 'Tom is a hick.\\t湯姆是個鄉巴佬。',\n",
       " 'Tom is absent.\\t湯姆缺席。',\n",
       " 'Tom is boring.\\t湯姆是個無聊的人。',\n",
       " 'Tom is so hot.\\t湯姆非常性感。',\n",
       " \"Tom isn't old.\\t湯姆不老。\",\n",
       " 'Tom loves you.\\t湯姆愛你。',\n",
       " 'Tom overslept.\\t湯姆睡過頭了。',\n",
       " 'Tom runs fast.\\t湯姆跑得快。',\n",
       " 'Tom was fired.\\t湯姆被解僱了。',\n",
       " 'Wait a moment.\\t請稍等。',\n",
       " 'We kept quiet.\\t我們保持了沉默。',\n",
       " 'We like music.\\t我們喜歡音樂。',\n",
       " 'We need money.\\t我們需要錢。',\n",
       " 'We understand.\\t我們明白。',\n",
       " 'We want candy.\\t我們想要糖。',\n",
       " \"We'll be busy.\\t我們會很忙。\",\n",
       " \"We'll do that.\\t我們會去做。\",\n",
       " \"We've arrived.\\t我們已經到了。\",\n",
       " 'Where are you?\\t你在哪兒?',\n",
       " 'Where is Mary?\\t瑪麗在哪裡？',\n",
       " 'Who helps her?\\t誰幫幫她？',\n",
       " 'Whose is this?\\t這是誰的？',\n",
       " 'Why blame Tom?\\t為什麼責備湯姆？',\n",
       " 'You are lying.\\t你在撒謊。',\n",
       " 'You are sharp.\\t你很聰明。',\n",
       " 'You could run.\\t你能跑。',\n",
       " 'You look pale.\\t你看起來很蒼白。',\n",
       " 'You should go.\\t你應該去。',\n",
       " 'You should go.\\t你最好去。',\n",
       " 'You will fail.\\t你會失敗。',\n",
       " 'You work hard.\\t你工作努力。',\n",
       " \"You're joking!\\t你在開玩笑吧！\",\n",
       " \"You're so bad.\\t你真壞。\",\n",
       " 'A bird can fly.\\t鳥會飛。',\n",
       " 'Allow me to go.\\t請允許我去。',\n",
       " 'Are you crying?\\t你在哭嗎？',\n",
       " \"Aren't you Tom?\\t你不是湯姆嗎?\",\n",
       " \"Aren't you Tom?\\t你不是湯姆嗎？\",\n",
       " \"Aren't you hot?\\t你不熱嗎？\",\n",
       " 'Be nice to her.\\t對她好點。',\n",
       " 'Birds lay eggs.\\t鳥下蛋。',\n",
       " 'Boil the water.\\t把水燒開。',\n",
       " 'Bring it to me.\\t把它帶給我。',\n",
       " 'Can I eat this?\\t我可以吃這個嗎?',\n",
       " 'Can I sit here?\\t我能坐這裡嗎？',\n",
       " 'Clean the room.\\t打掃房間。',\n",
       " 'Close the door.\\t關門。',\n",
       " '\"Coffee, please.\"\\t我要咖啡，謝謝。',\n",
       " 'Deal with them.\\t解決他們。',\n",
       " 'Do we know you?\\t我們認識你麼？',\n",
       " 'Do you know me?\\t你認識我嗎？',\n",
       " 'Do you know me?\\t你還認識我嗎？',\n",
       " 'Do you like it?\\t你喜歡嗎 ?',\n",
       " 'Do you like it?\\t你喜歡嗎？',\n",
       " 'Do you miss me?\\t你想我了？',\n",
       " 'Do you miss me?\\t你想我嗎？',\n",
       " 'Do you promise?\\t你保證嗎？',\n",
       " \"Don't touch it.\\t別碰它。\",\n",
       " 'Everybody lies.\\t每個人都會說謊。',\n",
       " 'Foxes eat hens.\\t狐狸吃母雞。',\n",
       " 'Get out of bed!\\t起床!',\n",
       " 'Good afternoon.\\t下午好。',\n",
       " 'Have some more.\\t你該多吃點。',\n",
       " 'He accelerated.\\t他提速了。',\n",
       " 'He cannot swim.\\t他不會游泳。',\n",
       " 'He confused us.\\t他把我們弄糊塗了。',\n",
       " \"He doesn't lie.\\t他不說謊。\",\n",
       " 'He is American.\\t他是美國人。',\n",
       " 'He lives alone.\\t他一個人生活。',\n",
       " 'He lives alone.\\t他獨自生活。',\n",
       " 'He looks young.\\t他看起來很年輕。',\n",
       " 'He lost a book.\\t他弄丟了一本書。',\n",
       " 'He sang a song.\\t他唱了一首歌。',\n",
       " 'He was drowned.\\t他被淹死了。',\n",
       " 'He was drowned.\\t他溺水而亡。',\n",
       " \"He's a big boy.\\t他是個大男孩。\",\n",
       " \"He's an author.\\t他是作家。\",\n",
       " 'Here is a book.\\t這裡有一本書。',\n",
       " 'How much is it?\\t它多貴?',\n",
       " 'How much is it?\\t多少錢?',\n",
       " \"How's it going?\\t你們好嗎？\",\n",
       " 'I am Hungarian.\\t我是匈牙利人。',\n",
       " 'I am a teacher.\\t我是個老師。',\n",
       " 'I am exhausted.\\t我累死了。',\n",
       " 'I am off today.\\t我今天休息。',\n",
       " 'I am too short.\\t我太矮了。',\n",
       " 'I borrow money.\\t我借錢。',\n",
       " 'I broke my arm.\\t我的手臂斷了。',\n",
       " \"I don't get it.\\t我不懂。\",\n",
       " 'I go to school.\\t我去學校。',\n",
       " 'I got arrested.\\t我被逮捕了。',\n",
       " 'I got up early.\\t我起床早。',\n",
       " 'I had a vision.\\t我有一個設想。',\n",
       " 'I had a vision.\\t我有一個願景。',\n",
       " 'I hate Mondays.\\t我討厭星期一。',\n",
       " 'I have a cough.\\t我咳嗽。',\n",
       " 'I have a dream.\\t我有一個夢想。',\n",
       " 'I have hiccups.\\t我打嗝了。',\n",
       " 'I have no time.\\t我沒時間。',\n",
       " 'I hiccup a lot.\\t我經常打嗝。',\n",
       " 'I like flowers.\\t我喜歡花。',\n",
       " 'I like jogging.\\t我喜歡跑步。',\n",
       " 'I like reading.\\t我喜歡閱讀。',\n",
       " 'I like running.\\t我喜歡跑步。',\n",
       " 'I like to read.\\t我喜歡閱讀。',\n",
       " 'I love my life.\\t我愛我的生活。',\n",
       " 'I love my life.\\t我愛我的生命。',\n",
       " 'I love my wife.\\t我愛我的妻子。',\n",
       " 'I love parties.\\t我愛派對。',\n",
       " 'I met a friend.\\t我遇見一個朋友。',\n",
       " 'I need a stamp.\\t我需要一張郵票。',\n",
       " 'I need it ASAP.\\t我盡快需要。',\n",
       " 'I need my coat.\\t我需要我的大衣。',\n",
       " 'I need to know.\\t我需要知道。',\n",
       " 'I often hiccup.\\t我經常打嗝。',\n",
       " 'I said shut up!\\t我說過了，閉嘴！',\n",
       " 'I saw five men.\\t我看到了五個男人。',\n",
       " 'I should do it.\\t我應該去做。',\n",
       " 'I study Korean.\\t我學韓語。',\n",
       " 'I was a doctor.\\t我以前是醫生。',\n",
       " 'I was learning.\\t我在學習。',\n",
       " 'I will sue you.\\t我要控告你。',\n",
       " \"I'd like to go.\\t我想要去。\",\n",
       " \"I'll alert Tom.\\t我會警告湯姆。\",\n",
       " \"I'll come back.\\t我會回來的。\",\n",
       " \"I'll scold him.\\t我會責備他。\",\n",
       " \"I'll see to it.\\t我會留意的。\",\n",
       " \"I'll see to it.\\t由我來做.\",\n",
       " \"I'll stay home.\\t我會待在家裡。\",\n",
       " \"I'll treat you.\\t我請你。\",\n",
       " \"I'm a free man.\\t我是一個自由的人。\",\n",
       " \"I'm a good guy.\\t我是一個好人。\",\n",
       " \"I'm behind him.\\t我在他後面。\",\n",
       " \"I'm free today.\\t我今天有空。\",\n",
       " \"I'm from Tokyo.\\t我來自東京。\",\n",
       " \"I'm not greedy.\\t我不貪婪。\",\n",
       " \"I'm not guilty.\\t我沒有罪。\",\n",
       " \"I'm so excited.\\t我很激動。\",\n",
       " \"I'm undressing.\\t我脫衣服。\",\n",
       " \"I'm very happy.\\t我很快樂。\",\n",
       " \"I'm very happy.\\t我很快樂。\",\n",
       " \"I've seen that.\\t我見過。\",\n",
       " 'Is he Japanese?\\t他是日本人嗎？',\n",
       " 'Is that better?\\t那更好嗎？',\n",
       " \"Isn't it black?\\t它不是黑色的嗎？\",\n",
       " 'It is so early.\\t太早了。',\n",
       " 'It is too late.\\t太晚了。',\n",
       " 'It makes sense.\\t那樣說得通。',\n",
       " 'It took months.\\t它花了幾個月。',\n",
       " \"It's brand new.\\t這是全新的。\",\n",
       " \"It's dangerous!\\t它是危險的!\",\n",
       " \"It's not funny.\\t這不好笑。\",\n",
       " \"It's our fault.\\t它是我們的錯誤。\",\n",
       " \"It's the truth.\\t這是事實。\",\n",
       " \"It's too large.\\t它太大了。\",\n",
       " \"It's up to you.\\t這就要看您了。\",\n",
       " \"It's up to you.\\t由你來決定。\",\n",
       " \"It's very cold.\\t非常冷。\",\n",
       " \"It's well done.\\t做得很好。\",\n",
       " \"It's your book.\\t它是你的書。\",\n",
       " \"It's your move.\\t該你走了。\",\n",
       " 'Keep listening.\\t繼續聽。',\n",
       " 'Let me do that.\\t讓我去做。',\n",
       " 'Let us go home.\\t讓我們回家吧。',\n",
       " \"Let's just eat.\\t我們吃吧。\",\n",
       " 'Listen to this.\\t聽聽這個。',\n",
       " 'May I use this?\\t我可以用嗎？',\n",
       " 'My name is Tom.\\t我叫Tom。',\n",
       " 'Now I remember.\\t現在我想起來了。',\n",
       " 'Please come in.\\t請進來。',\n",
       " 'Please do that.\\t麻煩您那樣做。',\n",
       " 'Please help me.\\t請幫助我。',\n",
       " 'Please help me.\\t請幫我。',\n",
       " 'Please join us.\\t請加入我們。',\n",
       " 'Please tell me.\\t請告訴我。',\n",
       " 'Please wash it.\\t請清洗它。',\n",
       " 'Prices went up.\\t物價上漲。',\n",
       " 'Read this book.\\t看這本書。',\n",
       " 'Say it clearly.\\t說清楚。',\n",
       " 'Science is fun.\\t科學好玩。',\n",
       " 'See you around.\\t再見！',\n",
       " 'She fooled him.\\t她愚弄了他。',\n",
       " 'She grew roses.\\t她種了玫瑰。',\n",
       " 'She might come.\\t她也許會來。',\n",
       " 'She seems rich.\\t她看來有錢。',\n",
       " 'Smoke appeared.\\t煙霧出現了。',\n",
       " 'Someone called.\\t有人打電話來。',\n",
       " 'Stop grumbling.\\t停止發牢騷吧。',\n",
       " 'Stop resisting!\\t停止抵抗！',\n",
       " 'Take your time.\\t你可以慢慢來。',\n",
       " \"That's a shame.\\t那是一個恥辱。\",\n",
       " \"That's my coat.\\t那是我的大衣。\",\n",
       " \"That's perfect.\\t那是完美的。\",\n",
       " \"That's too bad.\\t多遺憾啊！\",\n",
       " \"That's too bad.\\t那太糟糕了。\",\n",
       " 'The birds sang.\\t鳥兒歌唱。',\n",
       " 'The flag is up.\\t旗子升起了。',\n",
       " 'The phone rang.\\t電話正在響。',\n",
       " 'Their eyes met.\\t他們目光相接。',\n",
       " 'These are pens.\\t這些是筆。',\n",
       " 'They hated Tom.\\t他們恨湯姆。',\n",
       " 'They let me go.\\t他們讓我走。',\n",
       " 'They love that.\\t他們喜歡那個',\n",
       " 'They trust Tom.\\t他們信任湯姆。',\n",
       " 'They want more.\\t他們想要更多。',\n",
       " 'They want this.\\t他們想要這個。',\n",
       " 'They were good.\\t他們不錯。',\n",
       " 'This is a book.\\t這是一本書。',\n",
       " 'This is my bag.\\t那是我的包。',\n",
       " 'Tom can change.\\t湯姆能改變。',\n",
       " \"Tom can't swim.\\t湯姆不會游泳。\",\n",
       " 'Tom has a plan.\\t湯姆有個計畫。',\n",
       " 'Tom is a rabbi.\\t湯姆是個拉比。',\n",
       " \"Tom isn't dumb.\\t湯姆不傻。\",\n",
       " 'Tom looks pale.\\t湯姆看起來很蒼白。',\n",
       " 'Tom walked out.\\t湯姆走了出去。',\n",
       " \"Tom's fearless.\\t湯姆無所畏懼。\",\n",
       " \"Tom's laughing.\\t湯姆在笑。\",\n",
       " \"Tom's thrilled.\\t湯姆興奮不已。\",\n",
       " 'Treat him well.\\t對他好一點。',\n",
       " 'Turn on the TV.\\t開電視。',\n",
       " 'Turn up the TV.\\t把電視聲音調大點兒。',\n",
       " 'Was Tom asleep?\\t湯姆睡著了嗎？',\n",
       " 'Wash your feet.\\t洗您的腳。',\n",
       " 'Wash your feet.\\t洗你的腳。',\n",
       " 'Watch yourself.\\t自己當心啊。',\n",
       " 'We forgive you.\\t我們原諒你。',\n",
       " 'We knew no one.\\t我們誰也不認識。',\n",
       " 'We study music.\\t我們學習音樂。',\n",
       " \"We're not late.\\t我們沒遲到。\",\n",
       " '\"Well, let\\'s go.\"\\t好吧，我們走吧。',\n",
       " 'Were you right?\\t你是對的嗎？',\n",
       " 'What about you?\\t你們呢？',\n",
       " 'What about you?\\t您呢？',\n",
       " 'What do you do?\\t你的職業是什麼?',\n",
       " \"What's her job?\\t她做什麼工作？\",\n",
       " 'Where do we go?\\t我們去哪兒？',\n",
       " 'Where were you?\\t之前你在哪裡？',\n",
       " \"Who's that guy?\\t那傢伙是誰？\",\n",
       " 'Why do you ask?\\t你問這個幹什麼?',\n",
       " 'Why is he here?\\t為什麼他在這兒？',\n",
       " 'Wipe your eyes.\\t擦擦你的眼睛。',\n",
       " '\"Yes, I know it.\"\\t是的，我知道。',\n",
       " '\"Yes, of course.\"\\t是的，當然。',\n",
       " 'You look bored.\\t你看起來很無聊。',\n",
       " 'You look tense.\\t你看起來很緊張。',\n",
       " 'You look tired.\\t你看起來很疲倦。',\n",
       " 'You must do it.\\t你必須去做。',\n",
       " \"You'll love it.\\t你會愛它。\",\n",
       " \"You're kidding!\\t你開玩笑吧！\",\n",
       " 'Are you over 18?\\t你有18歲了嗎？',\n",
       " 'Are you serious?\\t你是認真的嗎？',\n",
       " 'Are you thirsty?\\t你渴嗎？',\n",
       " 'Balls are round.\\t球是圓的。',\n",
       " 'Be happy for me.\\t為我感到高興吧。',\n",
       " 'Behave yourself.\\t規矩點。',\n",
       " 'Black suits you.\\t黑色很襯你。',\n",
       " 'Boil some water.\\t燒一點水。',\n",
       " 'Call the police!\\t叫警察！',\n",
       " 'Call the police!\\t報警！',\n",
       " 'Call the police.\\t報警！',\n",
       " 'Can you find it?\\t你能找到它嗎？',\n",
       " 'Clean your room.\\t清掃你的房間。',\n",
       " 'Close your eyes.\\t閉上你的眼睛。',\n",
       " 'Close your eyes.\\t閉上你們的眼睛。',\n",
       " 'Come if you can.\\t如果你能就來吧。',\n",
       " 'Congratulations!\\t祝賀你。',\n",
       " 'Congratulations!\\t恭喜！',\n",
       " 'Count to thirty.\\t數到三十。',\n",
       " 'Did Tom say who?\\t湯姆說是誰了嗎？',\n",
       " 'Did you like it?\\t你以前喜歡嗎？',\n",
       " 'Did you miss me?\\t你想我了？',\n",
       " 'Do you go often?\\t您經常去嗎？',\n",
       " 'Do you have one?\\t你有嗎？',\n",
       " 'Do you know him?\\t你認識他嗎?',\n",
       " 'Do you like rap?\\t你喜歡說唱音樂嗎？',\n",
       " 'Do you remember?\\t你記得嗎？',\n",
       " 'Does love exist?\\t愛存在嗎？',\n",
       " \"Don't be fooled.\\t別被騙了。\",\n",
       " \"Don't bother me.\\t別來煩我。\",\n",
       " \"Don't forget me.\\t別忘了我。\",\n",
       " \"Don't open that.\\t別打開那個。\",\n",
       " \"Don't run risks.\\t不要冒險。\",\n",
       " \"Don't tell lies.\\t別說謊!\",\n",
       " 'Drive carefully.\\t開車小心點。',\n",
       " 'Get out of here.\\t離開這裡。',\n",
       " 'Give Tom a hand.\\t幫湯姆一把。',\n",
       " 'Give it to them.\\t把它給他們。',\n",
       " 'Go to the patio.\\t去露台。',\n",
       " 'Green suits you.\\t綠色適合你。',\n",
       " 'Have a nice day.\\t祝你一天過得愉快。',\n",
       " 'He acts quickly.\\t他行動迅速。',\n",
       " 'He began to cry.\\t他開始哭了。',\n",
       " 'He is my father.\\t他是我父親。',\n",
       " 'He is very kind.\\t他非常親切。',\n",
       " 'He is very tall.\\t他很高。',\n",
       " 'He just arrived.\\t他剛到。',\n",
       " 'He looks strong.\\t他看起來很強壯。',\n",
       " 'He loves trains.\\t他喜歡火車。',\n",
       " 'He pressured me.\\t他向我施壓。',\n",
       " 'He studied hard.\\t他努力學習。',\n",
       " 'He tends to lie.\\t他企圖說謊。',\n",
       " 'He tires easily.\\t他很容易覺得累。',\n",
       " 'He was very old.\\t他很老。',\n",
       " 'He will survive.\\t他會倖存。',\n",
       " \"He's a comedian.\\t他是喜劇演員。\",\n",
       " \"He's a good guy.\\t他是個好人。\",\n",
       " \"He's a good lad.\\t他是個好人。\",\n",
       " \"He's a good man.\\t他是個好人。\",\n",
       " \"He's a good man.\\t他是個好人。\",\n",
       " \"He's a tall boy.\\t他是一個高大的男孩。\",\n",
       " \"He's not a hero.\\t他不是英雄。\",\n",
       " \"He's not stupid.\\t他不是傻子。\",\n",
       " \"He's very angry.\\t他非常生氣。\",\n",
       " \"Here's your tea.\\t這是你的茶。\",\n",
       " 'Hi! How are you?\\t嗨！你好嗎？',\n",
       " 'His word is law.\\t他的話就是命令。',\n",
       " 'How do you feel?\\t你感覺如何？',\n",
       " 'How do you know?\\t我怎麼知道？',\n",
       " 'How is everyone?\\t大家好嗎？',\n",
       " 'How is it going?\\t你們好嗎？',\n",
       " 'How old are you?\\t你幾歲?',\n",
       " 'How rude of you!\\t你真粗魯！',\n",
       " 'I ache all over.\\t我全身酸痛。',\n",
       " 'I am a good boy.\\t我是一個好男孩。',\n",
       " 'I am from China.\\t我是從中國來的。',\n",
       " 'I am from China.\\t我來自中國。',\n",
       " 'I am in trouble.\\t我有麻煩了。',\n",
       " 'I am in trouble.\\t我遇到困難了。',\n",
       " 'I ate the apple.\\t我吃了這個蘋果。',\n",
       " 'I baked cookies.\\t我烤了曲奇。',\n",
       " 'I began running.\\t我開始跑。',\n",
       " 'I booked a seat.\\t我訂了一個位子。',\n",
       " 'I bought a book.\\t我買了一本書。',\n",
       " 'I called Tom up.\\t我打電話給湯姆了。',\n",
       " 'I called her up.\\t我叫她起來。',\n",
       " 'I can swim well.\\t我游泳可以游得很好。',\n",
       " \"I can't see you.\\t我看不見你。\",\n",
       " \"I can't undo it.\\t我不能取消它。\",\n",
       " 'I feel relieved.\\t我感覺輕鬆了。',\n",
       " 'I get up at six.\\t我六點起床。',\n",
       " 'I had no choice.\\t那時我沒有選擇的餘地。',\n",
       " 'I hate studying.\\t我討厭學習。',\n",
       " 'I have brothers.\\t我有兄弟。',\n",
       " 'I have ten pens.\\t我有十支筆。',\n",
       " 'I have to hurry!\\t我要趕緊了!',\n",
       " 'I have two cats.\\t我有兩隻貓。',\n",
       " 'I just threw up.\\t我剛才吐了。',\n",
       " 'I lent him a CD.\\t我借給他一盤CD。',\n",
       " '\"I like Tom, too.\"\\t我也喜歡湯姆。',\n",
       " 'I like football.\\t我喜歡足球。',\n",
       " 'I like potatoes.\\t我喜歡土豆。',\n",
       " 'I like the cold.\\t我喜歡寒冷。',\n",
       " 'I like this dog.\\t我喜歡這隻狗。',\n",
       " 'I like your car.\\t我喜歡您的車。',\n",
       " 'I lived in Rome.\\t我住在羅馬。',\n",
       " 'I love this car.\\t我愛這台車。',\n",
       " 'I might say yes.\\t我可能會說是。',\n",
       " 'I must help her.\\t我必須幫助她。',\n",
       " 'I need a friend.\\t我需要個朋友。',\n",
       " 'I need evidence.\\t我需要證據。',\n",
       " 'I need you here.\\t我需要你在這裡。',\n",
       " 'I paid the bill.\\t我買了單。',\n",
       " 'I played tennis.\\t我打網球了。',\n",
       " 'I run every day.\\t我每天跑步。',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = open(DATA_FILE, mode=\"r\", encoding=\"utf-8\").read().split('\\n')\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:12:13.476689Z",
     "start_time": "2019-09-08T15:12:13.420966Z"
    }
   },
   "outputs": [],
   "source": [
    "# 逐行的讀取與處理\n",
    "for line in lines[: min(num_samples, len(lines)-1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    \n",
    "    # 我們使用“tab”作為“開始序列[SOS]”字符或目標，“\\n”作為“結束序列[EOS]”字符。 <-- **重要\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    \n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    # 順便把字符也記錄起來\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:12:15.790839Z",
     "start_time": "2019-09-08T15:12:15.779689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 73\n",
      "Number of unique output tokens: 2165\n",
      "Max sequence length for inputs: 33\n",
      "Max sequence length for outputs: 22\n"
     ]
    }
   ],
   "source": [
    "input_characters = sorted(list(input_characters)) # 全部輸入的字符集\n",
    "target_characters = sorted(list(target_characters)) # 全部目標字符集\n",
    "\n",
    "num_encoder_tokens = len(input_characters) # 所有輸入字符的數量\n",
    "num_decoder_tokens = len(target_characters) # 所有輸目標字符的數量\n",
    "\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts]) # 最長的輸入句子長度\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts]) # 最長的目標句子長度\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:12:18.180637Z",
     "start_time": "2019-09-08T15:12:18.171297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 33, 73)\n",
      "(10000, 22, 2165)\n",
      "(10000, 22, 2165)\n"
     ]
    }
   ],
   "source": [
    "# 輸入字符的索引字典\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "\n",
    "# 輸目標字符的索引字典\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "# barch size, time step, features\n",
    "# 包含英文句子的one-hot向量化的三維形狀數組（num_pairs，max_english_sentence_length，num_english_characters）\n",
    "encoder_input_data = np.zeros( # 10000, 33, 73\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "\n",
    "# 這裡訓練的時候看起來有teacher forcing，因為decoder的input照理說就是上一個time step的decoder的output\n",
    "\n",
    "# 包含中文句子的one-hot向量化的三維形狀數組（num_pairs，max_chinese_sentence_length，num_chinese_characters）\n",
    "decoder_input_data = np.zeros( # 10000, 22, 2165\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "# decoder_target_data與decoder_input_data相同，但是偏移了一個時間步長。 \n",
    "# decoder_target_data [:， t，：]將與decoder_input_data [：，t + 1，：]相同\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:12:21.221479Z",
     "start_time": "2019-09-08T15:12:21.216044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi.', 'Hi.', 'Run.', 'Wait!', 'Hello!', 'I try.'] ['\\t嗨。\\n', '\\t你好。\\n', '\\t你用跑的。\\n', '\\t等等！\\n', '\\t你好。\\n', '\\t讓我來。\\n']\n"
     ]
    }
   ],
   "source": [
    "print(input_texts[:6], target_texts[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:28:13.793550Z",
     "start_time": "2019-09-08T16:28:13.576965Z"
    }
   },
   "outputs": [],
   "source": [
    "# 把資料轉換成要用來訓練用的張量資料結構 <-- 重要\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        # 10000, 33, 73\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        \n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep  --> 希望答案就是下一個字\n",
    "        # FIX?: input的最後一個字<EOS>train的時候好像沒什麼用~\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        \n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.  <-- <SOS>必須是decoder_input_data的第一個字\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 構建網絡架構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T15:12:25.978573Z",
     "start_time": "2019-09-08T15:12:25.307386Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0908 23:12:25.331032 140713521444672 deprecation.py:237] From /home/rossleecooloh/.pyenv/versions/3.6.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4139: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, None, 73)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None, 2165)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 256), (None, 337920      encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 256),  2480128     decoder_input[0][0]              \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 2165)   556405      decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 3,374,453\n",
      "Trainable params: 3,374,453\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ===== 編碼 (encoder) ====\n",
    "# RNN基本上就是for-loop\n",
    "\n",
    "# 定義輸入的序列\n",
    "# 注意：因為輸入序列長度(timesteps)可變的情況，使用input_shape =（None，num_features） <-- 重要!!\n",
    "# https://stats.stackexchange.com/questions/377091/time-steps-in-keras-lstm\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), name='encoder_input') \n",
    "encoder = LSTM(latent_dim, return_state=True, name='encoder_lstm') # 需要取得LSTM的內部state, 因此設定\"return_state=True\"\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# 因為沒有return_sequence所以會把timestep全部滾完，輸出最後一個state <-- 重要!!\n",
    "\n",
    "# return_sequences=True && return_state=True\n",
    "# 此时，我们既要输出全部时间步的 hidden state ，又要输出 cell state。\n",
    "\n",
    "# encoder_outputs 存放的就是全部时间步的 hidden state\n",
    "# state_h 存放的是最后一个时间步的 hidden state\n",
    "# state_c 存放的是最后一个时间步的 cell state\n",
    "\n",
    "# 我們拋棄掉`encoder_outputs`因為我們只需要LSTM cell的內部state參數\n",
    "encoder_states = [state_h, state_c]  # [256, 256]\n",
    "\n",
    "# ==== 解碼 (decoder) ====\n",
    "\n",
    "# 設定解碼器(decoder)\n",
    "# 注意：因為輸出序列的長度(timesteps)是變動的，使用input_shape =（None，num_features） <-- 重要!!\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_input')\n",
    "\n",
    "# 此时，我们既要输出全部时间步的 hidden state ，又要输出 cell state。\n",
    "# 我們設定我們的解碼器回傳整個輸出的序列同時也回傳內部的states參數\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')#(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# 因為這邊return_sequence所以會把timestep繼續延續下去，一個一個state去滾 <-- 重要!!\n",
    "\n",
    "# 在訓練時我們不會使用這些回傳的states, 但是在預測時我們會用到這些states參數\n",
    "# **解碼器的初始狀態是使用編碼器的最後的狀態(states)**\n",
    "# https://keras.io/layers/recurrent/\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                            initial_state=encoder_states) # 我們使用`encoder_states`來做為初始值(initial state) <-- 重要 (call() in class)\n",
    "\n",
    "# 接密集層(dense)來進行softmax運算每一個字符可能的機率\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_output')#(decoder_outputs)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 定義一個模型接收encoder_input_data` & `decoder_input_data`做為輸入而輸出`decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# 打印出模型結構\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T14:53:11.596006Z",
     "start_time": "2019-09-08T14:53:11.591973Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "# 產生網絡拓撲圖\n",
    "# plot_model(model, to_file='seq2seq_graph.png')\n",
    "# Image('seq2seq_graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./LSTM結構拓樸.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T14:53:01.358765Z",
     "start_time": "2019-09-08T14:52:59.533897Z"
    }
   },
   "source": [
    "### 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:24:03.949435Z",
     "start_time": "2019-09-08T15:12:31.426462Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0908 23:12:31.455026 140713521444672 deprecation.py:237] From /home/rossleecooloh/.pyenv/versions/3.6.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 44s 6ms/step - loss: 1.9903 - val_loss: 2.4688\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.8491 - val_loss: 2.3261\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.7301 - val_loss: 2.2461\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.6432 - val_loss: 2.1634\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.5608 - val_loss: 2.0808\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.4949 - val_loss: 2.0056\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.4316 - val_loss: 1.9536\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.3659 - val_loss: 1.9181\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.3120 - val_loss: 1.8656\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.2660 - val_loss: 1.8587\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.2229 - val_loss: 1.8434\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.1855 - val_loss: 1.8086\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.1480 - val_loss: 1.8386\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.1134 - val_loss: 1.7620\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.0809 - val_loss: 1.7587\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.0496 - val_loss: 1.7731\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 1.0208 - val_loss: 1.7474\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.9931 - val_loss: 1.7249\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.9656 - val_loss: 1.7174\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.9405 - val_loss: 1.7153\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.9148 - val_loss: 1.7097\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.8898 - val_loss: 1.7311\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.8670 - val_loss: 1.7069\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.8456 - val_loss: 1.7041\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.8236 - val_loss: 1.6995\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.8016 - val_loss: 1.7087\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.7813 - val_loss: 1.7166\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.7617 - val_loss: 1.7109\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.7419 - val_loss: 1.7124\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.7239 - val_loss: 1.7201\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.7051 - val_loss: 1.7327\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.6880 - val_loss: 1.7280\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.6706 - val_loss: 1.7292\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.6536 - val_loss: 1.7373\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.6383 - val_loss: 1.7509\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.6224 - val_loss: 1.7471\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.6070 - val_loss: 1.7557\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.5930 - val_loss: 1.7583\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.5787 - val_loss: 1.7773\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.5641 - val_loss: 1.7732\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.5509 - val_loss: 1.7887\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.5378 - val_loss: 1.7926\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.5251 - val_loss: 1.7952\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.5126 - val_loss: 1.7994\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.5007 - val_loss: 1.8106\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.4890 - val_loss: 1.8083\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.4774 - val_loss: 1.8253\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.4664 - val_loss: 1.8358\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.4561 - val_loss: 1.8447\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.4466 - val_loss: 1.8459\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.4360 - val_loss: 1.8417\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.4262 - val_loss: 1.8578\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.4172 - val_loss: 1.8596\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.4083 - val_loss: 1.8654\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.3992 - val_loss: 1.8864\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.3904 - val_loss: 1.8961\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.3812 - val_loss: 1.8946\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.3736 - val_loss: 1.8945\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.3659 - val_loss: 1.9060\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.3586 - val_loss: 1.9062\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.3509 - val_loss: 1.9140\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.3431 - val_loss: 1.9220\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.3361 - val_loss: 1.9310\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.3289 - val_loss: 1.9505\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.3221 - val_loss: 1.9457\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.3158 - val_loss: 1.9489\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.3094 - val_loss: 1.9653\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.3030 - val_loss: 1.9777\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2961 - val_loss: 1.9796\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2898 - val_loss: 1.9756\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2834 - val_loss: 2.0017\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2774 - val_loss: 2.0055\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2714 - val_loss: 1.9980\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2660 - val_loss: 2.0052\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2600 - val_loss: 2.0113\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2549 - val_loss: 2.0300\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2493 - val_loss: 2.0343\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2437 - val_loss: 2.0493\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2393 - val_loss: 2.0397\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2336 - val_loss: 2.0470\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2288 - val_loss: 2.0660\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2236 - val_loss: 2.0591\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2190 - val_loss: 2.0683\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2142 - val_loss: 2.0760\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2091 - val_loss: 2.0831\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2050 - val_loss: 2.0957\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2004 - val_loss: 2.1007\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1948 - val_loss: 2.1115\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1904 - val_loss: 2.1098\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1865 - val_loss: 2.1204\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1814 - val_loss: 2.1292\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1780 - val_loss: 2.1431\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1732 - val_loss: 2.1452\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1695 - val_loss: 2.1558\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1650 - val_loss: 2.1519\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1615 - val_loss: 2.1645\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1571 - val_loss: 2.1821\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1538 - val_loss: 2.1827\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1500 - val_loss: 2.1713\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1458 - val_loss: 2.1829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rossleecooloh/.pyenv/versions/3.6.6/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'encoder_lstm/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# 設定模型超參數\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# 開始訓練\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "# 儲存模型\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-08T14:59:48.717Z"
    }
   },
   "source": [
    "### 模型預測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是預測階段的步驟:\n",
    "\n",
    "1. 對輸入進行編碼(encode)並取得解碼器所需要的初始狀態(initial decoder state)\n",
    "2. 以此初始狀態運行一步解碼器，並以“開始序列”標記作為目標。輸出將是下一個目標標記\n",
    "3. 重複當前目標標記和當前狀態\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 取樣模型圖定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:24:12.914964Z",
     "start_time": "2019-09-08T16:24:12.740669Z"
    }
   },
   "outputs": [],
   "source": [
    "# ![prediction](./seq2seq預測.PNG)\n",
    "# 定義要進行取樣的模型 --> 1個1個time step抓出來預測\n",
    "\n",
    "# 訓練好之後就可以組起來\n",
    "\n",
    "# 定義編碼器(encoder)的模型\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# 定義解碼器LSTM cell的初始權重輸入\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# # 解碼器(decoder)定義初始狀態(initial decoder state) --> 先把框架起來，後面會放encoder的input\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs) #我們使用`decoder_states_inputs`來做為初始值(initial state)\n",
    "\n",
    "decoder_states = [state_h, state_c] # [256, 256]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)  # 把所有hidden state丟進Dense並用softmax當激活函數 (output維度2xxx)\n",
    "\n",
    "# 定義解碼器(decoder)的模型\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "# 反向查找字符索引來將序列解碼為可讀的內容。\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解碼函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:26:39.529943Z",
     "start_time": "2019-09-08T16:26:39.521229Z"
    }
   },
   "outputs": [],
   "source": [
    "# 對序列進行解碼\n",
    "def decode_sequence(input_seq):\n",
    "    # input_seq就是input英文句子, output出最後的state_h和state_c\n",
    "    # 將輸入編碼成為state向量\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # 產生長度為1的空白目標序列\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    \n",
    "    # 發佈特定的目標序列起始字符\"[SOS]\",在這個範例中是使用 \"\\t\"字符\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "    \n",
    "    # 對批次的序列進行抽樣迴圈\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "        \n",
    "        # output_tokens就是預測出來的(, 經過Dense的2xxx維)的output字\n",
    "        # 對符標抽樣\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # 停止迴圈的條件: 到達最大的長度或是找到\"停止[EOS]\"字符,在這個範例中是使用 \"\\n\"字符\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 更新目標序列(of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.  # 再把剛剛output_tokens的預測結果更新為下一個字的input\n",
    "\n",
    "        # 更新 states --> 重要!! 也要把剛剛output出來的h, c更新到下一個time step (1個1個time step拆出來預測)\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 結果預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:34.320639Z",
     "start_time": "2019-09-08T16:35:34.314663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "(1, 33, 73)\n",
      "Why me?                          \n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data[0: 1])\n",
    "print(encoder_input_data[0: 1].shape)\n",
    "answer = ''\n",
    "word_idx = 20\n",
    "for i in range(encoder_input_data[0].shape[0]):\n",
    "    answer += reverse_input_char_index[np.argmax(encoder_input_data[word_idx][i])]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T16:35:53.433572Z",
     "start_time": "2019-09-08T16:35:52.185543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: 嗨。\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: 嗨。\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: 你用跑的。\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: 等等！\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: 你好。\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: 湯姆睡著了。\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: 我贏了。\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: 不會吧。\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: 你們這是做。\n",
      "\n",
      "-\n",
      "Input sentence: He ran.\n",
      "Decoded sentence: 他跑了。\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: 開車。\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: 我同意。\n",
      "\n",
      "-\n",
      "Input sentence: I quit.\n",
      "Decoded sentence: 我來自中國。\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: 我沒事。\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: 聽著。\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: 沒門！\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: 沒門！\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: 你確定？\n",
      "\n",
      "-\n",
      "Input sentence: Try it.\n",
      "Decoded sentence: 試試吧。\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: 我們能要去。\n",
      "\n",
      "-\n",
      "Input sentence: Why me?\n",
      "Decoded sentence: 為什麼是我？\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: 回頭看！\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: 友善點。\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: 抓住那個。\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: 回頭看上。\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: 抓住這個。\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: 打電話回家。\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: 打電話回家！\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: 回家吧。\n",
      "\n",
      "-\n",
      "Input sentence: Get Tom.\n",
      "Decoded sentence: 把書放在那裡。\n",
      "\n",
      "-\n",
      "Input sentence: Get out!\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Go away.\n",
      "Decoded sentence: 走開！\n",
      "\n",
      "-\n",
      "Input sentence: Goodbye!\n",
      "Decoded sentence: 祝你好運。\n",
      "\n",
      "-\n",
      "Input sentence: Goodbye!\n",
      "Decoded sentence: 祝你好運。\n",
      "\n",
      "-\n",
      "Input sentence: Hang on!\n",
      "Decoded sentence: 等一下！\n",
      "\n",
      "-\n",
      "Input sentence: He came.\n",
      "Decoded sentence: 他來了。\n",
      "\n",
      "-\n",
      "Input sentence: He runs.\n",
      "Decoded sentence: 他跑。\n",
      "\n",
      "-\n",
      "Input sentence: Help me.\n",
      "Decoded sentence: 幫我一下。\n",
      "\n",
      "-\n",
      "Input sentence: Hold on.\n",
      "Decoded sentence: 我們好。\n",
      "\n",
      "-\n",
      "Input sentence: Hug Tom.\n",
      "Decoded sentence: 真奇怪。\n",
      "\n",
      "-\n",
      "Input sentence: I agree.\n",
      "Decoded sentence: 我同意。\n",
      "\n",
      "-\n",
      "Input sentence: I'm ill.\n",
      "Decoded sentence: 我生病了。\n",
      "\n",
      "-\n",
      "Input sentence: I'm old.\n",
      "Decoded sentence: 我有點了。\n",
      "\n",
      "-\n",
      "Input sentence: It's OK.\n",
      "Decoded sentence: 沒關係。\n",
      "\n",
      "-\n",
      "Input sentence: It's me.\n",
      "Decoded sentence: 是我。\n",
      "\n",
      "-\n",
      "Input sentence: Join us.\n",
      "Decoded sentence: 來加入我們吧。\n",
      "\n",
      "-\n",
      "Input sentence: Keep it.\n",
      "Decoded sentence: 繼續看！\n",
      "\n",
      "-\n",
      "Input sentence: Kiss me.\n",
      "Decoded sentence: 在這裡。\n",
      "\n",
      "-\n",
      "Input sentence: Perfect!\n",
      "Decoded sentence: 請幫我。\n",
      "\n",
      "-\n",
      "Input sentence: See you.\n",
      "Decoded sentence: 再見！\n",
      "\n",
      "-\n",
      "Input sentence: Shut up!\n",
      "Decoded sentence: 把他本書來。\n",
      "\n",
      "-\n",
      "Input sentence: Skip it.\n",
      "Decoded sentence: 繼續看。\n",
      "\n",
      "-\n",
      "Input sentence: Take it.\n",
      "Decoded sentence: 照顧好自己的。\n",
      "\n",
      "-\n",
      "Input sentence: Wake up!\n",
      "Decoded sentence: 醒醒！\n",
      "\n",
      "-\n",
      "Input sentence: Wash up.\n",
      "Decoded sentence: 去清洗一下。\n",
      "\n",
      "-\n",
      "Input sentence: We know.\n",
      "Decoded sentence: 我們知道。\n",
      "\n",
      "-\n",
      "Input sentence: Welcome.\n",
      "Decoded sentence: 歡迎。\n",
      "\n",
      "-\n",
      "Input sentence: Who won?\n",
      "Decoded sentence: 誰贏了？\n",
      "\n",
      "-\n",
      "Input sentence: Why not?\n",
      "Decoded sentence: 為什麼不？\n",
      "\n",
      "-\n",
      "Input sentence: You run.\n",
      "Decoded sentence: 你跑。\n",
      "\n",
      "-\n",
      "Input sentence: Back off.\n",
      "Decoded sentence: 把它給了！\n",
      "\n",
      "-\n",
      "Input sentence: Be still.\n",
      "Decoded sentence: 現實點吧。\n",
      "\n",
      "-\n",
      "Input sentence: Cuff him.\n",
      "Decoded sentence: 抓住他。\n",
      "\n",
      "-\n",
      "Input sentence: Drive on.\n",
      "Decoded sentence: 開車小心。\n",
      "\n",
      "-\n",
      "Input sentence: Get away!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Get away!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Get down!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Get lost!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Get real.\n",
      "Decoded sentence: 繼續看。\n",
      "\n",
      "-\n",
      "Input sentence: Grab Tom.\n",
      "Decoded sentence: 抓住湯姆。\n",
      "\n",
      "-\n",
      "Input sentence: Grab him.\n",
      "Decoded sentence: 抓住那個。\n",
      "\n",
      "-\n",
      "Input sentence: Have fun.\n",
      "Decoded sentence: 玩得開心。\n",
      "\n",
      "-\n",
      "Input sentence: He tries.\n",
      "Decoded sentence: 他來試試。\n",
      "\n",
      "-\n",
      "Input sentence: Humor me.\n",
      "Decoded sentence: 你就隨了我的意吧。\n",
      "\n",
      "-\n",
      "Input sentence: Hurry up.\n",
      "Decoded sentence: 快點！\n",
      "\n",
      "-\n",
      "Input sentence: Hurry up.\n",
      "Decoded sentence: 快點！\n",
      "\n",
      "-\n",
      "Input sentence: I forgot.\n",
      "Decoded sentence: 我忘了。\n",
      "\n",
      "-\n",
      "Input sentence: I resign.\n",
      "Decoded sentence: 我放棄。\n",
      "\n",
      "-\n",
      "Input sentence: I'll pay.\n",
      "Decoded sentence: 我來付錢。\n",
      "\n",
      "-\n",
      "Input sentence: I'm busy.\n",
      "Decoded sentence: 我很忙。\n",
      "\n",
      "-\n",
      "Input sentence: I'm cold.\n",
      "Decoded sentence: 我好了。\n",
      "\n",
      "-\n",
      "Input sentence: I'm fine.\n",
      "Decoded sentence: 我生病了。\n",
      "\n",
      "-\n",
      "Input sentence: I'm full.\n",
      "Decoded sentence: 我生病了。\n",
      "\n",
      "-\n",
      "Input sentence: I'm sick.\n",
      "Decoded sentence: 我生病了。\n",
      "\n",
      "-\n",
      "Input sentence: I'm sick.\n",
      "Decoded sentence: 我生病了。\n",
      "\n",
      "-\n",
      "Input sentence: Leave me.\n",
      "Decoded sentence: 讓我一個人呆會兒。\n",
      "\n",
      "-\n",
      "Input sentence: Let's go!\n",
      "Decoded sentence: 我們走吧!\n",
      "\n",
      "-\n",
      "Input sentence: Let's go!\n",
      "Decoded sentence: 我們走吧!\n",
      "\n",
      "-\n",
      "Input sentence: Let's go!\n",
      "Decoded sentence: 我們走吧!\n",
      "\n",
      "-\n",
      "Input sentence: Look out!\n",
      "Decoded sentence: 回頭看！\n",
      "\n",
      "-\n",
      "Input sentence: She runs.\n",
      "Decoded sentence: 她也許會來。\n",
      "\n",
      "-\n",
      "Input sentence: Stand up.\n",
      "Decoded sentence: 你可以慢慢來。\n",
      "\n",
      "-\n",
      "Input sentence: They won.\n",
      "Decoded sentence: 他們想要更多。\n",
      "\n",
      "-\n",
      "Input sentence: Tom died.\n",
      "Decoded sentence: 湯姆去世了。\n",
      "\n",
      "-\n",
      "Input sentence: Tom quit.\n",
      "Decoded sentence: 湯姆不干了。\n",
      "\n",
      "-\n",
      "Input sentence: Tom swam.\n",
      "Decoded sentence: 湯姆游泳了。\n",
      "\n",
      "-\n",
      "Input sentence: Trust me.\n",
      "Decoded sentence: 停止發牢騷。\n",
      "\n",
      "-\n",
      "Input sentence: Try hard.\n",
      "Decoded sentence: 努力。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # 從訓練集中取出一個序列並試著解碼\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
