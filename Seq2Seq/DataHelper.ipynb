{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T03:40:21.500150Z",
     "start_time": "2019-08-25T03:40:21.475220Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/zake7749/Sequence-to-Sequence-101/blob/master/Epoch1-BasicSeq2Seq/dataset/DataHelper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T03:39:37.159089Z",
     "start_time": "2019-08-25T03:39:37.145450Z"
    }
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T03:39:54.510843Z",
     "start_time": "2019-08-25T03:39:54.351191Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.char2idx = {'SOS': 0, 'EOS': 1, 'PAD': 2, 'UNK': 3}\n",
    "        self.idx2char = {0: 'SOS', 1: 'EOS', 2: 'PAD', 3: 'UNK'}\n",
    "        self.num_chars = 4\n",
    "        self.max_length = 0\n",
    "        self.word_list = []\n",
    "\n",
    "    def build_vocab(self, data_path):\n",
    "        \"\"\"Construct the relation between words and indices\"\"\"\n",
    "        with open(data_path, 'r', encoding='utf-8') as dataset:\n",
    "            for word in dataset:\n",
    "                word = word.strip('\\n')\n",
    "\n",
    "                self.word_list.append(word)\n",
    "                if self.max_length < len(word):\n",
    "                    self.max_length = len(word)\n",
    "\n",
    "                chars = self.split_sequence(word)\n",
    "                for char in chars:\n",
    "                    if char not in self.char2idx:\n",
    "                        self.char2idx[char] = self.num_chars\n",
    "                        self.idx2char[self.num_chars] = char\n",
    "                        self.num_chars += 1\n",
    "\n",
    "    def sequence_to_indices(self, sequence, add_eos=False, add_sos=False):\n",
    "        \"\"\"Transform a char sequence to index sequence\n",
    "            :param sequence: a string composed with chars\n",
    "            :param add_eos: if true, add the <EOS> tag at the end of given sentence\n",
    "            :param add_sos: if true, add the <SOS> tag at the beginning of given sentence\n",
    "        \"\"\"\n",
    "        index_sequence = [self.char2idx['SOS']] if add_sos else []\n",
    "\n",
    "        for char in self.split_sequence(sequence):\n",
    "            if char not in self.char2idx:\n",
    "                index_sequence.append((self.char2idx['UNK']))\n",
    "            else:\n",
    "                index_sequence.append(self.char2idx[char])\n",
    "\n",
    "        if add_eos:\n",
    "            index_sequence.append(self.char2idx['EOS'])\n",
    "\n",
    "        return index_sequence\n",
    "\n",
    "    def indices_to_sequence(self, indices):\n",
    "        \"\"\"Transform a list of indices\n",
    "            :param indices: a list\n",
    "        \"\"\"\n",
    "        sequence = \"\"\n",
    "        for idx in indices:\n",
    "            char = self.idx2char[idx]\n",
    "            if char == \"EOS\":\n",
    "                break\n",
    "            else:\n",
    "                sequence += char\n",
    "        return sequence\n",
    "\n",
    "    def split_sequence(self, sequence):\n",
    "        \"\"\"Vary from languages and tasks. In our task, we simply return chars in given sentence\n",
    "        For example:\n",
    "            Input : alphabet\n",
    "            Return: [a, l, p, h, a, b, e, t]\n",
    "        \"\"\"\n",
    "        return [char for char in sequence]\n",
    "\n",
    "    def __str__(self):\n",
    "        str = \"Vocab information:\\n\"\n",
    "        for idx, char in self.idx2char.items():\n",
    "            str += \"Char: %s Index: %d\\n\" % (char, idx)\n",
    "        return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer(object):\n",
    "\n",
    "    def __init__(self, path, use_cuda):\n",
    "        self.indices_sequences = []\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # Load and build the vocab\n",
    "        self.vocab = Vocabulary()\n",
    "        self.vocab.build_vocab(path)\n",
    "        self.PAD_ID = self.vocab.char2idx[\"PAD\"]\n",
    "        self.SOS_ID = self.vocab.char2idx[\"SOS\"]\n",
    "        self.vocab_size = self.vocab.num_chars\n",
    "        self.max_length = self.vocab.max_length\n",
    "\n",
    "        self._build_training_set(path)\n",
    "\n",
    "    def _build_training_set(self, path):\n",
    "        # Change sentences to indices, and append <EOS> at the end of all pairs\n",
    "        for word in self.vocab.word_list:\n",
    "            indices_seq = self.vocab.sequence_to_indices(word, add_eos=True)\n",
    "            # input and target are the same in auto-encoder\n",
    "            self.indices_sequences.append([indices_seq, indices_seq[:]])\n",
    "\n",
    "    def mini_batches(self, batch_size):\n",
    "        input_batches = []\n",
    "        target_batches = []\n",
    "\n",
    "        np.random.shuffle(self.indices_sequences)\n",
    "        mini_batches = [\n",
    "            self.indices_sequences[k: k + batch_size]\n",
    "            for k in range(0, len(self.indices_sequences), batch_size)\n",
    "        ]\n",
    "\n",
    "        for batch in mini_batches:\n",
    "            seq_pairs = sorted(batch, key=lambda seqs: len(seqs[0]), reverse=True)  # sorted by input_lengths\n",
    "            input_seqs = [pair[0] for pair in seq_pairs]\n",
    "            target_seqs = [pair[1] for pair in seq_pairs]\n",
    "\n",
    "            input_lengths = [len(s) for s in input_seqs]\n",
    "            in_max = input_lengths[0]\n",
    "            input_padded = [self.pad_sequence(s, in_max) for s in input_seqs]\n",
    "\n",
    "            target_lengths = [len(s) for s in target_seqs]\n",
    "            out_max = target_lengths[0]\n",
    "            target_padded = [self.pad_sequence(s, out_max) for s in target_seqs]\n",
    "\n",
    "            input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)  # time * batch\n",
    "            target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1)  # time * batch\n",
    "\n",
    "            if self.use_cuda:\n",
    "                input_var = input_var.cuda()\n",
    "                target_var = target_var.cuda()\n",
    "\n",
    "            yield (input_var, input_lengths), (target_var, target_lengths)\n",
    "\n",
    "    def pad_sequence(self, sequence, max_length):\n",
    "        sequence += [self.PAD_ID for i in range(max_length - len(sequence))]\n",
    "        return sequence\n",
    "\n",
    "    def evaluation_batch(self, words):\n",
    "        \"\"\"\n",
    "        Prepare a batch of var for evaluating\n",
    "        :param words: a list, store the testing data \n",
    "        :return: evaluation_batch\n",
    "        \"\"\"\n",
    "        evaluation_batch = []\n",
    "\n",
    "        for word in words:\n",
    "            indices_seq = self.vocab.sequence_to_indices(word, add_eos=True)\n",
    "            evaluation_batch.append([indices_seq])\n",
    "\n",
    "        seq_pairs = sorted(evaluation_batch, key=lambda seqs: len(seqs[0]), reverse=True)\n",
    "        input_seqs = [pair[0] for pair in seq_pairs]\n",
    "        input_lengths = [len(s) for s in input_seqs]\n",
    "        in_max = input_lengths[0]\n",
    "        input_padded = [self.pad_sequence(s, in_max) for s in input_seqs]\n",
    "\n",
    "        input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)  # time * batch\n",
    "\n",
    "        if self.use_cuda:\n",
    "            input_var = input_var.cuda()\n",
    "\n",
    "        return input_var, input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.build_vocab('Google-10000-English.txt')\n",
    "print(vocab)\n",
    "\n",
    "test = \"helloworld\"\n",
    "print(\"Sequence before transformed:\", test)\n",
    "ids = vocab.sequence_to_indices(test)\n",
    "print(\"Indices sequence:\", ids)\n",
    "sent = vocab.indices_to_sequence(ids)\n",
    "print(\"Sequence after transformed:\",sent)\n",
    "\n",
    "data_transformer = DataTransformer('Google-10000-English.txt', use_cuda=False)\n",
    "\n",
    "for ib, tb in data_transformer.mini_batches(batch_size=3):\n",
    "    print(\"B0-0\")\n",
    "    print(ib, tb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T03:46:57.807187Z",
     "start_time": "2019-08-25T03:46:57.754598Z"
    }
   },
   "outputs": [],
   "source": [
    "%magic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
