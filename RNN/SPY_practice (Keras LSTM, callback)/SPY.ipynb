{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on 20190203\n",
    "Reviewed on 20190203\n",
    "\n",
    "@author: Ross.Lee\n",
    "\n",
    "Ref: https://medium.com/@daniel820710/%E5%88%A9%E7%94%A8keras%E5%BB%BA%E6%A7%8Blstm%E6%A8%A1%E5%9E%8B-%E4%BB%A5stock-prediction-%E7%82%BA%E4%BE%8B-1-67456e0a0b\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T19:09:38.790586Z",
     "start_time": "2019-02-03T19:09:37.197581Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector, LeakyReLU, BatchNormalization, Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T19:10:20.495332Z",
     "start_time": "2019-02-03T19:10:17.521223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (5680, 30, 10)\n",
      "Y_train shape (5680, 5)\n"
     ]
    }
   ],
   "source": [
    "def readTrain():\n",
    "    train = pd.read_csv(\"SPY.csv\")\n",
    "    return train\n",
    "\n",
    "\n",
    "def augFeatures(train):\n",
    "    train[\"Date\"] = pd.to_datetime(train[\"Date\"])\n",
    "    train[\"year\"] = train[\"Date\"].dt.year\n",
    "    train[\"month\"] = train[\"Date\"].dt.month\n",
    "    train[\"date\"] = train[\"Date\"].dt.day\n",
    "    train[\"day\"] = train[\"Date\"].dt.dayofweek\n",
    "    return train\n",
    "\n",
    "\n",
    "def normalize(train):\n",
    "    train = train.drop([\"Date\"], axis=1)\n",
    "    train_norm = train.apply(lambda x: (\n",
    "        x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "    return train_norm\n",
    "\n",
    "\n",
    "def buildTrain(train, pastDay=30, futureDay=5):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0] - futureDay - pastDay):\n",
    "        # ex. 0-30 (idx 0~29), 1-31 (idx 1~30)\n",
    "        X_train.append(np.array(train.iloc[i: i + pastDay]))\n",
    "        Y_train.append(\n",
    "            np.array(train.iloc[i + pastDay: i + pastDay + futureDay][\"Adj Close\"]))\n",
    "    return np.array(X_train), np.array(Y_train)\n",
    "\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    np.random.seed(10)\n",
    "    randomList = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randomList)\n",
    "    return X[randomList], Y[randomList]\n",
    "\n",
    "\n",
    "def splitData(X, Y, rate):\n",
    "    X_train = X[int(X.shape[0] * rate):]\n",
    "    Y_train = Y[int(Y.shape[0] * rate):]\n",
    "    X_val = X[:int(X.shape[0] * rate)]\n",
    "    Y_val = Y[:int(Y.shape[0] * rate)]\n",
    "    return X_train, Y_train, X_val, Y_val\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        # read SPY.csv\n",
    "    train = readTrain()\n",
    "\n",
    "    # Augment the features (year, month, date, day)\n",
    "    train_Aug = augFeatures(train)\n",
    "\n",
    "    # Normalization\n",
    "    train_norm = normalize(train_Aug)\n",
    "\n",
    "    # build Data, use last 30 days to predict next 5 days\n",
    "    X_train, Y_train = buildTrain(train_norm, 30, 5)\n",
    "\n",
    "    # shuffle the data, and random seed is 10\n",
    "    X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "    # split training data and validation data\n",
    "    X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "    # X_trian: (5710, 30, 10)\n",
    "    # Y_train: (5710, 5, 1)\n",
    "    # X_val: (634, 30, 10)\n",
    "    # Y_val: (634, 5, 1)\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"Y_train shape\", Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T12:29:40.160297Z",
     "start_time": "2019-02-03T12:29:40.154949Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Some Notes\n",
    " - RNN: (None, Timestep, Features)，None is the batch size\n",
    "    - example \n",
    "        - one batch:  \n",
    "                A  B  C  D  \n",
    "              0 X  X  X  X  \n",
    "              1 X  X  X  X  \n",
    "              2 X  X  X  X  \n",
    "              3 X  X  X  X  \n",
    "        - 0~3 -> Timestep: RNN的每個sub-iter就會吃一列，全部跑完後再BackProp回來  \n",
    "        - A~B -> Features: 每次RNN吃的特徵數目\n",
    " \n",
    " - One to one -> Timestep只有1，預測的y也是1筆。例如今天預測明天\n",
    " - many to one -> Timestep不只1，預測的y為1筆。例如前一個月預測明天\n",
    " - one to many -> Timestep只有1\n",
    " - many to many -> Timestep不只1，預測的y也不只1筆。例如前一個月預測未來5天\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One to One RNN\n",
    "`由於是一對一模型，因此return_sequences 也可設為False ，但Y_train 以及Y_val的維度需改為二維(5710,1)以及(634,1)，因為有return_sequences，所以會輸出timestep的維度。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T10:31:22.437955Z",
     "start_time": "2019-02-03T10:31:22.435096Z"
    }
   },
   "source": [
    "### Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T19:10:26.551520Z",
     "start_time": "2019-02-03T19:10:26.545890Z"
    }
   },
   "outputs": [],
   "source": [
    "def buildOneToOneModel(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, input_shape=(shape[1], shape[2]), return_sequences=True))\n",
    "    # output shape: (1, 1)\n",
    "    model.add(TimeDistributed(Dense(1)))    # or use model.add(Dense(1))\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the one to one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T19:11:40.990128Z",
     "start_time": "2019-02-03T19:11:22.393601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (5710, 1, 10)\n",
      "Y_train shape (5710, 1, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 1, 10)             840       \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 1, 1)              11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5710 samples, validate on 634 samples\n",
      "Epoch 1/1000\n",
      "5710/5710 [==============================] - 1s 123us/step - loss: 0.0719 - val_loss: 0.0531\n",
      "Epoch 2/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 0.0424 - val_loss: 0.0289\n",
      "Epoch 3/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 0.0202 - val_loss: 0.0114\n",
      "Epoch 4/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 0.0065 - val_loss: 0.0027\n",
      "Epoch 5/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 0.0014 - val_loss: 6.0477e-04\n",
      "Epoch 6/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 4.2860e-04 - val_loss: 3.1414e-04\n",
      "Epoch 7/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 2.8866e-04 - val_loss: 2.4066e-04\n",
      "Epoch 8/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3593e-04 - val_loss: 2.0358e-04\n",
      "Epoch 9/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.0660e-04 - val_loss: 1.8354e-04\n",
      "Epoch 10/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 1.8869e-04 - val_loss: 1.7098e-04\n",
      "Epoch 11/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 1.7587e-04 - val_loss: 1.6131e-04\n",
      "Epoch 12/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 1.6586e-04 - val_loss: 1.5330e-04\n",
      "Epoch 13/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 1.5675e-04 - val_loss: 1.4670e-04\n",
      "Epoch 14/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 1.4867e-04 - val_loss: 1.3961e-04\n",
      "Epoch 15/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 1.4092e-04 - val_loss: 1.3334e-04\n",
      "Epoch 16/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 1.3397e-04 - val_loss: 1.2658e-04\n",
      "Epoch 17/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 1.2678e-04 - val_loss: 1.2045e-04\n",
      "Epoch 18/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 1.2076e-04 - val_loss: 1.1460e-04\n",
      "Epoch 19/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 1.1421e-04 - val_loss: 1.0931e-04\n",
      "Epoch 20/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 1.0800e-04 - val_loss: 1.0378e-04\n",
      "Epoch 21/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 1.0235e-04 - val_loss: 9.8864e-05\n",
      "Epoch 22/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 9.7122e-05 - val_loss: 9.4554e-05\n",
      "Epoch 23/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 9.2233e-05 - val_loss: 9.0147e-05\n",
      "Epoch 24/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 8.7514e-05 - val_loss: 8.5692e-05\n",
      "Epoch 25/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 8.3418e-05 - val_loss: 8.1816e-05\n",
      "Epoch 26/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 7.9326e-05 - val_loss: 7.8328e-05\n",
      "Epoch 27/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 7.5544e-05 - val_loss: 7.5055e-05\n",
      "Epoch 28/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 7.2298e-05 - val_loss: 7.1991e-05\n",
      "Epoch 29/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 6.9103e-05 - val_loss: 6.9238e-05\n",
      "Epoch 30/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 6.6429e-05 - val_loss: 6.6710e-05\n",
      "Epoch 31/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 6.3557e-05 - val_loss: 6.4413e-05\n",
      "Epoch 32/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 6.1298e-05 - val_loss: 6.2550e-05\n",
      "Epoch 33/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.9199e-05 - val_loss: 6.0552e-05\n",
      "Epoch 34/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.7405e-05 - val_loss: 5.8929e-05\n",
      "Epoch 35/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.5587e-05 - val_loss: 5.7193e-05\n",
      "Epoch 36/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.4038e-05 - val_loss: 5.5907e-05\n",
      "Epoch 37/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.2553e-05 - val_loss: 5.4742e-05\n",
      "Epoch 38/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.1520e-05 - val_loss: 5.3598e-05\n",
      "Epoch 39/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.0373e-05 - val_loss: 5.2759e-05\n",
      "Epoch 40/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.9318e-05 - val_loss: 5.2359e-05\n",
      "Epoch 41/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.8691e-05 - val_loss: 5.1364e-05\n",
      "Epoch 42/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.7714e-05 - val_loss: 5.0410e-05\n",
      "Epoch 43/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.6877e-05 - val_loss: 5.0052e-05\n",
      "Epoch 44/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.6369e-05 - val_loss: 4.9345e-05\n",
      "Epoch 45/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 4.5641e-05 - val_loss: 4.8401e-05\n",
      "Epoch 46/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.5074e-05 - val_loss: 4.7740e-05\n",
      "Epoch 47/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.4609e-05 - val_loss: 4.7312e-05\n",
      "Epoch 48/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.4171e-05 - val_loss: 4.6902e-05\n",
      "Epoch 49/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.3744e-05 - val_loss: 4.6249e-05\n",
      "Epoch 50/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.3202e-05 - val_loss: 4.6228e-05\n",
      "Epoch 51/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.2950e-05 - val_loss: 4.5716e-05\n",
      "Epoch 52/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.2389e-05 - val_loss: 4.5098e-05\n",
      "Epoch 53/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.1975e-05 - val_loss: 4.4401e-05\n",
      "Epoch 54/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.1649e-05 - val_loss: 4.4312e-05\n",
      "Epoch 55/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.1305e-05 - val_loss: 4.3977e-05\n",
      "Epoch 56/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.0897e-05 - val_loss: 4.3542e-05\n",
      "Epoch 57/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 4.0731e-05 - val_loss: 4.3199e-05\n",
      "Epoch 58/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.0267e-05 - val_loss: 4.2952e-05\n",
      "Epoch 59/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.9975e-05 - val_loss: 4.2033e-05\n",
      "Epoch 60/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.9701e-05 - val_loss: 4.1920e-05\n",
      "Epoch 61/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.9272e-05 - val_loss: 4.1455e-05\n",
      "Epoch 62/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.9274e-05 - val_loss: 4.1478e-05\n",
      "Epoch 63/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.8569e-05 - val_loss: 4.0674e-05\n",
      "Epoch 64/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 3.8519e-05 - val_loss: 4.0581e-05\n",
      "Epoch 65/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.7945e-05 - val_loss: 4.0280e-05\n",
      "Epoch 66/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.7860e-05 - val_loss: 3.9673e-05\n",
      "Epoch 67/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.7492e-05 - val_loss: 3.9328e-05\n",
      "Epoch 68/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 3.7132e-05 - val_loss: 3.9518e-05\n",
      "Epoch 69/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 3.6950e-05 - val_loss: 3.8465e-05\n",
      "Epoch 70/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.6626e-05 - val_loss: 3.8006e-05\n",
      "Epoch 71/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.6206e-05 - val_loss: 3.8843e-05\n",
      "Epoch 72/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.6180e-05 - val_loss: 3.7545e-05\n",
      "Epoch 73/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.5745e-05 - val_loss: 3.7272e-05\n",
      "Epoch 74/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.5141e-05 - val_loss: 3.7087e-05\n",
      "Epoch 75/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.4982e-05 - val_loss: 3.6382e-05\n",
      "Epoch 76/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.5179e-05 - val_loss: 3.6535e-05\n",
      "Epoch 77/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.4435e-05 - val_loss: 3.6537e-05\n",
      "Epoch 78/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 3.3995e-05 - val_loss: 3.5519e-05\n",
      "Epoch 79/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 3.3826e-05 - val_loss: 3.5195e-05\n",
      "Epoch 80/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.3464e-05 - val_loss: 3.4785e-05\n",
      "Epoch 81/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.3423e-05 - val_loss: 3.5256e-05\n",
      "Epoch 82/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.2951e-05 - val_loss: 3.4277e-05\n",
      "Epoch 83/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.2700e-05 - val_loss: 3.4443e-05\n",
      "Epoch 84/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 3.2704e-05 - val_loss: 3.3380e-05\n",
      "Epoch 85/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 3.2113e-05 - val_loss: 3.3827e-05\n",
      "Epoch 86/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.2030e-05 - val_loss: 3.2775e-05\n",
      "Epoch 87/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.1796e-05 - val_loss: 3.2896e-05\n",
      "Epoch 88/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.1585e-05 - val_loss: 3.2674e-05\n",
      "Epoch 89/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 3.1408e-05 - val_loss: 3.1764e-05\n",
      "Epoch 90/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.0895e-05 - val_loss: 3.2208e-05\n",
      "Epoch 91/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.0995e-05 - val_loss: 3.1909e-05\n",
      "Epoch 92/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.0346e-05 - val_loss: 3.1405e-05\n",
      "Epoch 93/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.0228e-05 - val_loss: 3.0815e-05\n",
      "Epoch 94/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.0599e-05 - val_loss: 3.1246e-05\n",
      "Epoch 95/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.9720e-05 - val_loss: 3.0956e-05\n",
      "Epoch 96/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.9433e-05 - val_loss: 2.9973e-05\n",
      "Epoch 97/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.9357e-05 - val_loss: 2.9798e-05\n",
      "Epoch 98/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.8988e-05 - val_loss: 2.9438e-05\n",
      "Epoch 99/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.9126e-05 - val_loss: 2.9392e-05\n",
      "Epoch 100/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.8673e-05 - val_loss: 2.9294e-05\n",
      "Epoch 101/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.8080e-05 - val_loss: 2.8928e-05\n",
      "Epoch 102/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.8031e-05 - val_loss: 2.8745e-05\n",
      "Epoch 103/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 2.8053e-05 - val_loss: 2.8646e-05\n",
      "Epoch 104/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.7746e-05 - val_loss: 2.7918e-05\n",
      "Epoch 105/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.7669e-05 - val_loss: 2.7863e-05\n",
      "Epoch 106/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.7470e-05 - val_loss: 2.7565e-05\n",
      "Epoch 107/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.7337e-05 - val_loss: 2.8126e-05\n",
      "Epoch 108/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.7803e-05 - val_loss: 2.7003e-05\n",
      "Epoch 109/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.7103e-05 - val_loss: 2.7478e-05\n",
      "Epoch 110/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.6724e-05 - val_loss: 2.7302e-05\n",
      "Epoch 111/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.6494e-05 - val_loss: 2.6508e-05\n",
      "Epoch 112/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.6667e-05 - val_loss: 2.7897e-05\n",
      "Epoch 113/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.7184e-05 - val_loss: 2.6130e-05\n",
      "Epoch 114/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.6220e-05 - val_loss: 2.6914e-05\n",
      "Epoch 115/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.6372e-05 - val_loss: 2.6194e-05\n",
      "Epoch 116/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 2.5698e-05 - val_loss: 2.5691e-05\n",
      "Epoch 117/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5634e-05 - val_loss: 2.5571e-05\n",
      "Epoch 118/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5460e-05 - val_loss: 2.5140e-05\n",
      "Epoch 119/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5396e-05 - val_loss: 2.5902e-05\n",
      "Epoch 120/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5664e-05 - val_loss: 2.6088e-05\n",
      "Epoch 121/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5632e-05 - val_loss: 2.5794e-05\n",
      "Epoch 122/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5739e-05 - val_loss: 2.5949e-05\n",
      "Epoch 123/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5422e-05 - val_loss: 2.4695e-05\n",
      "Epoch 124/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4906e-05 - val_loss: 2.5337e-05\n",
      "Epoch 125/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5688e-05 - val_loss: 2.4483e-05\n",
      "Epoch 126/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5014e-05 - val_loss: 2.5413e-05\n",
      "Epoch 127/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4673e-05 - val_loss: 2.4821e-05\n",
      "Epoch 128/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5278e-05 - val_loss: 2.4175e-05\n",
      "Epoch 129/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4577e-05 - val_loss: 2.4365e-05\n",
      "Epoch 130/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4805e-05 - val_loss: 2.5859e-05\n",
      "Epoch 131/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.4685e-05 - val_loss: 2.4257e-05\n",
      "Epoch 132/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4535e-05 - val_loss: 2.3944e-05\n",
      "Epoch 133/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4599e-05 - val_loss: 2.3770e-05\n",
      "Epoch 134/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4446e-05 - val_loss: 2.3772e-05\n",
      "Epoch 135/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4342e-05 - val_loss: 2.4016e-05\n",
      "Epoch 136/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4288e-05 - val_loss: 2.3793e-05\n",
      "Epoch 137/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4664e-05 - val_loss: 2.3292e-05\n",
      "Epoch 138/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4515e-05 - val_loss: 2.3676e-05\n",
      "Epoch 139/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4261e-05 - val_loss: 2.3702e-05\n",
      "Epoch 140/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.4301e-05 - val_loss: 2.3774e-05\n",
      "Epoch 141/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4422e-05 - val_loss: 2.3602e-05\n",
      "Epoch 142/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4326e-05 - val_loss: 2.3895e-05\n",
      "Epoch 143/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3925e-05 - val_loss: 2.3473e-05\n",
      "Epoch 144/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3821e-05 - val_loss: 2.2932e-05\n",
      "Epoch 145/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3923e-05 - val_loss: 2.2983e-05\n",
      "Epoch 146/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3878e-05 - val_loss: 2.3389e-05\n",
      "Epoch 147/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4161e-05 - val_loss: 2.2624e-05\n",
      "Epoch 148/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4275e-05 - val_loss: 2.3128e-05\n",
      "Epoch 149/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3715e-05 - val_loss: 2.3895e-05\n",
      "Epoch 150/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3879e-05 - val_loss: 2.3309e-05\n",
      "Epoch 151/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3567e-05 - val_loss: 2.2980e-05\n",
      "Epoch 152/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3987e-05 - val_loss: 2.2634e-05\n",
      "Epoch 153/1000\n",
      "5710/5710 [==============================] - 0s 12us/step - loss: 2.3502e-05 - val_loss: 2.2880e-05\n",
      "Epoch 154/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3487e-05 - val_loss: 2.3203e-05\n",
      "Epoch 155/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3945e-05 - val_loss: 2.8492e-05\n",
      "Epoch 156/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4439e-05 - val_loss: 2.2935e-05\n",
      "Epoch 157/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3866e-05 - val_loss: 2.3808e-05\n",
      "Epoch 158/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4060e-05 - val_loss: 2.3964e-05\n",
      "Epoch 159/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3575e-05 - val_loss: 2.2361e-05\n",
      "Epoch 160/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 2.3600e-05 - val_loss: 2.3717e-05\n",
      "Epoch 161/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 2.3904e-05 - val_loss: 2.4798e-05\n",
      "Epoch 162/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3972e-05 - val_loss: 2.2760e-05\n",
      "Epoch 163/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3381e-05 - val_loss: 2.2684e-05\n",
      "Epoch 164/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3784e-05 - val_loss: 2.3648e-05\n",
      "Epoch 165/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 2.3683e-05 - val_loss: 2.3120e-05\n",
      "Epoch 166/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 2.3301e-05 - val_loss: 2.2198e-05\n",
      "Epoch 167/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3709e-05 - val_loss: 2.1966e-05\n",
      "Epoch 168/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 2.3373e-05 - val_loss: 2.3913e-05\n",
      "Epoch 169/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3905e-05 - val_loss: 2.6165e-05\n",
      "Epoch 170/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4301e-05 - val_loss: 2.3688e-05\n",
      "Epoch 171/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3890e-05 - val_loss: 2.2343e-05\n",
      "Epoch 172/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3092e-05 - val_loss: 2.2147e-05\n",
      "Epoch 173/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4392e-05 - val_loss: 2.2941e-05\n",
      "Epoch 174/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3557e-05 - val_loss: 2.2428e-05\n",
      "Epoch 175/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3443e-05 - val_loss: 2.6579e-05\n",
      "Epoch 176/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4085e-05 - val_loss: 2.2638e-05\n",
      "Epoch 177/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 2.3752e-05 - val_loss: 2.2348e-05\n",
      "Epoch 178/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 2.3123e-05 - val_loss: 2.3359e-05\n",
      "Epoch 179/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 2.3890e-05 - val_loss: 2.2598e-05\n",
      "Epoch 180/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 2.3344e-05 - val_loss: 2.2790e-05\n",
      "Epoch 181/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 2.3470e-05 - val_loss: 2.3031e-05\n",
      "Epoch 182/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 2.3923e-05 - val_loss: 2.2016e-05\n",
      "Epoch 00182: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8f3f978710>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_Aug = augFeatures(train=train)\n",
    "train_norm = normalize(train=train_Aug)\n",
    "# change the last day and next day\n",
    "X_train, Y_train = buildTrain(train=train_norm, pastDay=1, futureDay=1) # one to one\n",
    "X_train, Y_train = shuffle(X=X_train, Y=Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X=X_train, Y=Y_train, rate=0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:, np.newaxis]\n",
    "Y_val = Y_val[:, np.newaxis]\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"Y_train shape\", Y_train.shape)\n",
    "\n",
    "# Keras callback is useful: https://keras.io/zh/callbacks/\n",
    "\n",
    "model = buildOneToOneModel(X_train.shape)\n",
    "earlystopping = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "reducelr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.001)\n",
    "csvlog = CSVLogger('one2one_train_log')\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128,\n",
    "          validation_data=(X_val, Y_val), callbacks=[earlystopping, reducelr, csvlog])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T19:12:09.120610Z",
     "start_time": "2019-02-03T19:12:09.112785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.16894309]]\n",
      "\n",
      " [[-0.23473354]]\n",
      "\n",
      " [[-0.07928583]]\n",
      "\n",
      " [[-0.0411837 ]]\n",
      "\n",
      " [[ 0.00463182]]\n",
      "\n",
      " [[-0.2941407 ]]\n",
      "\n",
      " [[-0.02194393]]\n",
      "\n",
      " [[-0.03507504]]\n",
      "\n",
      " [[-0.00897813]]\n",
      "\n",
      " [[-0.02777452]]]\n",
      "[[[ 1.70236981e-01]]\n",
      "\n",
      " [[-2.32875884e-01]]\n",
      "\n",
      " [[-8.32417025e-02]]\n",
      "\n",
      " [[-3.34503232e-02]]\n",
      "\n",
      " [[-1.48952916e-04]]\n",
      "\n",
      " [[-2.93853160e-01]]\n",
      "\n",
      " [[-1.93453977e-02]]\n",
      "\n",
      " [[-3.43362259e-02]]\n",
      "\n",
      " [[-2.59278393e-03]]\n",
      "\n",
      " [[-2.75129136e-02]]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_train[0:10]))\n",
    "print(Y_train[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:19:47.331084Z",
     "start_time": "2019-02-03T09:19:47.326955Z"
    }
   },
   "source": [
    "## Many ot one\n",
    "`LSTM參數return_sequences=False ，未設定時default也為False，而且不可使用TimeDistribution`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:24:12.335393Z",
     "start_time": "2019-02-03T14:24:12.330014Z"
    }
   },
   "outputs": [],
   "source": [
    "def buildManyToOneModel(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, input_shape=(shape[1], shape[2])))\n",
    "    # output shape: (1, 1)\n",
    "    model.add(Dense(1))  # 不可使用TimeDistribution，否則輸出會變成(batch, 30, 10, 1)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:17:29.853434Z",
     "start_time": "2019-02-03T14:17:29.597465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_24 (LSTM)               (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# return_sequences=False && return_state=False 意義\n",
    "# https://blog.csdn.net/u011327333/article/details/78501054\n",
    "\"\"\"\n",
    "1. return_sequences=False && return_state=False\n",
    "    Keras API 中，return_sequences和return_state默認就是false。此時只會返回一個hidden state 值。\n",
    "如果input 數據包含多個時間步，則這個hidden state 是最後一個時間步的結果\n",
    "\n",
    "2. return_sequences=True && return_state=False\n",
    "    e.g. LSTM(1, return_sequences=True)\n",
    "    輸出的hidden state 包含全部時間步的結果。\n",
    "    \n",
    "3. return_sequences=True && return_state=True\n",
    "    lstm1, state_h, state_c = LSTM(1, return_sequences=True, return_state=True)\n",
    "    此時，我們既要輸出全部時間步的hidden state ，又要輸出cell state。\n",
    "    lstm1 存放的就是全部時間步的hidden state。\n",
    "    state_h 存放的是最後一個時間步的hidden state\n",
    "    state_c 存放的是最後一個時間步的cell state\n",
    "\n",
    "    一個輸出例子，假設我們輸入的時間步time step=3\n",
    "    [array([[[-0.02145359],\n",
    "        [-0.0540871 ],\n",
    "        [-0.09228823]]], dtype=float32),  # lstm1\n",
    "     array([[-0.09228823]], dtype=float32),  # state_h\n",
    "     array([[-0.19803026]], dtype=float32)]  # state_c\n",
    "     \n",
    "ref: https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, input_shape=(30, 10)))  # 吃最後一部hidden_state\n",
    "# 使用TimeDistribution的return_seq要等於True，輸出會變成(batch, 30, 1) --> 吃每一步的hidden_state\n",
    "# model.add(TimeDistributed(Dense(1)))\n",
    "model.add(Dense(1)) \n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train many to one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:20:34.433972Z",
     "start_time": "2019-02-03T14:18:41.890167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (5684, 30, 10)\n",
      "Y_train shape (5684, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_27 (LSTM)               (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5684 samples, validate on 631 samples\n",
      "Epoch 1/1000\n",
      "5684/5684 [==============================] - 2s 376us/step - loss: 0.1257 - val_loss: 0.0485\n",
      "Epoch 2/1000\n",
      "5684/5684 [==============================] - 1s 123us/step - loss: 0.0196 - val_loss: 0.0064\n",
      "Epoch 3/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 0.0030 - val_loss: 0.0015\n",
      "Epoch 4/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 0.0011 - val_loss: 8.5506e-04\n",
      "Epoch 5/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 6.9541e-04 - val_loss: 5.9005e-04\n",
      "Epoch 6/1000\n",
      "5684/5684 [==============================] - 1s 118us/step - loss: 5.0963e-04 - val_loss: 4.5813e-04\n",
      "Epoch 7/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 4.0335e-04 - val_loss: 3.7348e-04\n",
      "Epoch 8/1000\n",
      "5684/5684 [==============================] - 1s 113us/step - loss: 3.3257e-04 - val_loss: 3.1394e-04\n",
      "Epoch 9/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 2.8092e-04 - val_loss: 2.7170e-04\n",
      "Epoch 10/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 2.4378e-04 - val_loss: 2.3466e-04\n",
      "Epoch 11/1000\n",
      "5684/5684 [==============================] - 1s 112us/step - loss: 2.1403e-04 - val_loss: 2.0907e-04\n",
      "Epoch 12/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 1.9138e-04 - val_loss: 1.8763e-04\n",
      "Epoch 13/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 1.7169e-04 - val_loss: 1.7366e-04\n",
      "Epoch 14/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 1.5639e-04 - val_loss: 1.5764e-04\n",
      "Epoch 15/1000\n",
      "5684/5684 [==============================] - 1s 111us/step - loss: 1.4214e-04 - val_loss: 1.4316e-04\n",
      "Epoch 16/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 1.3012e-04 - val_loss: 1.3229e-04\n",
      "Epoch 17/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 1.2053e-04 - val_loss: 1.2287e-04\n",
      "Epoch 18/1000\n",
      "5684/5684 [==============================] - 1s 115us/step - loss: 1.1189e-04 - val_loss: 1.1471e-04\n",
      "Epoch 19/1000\n",
      "5684/5684 [==============================] - 1s 116us/step - loss: 1.0495e-04 - val_loss: 1.0732e-04\n",
      "Epoch 20/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 9.8734e-05 - val_loss: 1.0228e-04\n",
      "Epoch 21/1000\n",
      "5684/5684 [==============================] - 1s 113us/step - loss: 9.2443e-05 - val_loss: 9.6759e-05\n",
      "Epoch 22/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 8.6914e-05 - val_loss: 9.1235e-05\n",
      "Epoch 23/1000\n",
      "5684/5684 [==============================] - 1s 112us/step - loss: 8.2465e-05 - val_loss: 8.6494e-05\n",
      "Epoch 24/1000\n",
      "5684/5684 [==============================] - 1s 103us/step - loss: 7.8910e-05 - val_loss: 8.6496e-05\n",
      "Epoch 25/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 7.5382e-05 - val_loss: 7.9327e-05\n",
      "Epoch 26/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 7.1521e-05 - val_loss: 7.6175e-05\n",
      "Epoch 27/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 6.9186e-05 - val_loss: 7.3604e-05\n",
      "Epoch 28/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 6.6463e-05 - val_loss: 7.1557e-05\n",
      "Epoch 29/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 6.5551e-05 - val_loss: 7.0507e-05\n",
      "Epoch 30/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 6.3178e-05 - val_loss: 6.7224e-05\n",
      "Epoch 31/1000\n",
      "5684/5684 [==============================] - 1s 126us/step - loss: 6.2086e-05 - val_loss: 6.6126e-05\n",
      "Epoch 32/1000\n",
      "5684/5684 [==============================] - 1s 117us/step - loss: 5.9810e-05 - val_loss: 6.4830e-05\n",
      "Epoch 33/1000\n",
      "5684/5684 [==============================] - 1s 116us/step - loss: 5.8386e-05 - val_loss: 6.2580e-05\n",
      "Epoch 34/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 5.7157e-05 - val_loss: 6.2199e-05\n",
      "Epoch 35/1000\n",
      "5684/5684 [==============================] - 1s 119us/step - loss: 5.6863e-05 - val_loss: 6.1671e-05\n",
      "Epoch 36/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 5.5099e-05 - val_loss: 6.0965e-05\n",
      "Epoch 37/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 5.4635e-05 - val_loss: 5.9491e-05\n",
      "Epoch 38/1000\n",
      "5684/5684 [==============================] - 1s 109us/step - loss: 5.3410e-05 - val_loss: 5.7900e-05\n",
      "Epoch 39/1000\n",
      "5684/5684 [==============================] - 1s 114us/step - loss: 5.2829e-05 - val_loss: 5.9149e-05\n",
      "Epoch 40/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 5.2949e-05 - val_loss: 5.6622e-05\n",
      "Epoch 41/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 5.1208e-05 - val_loss: 5.7010e-05\n",
      "Epoch 42/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 5.1329e-05 - val_loss: 5.5519e-05\n",
      "Epoch 43/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 5.0889e-05 - val_loss: 5.5081e-05\n",
      "Epoch 44/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 5.0381e-05 - val_loss: 5.4165e-05\n",
      "Epoch 45/1000\n",
      "5684/5684 [==============================] - 1s 112us/step - loss: 4.9776e-05 - val_loss: 5.4981e-05\n",
      "Epoch 46/1000\n",
      "5684/5684 [==============================] - 1s 114us/step - loss: 4.9526e-05 - val_loss: 5.3924e-05\n",
      "Epoch 47/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 4.8855e-05 - val_loss: 5.2901e-05\n",
      "Epoch 48/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 4.7410e-05 - val_loss: 5.2080e-05\n",
      "Epoch 49/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 4.7005e-05 - val_loss: 5.1355e-05\n",
      "Epoch 50/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 4.6962e-05 - val_loss: 5.1178e-05\n",
      "Epoch 51/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 4.6312e-05 - val_loss: 5.2113e-05\n",
      "Epoch 52/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 4.5559e-05 - val_loss: 5.0243e-05\n",
      "Epoch 53/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 4.5503e-05 - val_loss: 5.0201e-05\n",
      "Epoch 54/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 4.5111e-05 - val_loss: 5.0044e-05\n",
      "Epoch 55/1000\n",
      "5684/5684 [==============================] - 1s 118us/step - loss: 4.5096e-05 - val_loss: 5.0920e-05\n",
      "Epoch 56/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 4.4329e-05 - val_loss: 4.9835e-05\n",
      "Epoch 57/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 4.4406e-05 - val_loss: 5.0050e-05\n",
      "Epoch 58/1000\n",
      "5684/5684 [==============================] - 1s 111us/step - loss: 4.4551e-05 - val_loss: 4.8223e-05\n",
      "Epoch 59/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 4.3621e-05 - val_loss: 4.6886e-05\n",
      "Epoch 60/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 4.2899e-05 - val_loss: 4.7104e-05\n",
      "Epoch 61/1000\n",
      "5684/5684 [==============================] - 1s 111us/step - loss: 4.2464e-05 - val_loss: 4.7016e-05\n",
      "Epoch 62/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 4.2275e-05 - val_loss: 4.8780e-05\n",
      "Epoch 63/1000\n",
      "5684/5684 [==============================] - 1s 112us/step - loss: 4.2085e-05 - val_loss: 4.9088e-05\n",
      "Epoch 64/1000\n",
      "5684/5684 [==============================] - 1s 116us/step - loss: 4.2593e-05 - val_loss: 4.7317e-05\n",
      "Epoch 65/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 4.1305e-05 - val_loss: 4.5599e-05\n",
      "Epoch 66/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 4.1740e-05 - val_loss: 4.8262e-05\n",
      "Epoch 67/1000\n",
      "5684/5684 [==============================] - 1s 113us/step - loss: 4.0848e-05 - val_loss: 4.4956e-05\n",
      "Epoch 68/1000\n",
      "5684/5684 [==============================] - 1s 109us/step - loss: 4.1670e-05 - val_loss: 4.5537e-05\n",
      "Epoch 69/1000\n",
      "5684/5684 [==============================] - 1s 111us/step - loss: 4.0133e-05 - val_loss: 4.4833e-05\n",
      "Epoch 70/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 4.0079e-05 - val_loss: 4.3991e-05\n",
      "Epoch 71/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 3.9074e-05 - val_loss: 4.3712e-05\n",
      "Epoch 72/1000\n",
      "5684/5684 [==============================] - 1s 109us/step - loss: 3.9263e-05 - val_loss: 4.5233e-05\n",
      "Epoch 73/1000\n",
      "5684/5684 [==============================] - 1s 111us/step - loss: 3.9651e-05 - val_loss: 4.3187e-05\n",
      "Epoch 74/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 3.9600e-05 - val_loss: 4.4773e-05\n",
      "Epoch 75/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 3.8808e-05 - val_loss: 4.5226e-05\n",
      "Epoch 76/1000\n",
      "5684/5684 [==============================] - 1s 116us/step - loss: 3.8765e-05 - val_loss: 4.2983e-05\n",
      "Epoch 77/1000\n",
      "5684/5684 [==============================] - 1s 103us/step - loss: 3.9088e-05 - val_loss: 4.5264e-05\n",
      "Epoch 78/1000\n",
      "5684/5684 [==============================] - 1s 109us/step - loss: 3.7487e-05 - val_loss: 4.1866e-05\n",
      "Epoch 79/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 3.7932e-05 - val_loss: 4.2086e-05\n",
      "Epoch 80/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 3.7915e-05 - val_loss: 4.1811e-05\n",
      "Epoch 81/1000\n",
      "5684/5684 [==============================] - 1s 111us/step - loss: 3.7859e-05 - val_loss: 4.1771e-05\n",
      "Epoch 82/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 3.7157e-05 - val_loss: 4.2495e-05\n",
      "Epoch 83/1000\n",
      "5684/5684 [==============================] - 1s 117us/step - loss: 3.7229e-05 - val_loss: 4.1205e-05\n",
      "Epoch 84/1000\n",
      "5684/5684 [==============================] - 1s 111us/step - loss: 3.6884e-05 - val_loss: 4.8220e-05\n",
      "Epoch 85/1000\n",
      "5684/5684 [==============================] - 1s 109us/step - loss: 3.7398e-05 - val_loss: 4.6217e-05\n",
      "Epoch 86/1000\n",
      "5684/5684 [==============================] - 1s 121us/step - loss: 3.6665e-05 - val_loss: 4.0098e-05\n",
      "Epoch 87/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 3.7195e-05 - val_loss: 4.0695e-05\n",
      "Epoch 88/1000\n",
      "5684/5684 [==============================] - 1s 112us/step - loss: 3.5650e-05 - val_loss: 4.0735e-05\n",
      "Epoch 89/1000\n",
      "5684/5684 [==============================] - 1s 112us/step - loss: 3.5411e-05 - val_loss: 3.9163e-05\n",
      "Epoch 90/1000\n",
      "5684/5684 [==============================] - 1s 113us/step - loss: 3.5358e-05 - val_loss: 4.0079e-05\n",
      "Epoch 91/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 3.6658e-05 - val_loss: 3.9109e-05\n",
      "Epoch 92/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 3.5902e-05 - val_loss: 4.0958e-05\n",
      "Epoch 93/1000\n",
      "5684/5684 [==============================] - 1s 115us/step - loss: 3.5307e-05 - val_loss: 4.0086e-05\n",
      "Epoch 94/1000\n",
      "5684/5684 [==============================] - 1s 111us/step - loss: 3.4847e-05 - val_loss: 3.8002e-05\n",
      "Epoch 95/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 3.4580e-05 - val_loss: 3.8091e-05\n",
      "Epoch 96/1000\n",
      "5684/5684 [==============================] - 1s 115us/step - loss: 3.4425e-05 - val_loss: 3.8299e-05\n",
      "Epoch 97/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 3.4761e-05 - val_loss: 4.0020e-05\n",
      "Epoch 98/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 3.4242e-05 - val_loss: 3.9943e-05\n",
      "Epoch 99/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 3.4569e-05 - val_loss: 3.8927e-05\n",
      "Epoch 100/1000\n",
      "5684/5684 [==============================] - 1s 114us/step - loss: 3.3870e-05 - val_loss: 3.7338e-05\n",
      "Epoch 101/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 3.3088e-05 - val_loss: 3.6724e-05\n",
      "Epoch 102/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 3.3459e-05 - val_loss: 3.7046e-05\n",
      "Epoch 103/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 3.2857e-05 - val_loss: 3.6857e-05\n",
      "Epoch 104/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 3.3485e-05 - val_loss: 4.3098e-05\n",
      "Epoch 105/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 3.4531e-05 - val_loss: 3.6764e-05\n",
      "Epoch 106/1000\n",
      "5684/5684 [==============================] - 1s 117us/step - loss: 3.2365e-05 - val_loss: 3.6011e-05\n",
      "Epoch 107/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 3.4711e-05 - val_loss: 3.8885e-05\n",
      "Epoch 108/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 3.2228e-05 - val_loss: 3.8857e-05\n",
      "Epoch 109/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 3.2918e-05 - val_loss: 3.6043e-05\n",
      "Epoch 110/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 3.2014e-05 - val_loss: 3.5250e-05\n",
      "Epoch 111/1000\n",
      "5684/5684 [==============================] - 1s 116us/step - loss: 3.2376e-05 - val_loss: 4.1740e-05\n",
      "Epoch 112/1000\n",
      "5684/5684 [==============================] - 1s 116us/step - loss: 3.1997e-05 - val_loss: 3.4109e-05\n",
      "Epoch 113/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 3.2170e-05 - val_loss: 3.4647e-05\n",
      "Epoch 114/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 3.3153e-05 - val_loss: 3.8519e-05\n",
      "Epoch 115/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 3.2955e-05 - val_loss: 3.4263e-05\n",
      "Epoch 116/1000\n",
      "5684/5684 [==============================] - 1s 113us/step - loss: 3.2543e-05 - val_loss: 3.6565e-05\n",
      "Epoch 117/1000\n",
      "5684/5684 [==============================] - 1s 119us/step - loss: 3.1912e-05 - val_loss: 4.0024e-05\n",
      "Epoch 118/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 3.2174e-05 - val_loss: 3.5625e-05\n",
      "Epoch 119/1000\n",
      "5684/5684 [==============================] - 1s 117us/step - loss: 3.1056e-05 - val_loss: 3.6719e-05\n",
      "Epoch 120/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 3.0845e-05 - val_loss: 3.5131e-05\n",
      "Epoch 121/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 3.2401e-05 - val_loss: 3.8837e-05\n",
      "Epoch 122/1000\n",
      "5684/5684 [==============================] - 1s 116us/step - loss: 3.0477e-05 - val_loss: 3.4996e-05\n",
      "Epoch 123/1000\n",
      "5684/5684 [==============================] - 1s 112us/step - loss: 3.1192e-05 - val_loss: 3.2992e-05\n",
      "Epoch 124/1000\n",
      "5684/5684 [==============================] - 1s 112us/step - loss: 3.1102e-05 - val_loss: 3.4744e-05\n",
      "Epoch 125/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 3.1011e-05 - val_loss: 3.3688e-05\n",
      "Epoch 126/1000\n",
      "5684/5684 [==============================] - 1s 109us/step - loss: 3.2180e-05 - val_loss: 3.5060e-05\n",
      "Epoch 127/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 3.0704e-05 - val_loss: 3.8046e-05\n",
      "Epoch 128/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 3.0028e-05 - val_loss: 3.8419e-05\n",
      "Epoch 129/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 3.0108e-05 - val_loss: 3.3062e-05\n",
      "Epoch 130/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 2.9709e-05 - val_loss: 3.3995e-05\n",
      "Epoch 131/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 2.9637e-05 - val_loss: 3.2475e-05\n",
      "Epoch 132/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 2.9241e-05 - val_loss: 3.2390e-05\n",
      "Epoch 133/1000\n",
      "5684/5684 [==============================] - 1s 115us/step - loss: 3.0669e-05 - val_loss: 3.5280e-05\n",
      "Epoch 134/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 2.9455e-05 - val_loss: 3.1545e-05\n",
      "Epoch 135/1000\n",
      "5684/5684 [==============================] - 1s 117us/step - loss: 2.8551e-05 - val_loss: 3.1694e-05\n",
      "Epoch 136/1000\n",
      "5684/5684 [==============================] - 1s 117us/step - loss: 3.0340e-05 - val_loss: 3.4679e-05\n",
      "Epoch 137/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5684/5684 [==============================] - 1s 111us/step - loss: 2.9774e-05 - val_loss: 3.2953e-05\n",
      "Epoch 138/1000\n",
      "5684/5684 [==============================] - 1s 114us/step - loss: 2.9559e-05 - val_loss: 3.2007e-05\n",
      "Epoch 139/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 2.8904e-05 - val_loss: 3.6301e-05\n",
      "Epoch 140/1000\n",
      "5684/5684 [==============================] - 1s 120us/step - loss: 2.8463e-05 - val_loss: 3.4718e-05\n",
      "Epoch 141/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 2.8344e-05 - val_loss: 3.0936e-05\n",
      "Epoch 142/1000\n",
      "5684/5684 [==============================] - 1s 109us/step - loss: 2.9023e-05 - val_loss: 3.2627e-05\n",
      "Epoch 143/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 2.8559e-05 - val_loss: 3.0733e-05\n",
      "Epoch 144/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 2.8509e-05 - val_loss: 3.4867e-05\n",
      "Epoch 145/1000\n",
      "5684/5684 [==============================] - 1s 116us/step - loss: 2.8544e-05 - val_loss: 3.0739e-05\n",
      "Epoch 146/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 2.8333e-05 - val_loss: 3.5089e-05\n",
      "Epoch 147/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 3.0042e-05 - val_loss: 5.0956e-05\n",
      "Epoch 148/1000\n",
      "5684/5684 [==============================] - 1s 117us/step - loss: 2.9129e-05 - val_loss: 3.0124e-05\n",
      "Epoch 149/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 2.8050e-05 - val_loss: 3.1165e-05\n",
      "Epoch 150/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 3.0892e-05 - val_loss: 3.1368e-05\n",
      "Epoch 151/1000\n",
      "5684/5684 [==============================] - 1s 114us/step - loss: 2.8081e-05 - val_loss: 3.3839e-05\n",
      "Epoch 152/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 2.7827e-05 - val_loss: 3.1056e-05\n",
      "Epoch 153/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 2.6860e-05 - val_loss: 2.9617e-05\n",
      "Epoch 154/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 2.9276e-05 - val_loss: 3.3814e-05\n",
      "Epoch 155/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 2.9188e-05 - val_loss: 3.0005e-05\n",
      "Epoch 156/1000\n",
      "5684/5684 [==============================] - 1s 111us/step - loss: 3.0963e-05 - val_loss: 3.0555e-05\n",
      "Epoch 157/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 2.7196e-05 - val_loss: 3.0967e-05\n",
      "Epoch 158/1000\n",
      "5684/5684 [==============================] - 1s 113us/step - loss: 2.9065e-05 - val_loss: 3.5649e-05\n",
      "Epoch 159/1000\n",
      "5684/5684 [==============================] - 1s 111us/step - loss: 2.7955e-05 - val_loss: 3.0683e-05\n",
      "Epoch 160/1000\n",
      "5684/5684 [==============================] - 1s 117us/step - loss: 2.6583e-05 - val_loss: 3.1775e-05\n",
      "Epoch 161/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 2.6406e-05 - val_loss: 2.8834e-05\n",
      "Epoch 162/1000\n",
      "5684/5684 [==============================] - 1s 109us/step - loss: 2.7198e-05 - val_loss: 3.0095e-05\n",
      "Epoch 163/1000\n",
      "5684/5684 [==============================] - 1s 113us/step - loss: 2.6980e-05 - val_loss: 3.2210e-05\n",
      "Epoch 164/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 2.7192e-05 - val_loss: 3.0336e-05\n",
      "Epoch 165/1000\n",
      "5684/5684 [==============================] - 1s 119us/step - loss: 2.7252e-05 - val_loss: 2.8836e-05\n",
      "Epoch 166/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 2.8476e-05 - val_loss: 3.0401e-05\n",
      "Epoch 167/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 2.6422e-05 - val_loss: 3.1343e-05\n",
      "Epoch 168/1000\n",
      "5684/5684 [==============================] - 1s 103us/step - loss: 2.8816e-05 - val_loss: 3.4434e-05\n",
      "Epoch 169/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 2.8442e-05 - val_loss: 3.2751e-05\n",
      "Epoch 170/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 2.7597e-05 - val_loss: 2.9414e-05\n",
      "Epoch 171/1000\n",
      "5684/5684 [==============================] - 1s 117us/step - loss: 2.7000e-05 - val_loss: 3.1810e-05\n",
      "Epoch 00171: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdcaea58080>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, pastDay=30, futureDay=1)  # many to one\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "# because no return sequence, Y_train and Y_val shape must be 2 dimension\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, rate=0.1)\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"Y_train shape\", Y_train.shape)\n",
    "\n",
    "model = buildManyToOneModel(X_train.shape)\n",
    "earlystopping = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "reducelr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.001)\n",
    "csvlog = CSVLogger('many2one_train_log')\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128,\n",
    "          validation_data=(X_val, Y_val), callbacks=[earlystopping, reducelr, csvlog])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T10:04:28.281282Z",
     "start_time": "2019-02-03T10:04:28.270708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 30, 10)\n",
      "[[ 0.03397147]\n",
      " [-0.083556  ]]\n",
      "[[ 0.03222898]\n",
      " [-0.08687057]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0:2].shape)\n",
    "print(model.predict(X_train[0:2]))\n",
    "print(Y_train[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One to many"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the \n",
    "`因為是一對多模型Timesteps只有1，因此return_sequences=False 才可執行。只有return sequence的時候，才會回傳Timestep的維度`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:20:34.505070Z",
     "start_time": "2019-02-03T14:20:34.500496Z"
    }
   },
   "outputs": [],
   "source": [
    "def buildOneToManyModel(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, input_shape=(shape[1], shape[2])))\n",
    "    # output shape: (5, 1)\n",
    "    model.add(Dense(1))\n",
    "    model.add(RepeatVector(5))  # 將輸出的向量x5，進行多步預測。所以y的dim要再多給1個:(None, 5, 1)。\n",
    "    \"\"\"\n",
    "    RepeatVector:\n",
    "        n :整數，重複次數。\n",
    "\n",
    "        輸入尺寸:\n",
    "           2D張量，尺寸為(num_samples, features)。\n",
    "\n",
    "        輸出尺寸:\n",
    "           3D張量，尺寸為(num_samples, n, features)。\n",
    "    \"\"\"\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T13:49:29.983680Z",
     "start_time": "2019-02-03T13:49:29.553903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_20 (LSTM)               (None, 10)                640       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 11        \n",
      "_________________________________________________________________\n",
      "repeat_vector_5 (RepeatVecto (None, 5, 1)              0         \n",
      "=================================================================\n",
      "Total params: 651\n",
      "Trainable params: 651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fdcd46b9be0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buildOneToManyModel((1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train one to many model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:26:52.501401Z",
     "start_time": "2019-02-03T14:26:31.389559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (5706, 1, 10)\n",
      "Y_train shape (5706, 5, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_29 (LSTM)               (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 11        \n",
      "_________________________________________________________________\n",
      "repeat_vector_6 (RepeatVecto (None, 5, 1)              0         \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5706 samples, validate on 634 samples\n",
      "Epoch 1/1000\n",
      "5706/5706 [==============================] - 1s 260us/step - loss: 0.0719 - val_loss: 0.0552\n",
      "Epoch 2/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 0.0423 - val_loss: 0.0295\n",
      "Epoch 3/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 0.0202 - val_loss: 0.0115\n",
      "Epoch 4/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 0.0065 - val_loss: 0.0027\n",
      "Epoch 5/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 0.0014 - val_loss: 6.2247e-04\n",
      "Epoch 6/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 4.6916e-04 - val_loss: 3.6251e-04\n",
      "Epoch 7/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 3.3204e-04 - val_loss: 2.9181e-04\n",
      "Epoch 8/1000\n",
      "5706/5706 [==============================] - 0s 15us/step - loss: 2.7929e-04 - val_loss: 2.5462e-04\n",
      "Epoch 9/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 2.4945e-04 - val_loss: 2.3447e-04\n",
      "Epoch 10/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 2.3119e-04 - val_loss: 2.1929e-04\n",
      "Epoch 11/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 2.1811e-04 - val_loss: 2.0806e-04\n",
      "Epoch 12/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 2.0708e-04 - val_loss: 1.9848e-04\n",
      "Epoch 13/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 1.9770e-04 - val_loss: 1.9040e-04\n",
      "Epoch 14/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 1.8915e-04 - val_loss: 1.8239e-04\n",
      "Epoch 15/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 1.8093e-04 - val_loss: 1.7566e-04\n",
      "Epoch 16/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 1.7366e-04 - val_loss: 1.6847e-04\n",
      "Epoch 17/1000\n",
      "5706/5706 [==============================] - 0s 15us/step - loss: 1.6657e-04 - val_loss: 1.6228e-04\n",
      "Epoch 18/1000\n",
      "5706/5706 [==============================] - 0s 16us/step - loss: 1.5926e-04 - val_loss: 1.5547e-04\n",
      "Epoch 19/1000\n",
      "5706/5706 [==============================] - 0s 16us/step - loss: 1.5260e-04 - val_loss: 1.4949e-04\n",
      "Epoch 20/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 1.4651e-04 - val_loss: 1.4376e-04\n",
      "Epoch 21/1000\n",
      "5706/5706 [==============================] - 0s 16us/step - loss: 1.4059e-04 - val_loss: 1.3855e-04\n",
      "Epoch 22/1000\n",
      "5706/5706 [==============================] - 0s 17us/step - loss: 1.3505e-04 - val_loss: 1.3228e-04\n",
      "Epoch 23/1000\n",
      "5706/5706 [==============================] - 0s 17us/step - loss: 1.2980e-04 - val_loss: 1.2802e-04\n",
      "Epoch 24/1000\n",
      "5706/5706 [==============================] - 0s 15us/step - loss: 1.2493e-04 - val_loss: 1.2275e-04\n",
      "Epoch 25/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 1.2070e-04 - val_loss: 1.1878e-04\n",
      "Epoch 26/1000\n",
      "5706/5706 [==============================] - 0s 16us/step - loss: 1.1622e-04 - val_loss: 1.1521e-04\n",
      "Epoch 27/1000\n",
      "5706/5706 [==============================] - 0s 17us/step - loss: 1.1246e-04 - val_loss: 1.1125e-04\n",
      "Epoch 28/1000\n",
      "5706/5706 [==============================] - 0s 17us/step - loss: 1.0877e-04 - val_loss: 1.0829e-04\n",
      "Epoch 29/1000\n",
      "5706/5706 [==============================] - 0s 17us/step - loss: 1.0556e-04 - val_loss: 1.0483e-04\n",
      "Epoch 30/1000\n",
      "5706/5706 [==============================] - 0s 17us/step - loss: 1.0240e-04 - val_loss: 1.0191e-04\n",
      "Epoch 31/1000\n",
      "5706/5706 [==============================] - 0s 17us/step - loss: 9.9678e-05 - val_loss: 9.9707e-05\n",
      "Epoch 32/1000\n",
      "5706/5706 [==============================] - 0s 16us/step - loss: 9.7558e-05 - val_loss: 9.7673e-05\n",
      "Epoch 33/1000\n",
      "5706/5706 [==============================] - 0s 16us/step - loss: 9.5177e-05 - val_loss: 9.6327e-05\n",
      "Epoch 34/1000\n",
      "5706/5706 [==============================] - 0s 16us/step - loss: 9.3268e-05 - val_loss: 9.3836e-05\n",
      "Epoch 35/1000\n",
      "5706/5706 [==============================] - 0s 16us/step - loss: 9.1464e-05 - val_loss: 9.1681e-05\n",
      "Epoch 36/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 8.9758e-05 - val_loss: 8.9581e-05\n",
      "Epoch 37/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 8.8342e-05 - val_loss: 8.8415e-05\n",
      "Epoch 38/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 8.6856e-05 - val_loss: 8.7346e-05\n",
      "Epoch 39/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 8.6037e-05 - val_loss: 8.5934e-05\n",
      "Epoch 40/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 8.4849e-05 - val_loss: 8.5391e-05\n",
      "Epoch 41/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 8.3827e-05 - val_loss: 8.4031e-05\n",
      "Epoch 42/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 8.2951e-05 - val_loss: 8.3574e-05\n",
      "Epoch 43/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 8.2351e-05 - val_loss: 8.2458e-05\n",
      "Epoch 44/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 8.1545e-05 - val_loss: 8.1613e-05\n",
      "Epoch 45/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 8.1110e-05 - val_loss: 8.0910e-05\n",
      "Epoch 46/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 8.0518e-05 - val_loss: 8.1601e-05\n",
      "Epoch 47/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.9731e-05 - val_loss: 7.9578e-05\n",
      "Epoch 48/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 7.9367e-05 - val_loss: 7.9927e-05\n",
      "Epoch 49/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.8615e-05 - val_loss: 7.9237e-05\n",
      "Epoch 50/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.8599e-05 - val_loss: 7.8298e-05\n",
      "Epoch 51/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 7.7890e-05 - val_loss: 7.7610e-05\n",
      "Epoch 52/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 7.7886e-05 - val_loss: 7.7062e-05\n",
      "Epoch 53/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 7.7265e-05 - val_loss: 7.6399e-05\n",
      "Epoch 54/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.7004e-05 - val_loss: 7.6456e-05\n",
      "Epoch 55/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.6418e-05 - val_loss: 7.6149e-05\n",
      "Epoch 56/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.6472e-05 - val_loss: 7.5863e-05\n",
      "Epoch 57/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.5943e-05 - val_loss: 7.5042e-05\n",
      "Epoch 58/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.5244e-05 - val_loss: 7.4326e-05\n",
      "Epoch 59/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.4923e-05 - val_loss: 7.3993e-05\n",
      "Epoch 60/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 7.4601e-05 - val_loss: 7.4175e-05\n",
      "Epoch 61/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.3985e-05 - val_loss: 7.4304e-05\n",
      "Epoch 62/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.3981e-05 - val_loss: 7.3366e-05\n",
      "Epoch 63/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 7.3600e-05 - val_loss: 7.3843e-05\n",
      "Epoch 64/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.3222e-05 - val_loss: 7.2130e-05\n",
      "Epoch 65/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.2928e-05 - val_loss: 7.2241e-05\n",
      "Epoch 66/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.2711e-05 - val_loss: 7.1611e-05\n",
      "Epoch 67/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.2111e-05 - val_loss: 7.2394e-05\n",
      "Epoch 68/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.1919e-05 - val_loss: 7.1000e-05\n",
      "Epoch 69/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.1667e-05 - val_loss: 7.0433e-05\n",
      "Epoch 70/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.1349e-05 - val_loss: 6.9456e-05\n",
      "Epoch 71/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.0937e-05 - val_loss: 6.9533e-05\n",
      "Epoch 72/1000\n",
      "5706/5706 [==============================] - 0s 15us/step - loss: 7.0304e-05 - val_loss: 7.0569e-05\n",
      "Epoch 73/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.0716e-05 - val_loss: 6.9059e-05\n",
      "Epoch 74/1000\n",
      "5706/5706 [==============================] - 0s 17us/step - loss: 7.0329e-05 - val_loss: 6.9663e-05\n",
      "Epoch 75/1000\n",
      "5706/5706 [==============================] - 0s 15us/step - loss: 6.9722e-05 - val_loss: 6.8250e-05\n",
      "Epoch 76/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.9081e-05 - val_loss: 6.8071e-05\n",
      "Epoch 77/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.9112e-05 - val_loss: 6.7393e-05\n",
      "Epoch 78/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.8450e-05 - val_loss: 6.7338e-05\n",
      "Epoch 79/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 6.8047e-05 - val_loss: 6.7015e-05\n",
      "Epoch 80/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.8196e-05 - val_loss: 6.6693e-05\n",
      "Epoch 81/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.8463e-05 - val_loss: 6.6325e-05\n",
      "Epoch 82/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.7569e-05 - val_loss: 6.6749e-05\n",
      "Epoch 83/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.7544e-05 - val_loss: 6.5869e-05\n",
      "Epoch 84/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 6.7337e-05 - val_loss: 6.4809e-05\n",
      "Epoch 85/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.6768e-05 - val_loss: 6.4431e-05\n",
      "Epoch 86/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.6576e-05 - val_loss: 6.7017e-05\n",
      "Epoch 87/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.6077e-05 - val_loss: 6.4615e-05\n",
      "Epoch 88/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.5795e-05 - val_loss: 6.3885e-05\n",
      "Epoch 89/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.5424e-05 - val_loss: 6.3544e-05\n",
      "Epoch 90/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.5053e-05 - val_loss: 6.2786e-05\n",
      "Epoch 91/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.5127e-05 - val_loss: 6.2808e-05\n",
      "Epoch 92/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.4301e-05 - val_loss: 6.2133e-05\n",
      "Epoch 93/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.4583e-05 - val_loss: 6.4637e-05\n",
      "Epoch 94/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.4586e-05 - val_loss: 6.2648e-05\n",
      "Epoch 95/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.3950e-05 - val_loss: 6.2169e-05\n",
      "Epoch 96/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.3563e-05 - val_loss: 6.2481e-05\n",
      "Epoch 97/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.3920e-05 - val_loss: 6.2912e-05\n",
      "Epoch 98/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.3300e-05 - val_loss: 6.1264e-05\n",
      "Epoch 99/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.2943e-05 - val_loss: 6.2270e-05\n",
      "Epoch 100/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.2776e-05 - val_loss: 6.1422e-05\n",
      "Epoch 101/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.2621e-05 - val_loss: 6.0149e-05\n",
      "Epoch 102/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 6.2375e-05 - val_loss: 6.1589e-05\n",
      "Epoch 103/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 6.2592e-05 - val_loss: 5.9473e-05\n",
      "Epoch 104/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.2237e-05 - val_loss: 5.9592e-05\n",
      "Epoch 105/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.2072e-05 - val_loss: 6.0045e-05\n",
      "Epoch 106/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 6.1272e-05 - val_loss: 5.8901e-05\n",
      "Epoch 107/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 6.1232e-05 - val_loss: 5.9597e-05\n",
      "Epoch 108/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.1368e-05 - val_loss: 5.8394e-05\n",
      "Epoch 109/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 6.0680e-05 - val_loss: 5.8383e-05\n",
      "Epoch 110/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 6.0994e-05 - val_loss: 5.9504e-05\n",
      "Epoch 111/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.0261e-05 - val_loss: 5.8334e-05\n",
      "Epoch 112/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.0238e-05 - val_loss: 5.8951e-05\n",
      "Epoch 113/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.0132e-05 - val_loss: 5.8311e-05\n",
      "Epoch 114/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.9851e-05 - val_loss: 5.7639e-05\n",
      "Epoch 115/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.0484e-05 - val_loss: 5.7214e-05\n",
      "Epoch 116/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.9671e-05 - val_loss: 5.6817e-05\n",
      "Epoch 117/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.9152e-05 - val_loss: 5.7716e-05\n",
      "Epoch 118/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.9160e-05 - val_loss: 5.6849e-05\n",
      "Epoch 119/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.9221e-05 - val_loss: 5.6836e-05\n",
      "Epoch 120/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.9207e-05 - val_loss: 5.8860e-05\n",
      "Epoch 121/1000\n",
      "5706/5706 [==============================] - 0s 15us/step - loss: 5.9377e-05 - val_loss: 5.7445e-05\n",
      "Epoch 122/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.8798e-05 - val_loss: 5.7931e-05\n",
      "Epoch 123/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.8563e-05 - val_loss: 5.6527e-05\n",
      "Epoch 124/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.8638e-05 - val_loss: 5.6943e-05\n",
      "Epoch 125/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.8041e-05 - val_loss: 5.6084e-05\n",
      "Epoch 126/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.8851e-05 - val_loss: 5.6829e-05\n",
      "Epoch 127/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.8713e-05 - val_loss: 5.5863e-05\n",
      "Epoch 128/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.8046e-05 - val_loss: 5.7031e-05\n",
      "Epoch 129/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.8112e-05 - val_loss: 5.5083e-05\n",
      "Epoch 130/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.8309e-05 - val_loss: 5.7597e-05\n",
      "Epoch 131/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.8345e-05 - val_loss: 5.5732e-05\n",
      "Epoch 132/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.8173e-05 - val_loss: 5.5369e-05\n",
      "Epoch 133/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7961e-05 - val_loss: 5.5755e-05\n",
      "Epoch 134/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.8090e-05 - val_loss: 5.6608e-05\n",
      "Epoch 135/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.9217e-05 - val_loss: 5.6240e-05\n",
      "Epoch 136/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.8523e-05 - val_loss: 5.4636e-05\n",
      "Epoch 137/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7315e-05 - val_loss: 5.4997e-05\n",
      "Epoch 138/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.7563e-05 - val_loss: 5.7053e-05\n",
      "Epoch 139/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7828e-05 - val_loss: 5.5400e-05\n",
      "Epoch 140/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7568e-05 - val_loss: 5.5927e-05\n",
      "Epoch 141/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7272e-05 - val_loss: 5.6633e-05\n",
      "Epoch 142/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7576e-05 - val_loss: 5.5469e-05\n",
      "Epoch 143/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7082e-05 - val_loss: 5.6562e-05\n",
      "Epoch 144/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7268e-05 - val_loss: 5.5443e-05\n",
      "Epoch 145/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.7256e-05 - val_loss: 5.6193e-05\n",
      "Epoch 146/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7055e-05 - val_loss: 5.4794e-05\n",
      "Epoch 147/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7564e-05 - val_loss: 5.9134e-05\n",
      "Epoch 148/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7448e-05 - val_loss: 5.5046e-05\n",
      "Epoch 149/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7212e-05 - val_loss: 5.6839e-05\n",
      "Epoch 150/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7534e-05 - val_loss: 5.8173e-05\n",
      "Epoch 151/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7107e-05 - val_loss: 5.6045e-05\n",
      "Epoch 152/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7385e-05 - val_loss: 5.7066e-05\n",
      "Epoch 153/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7467e-05 - val_loss: 5.5529e-05\n",
      "Epoch 154/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6788e-05 - val_loss: 5.4163e-05\n",
      "Epoch 155/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7151e-05 - val_loss: 5.4622e-05\n",
      "Epoch 156/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.7365e-05 - val_loss: 5.5647e-05\n",
      "Epoch 157/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6974e-05 - val_loss: 5.4849e-05\n",
      "Epoch 158/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.6679e-05 - val_loss: 5.8009e-05\n",
      "Epoch 159/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7740e-05 - val_loss: 5.4831e-05\n",
      "Epoch 160/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6446e-05 - val_loss: 5.6123e-05\n",
      "Epoch 161/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6680e-05 - val_loss: 5.5083e-05\n",
      "Epoch 162/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6492e-05 - val_loss: 5.4705e-05\n",
      "Epoch 163/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.6558e-05 - val_loss: 5.5007e-05\n",
      "Epoch 164/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6807e-05 - val_loss: 5.6077e-05\n",
      "Epoch 165/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6759e-05 - val_loss: 5.3983e-05\n",
      "Epoch 166/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6286e-05 - val_loss: 5.4432e-05\n",
      "Epoch 167/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6226e-05 - val_loss: 5.4413e-05\n",
      "Epoch 168/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6386e-05 - val_loss: 5.6461e-05\n",
      "Epoch 169/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6838e-05 - val_loss: 5.4094e-05\n",
      "Epoch 170/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6468e-05 - val_loss: 5.5011e-05\n",
      "Epoch 171/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6813e-05 - val_loss: 5.4800e-05\n",
      "Epoch 172/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6756e-05 - val_loss: 5.6233e-05\n",
      "Epoch 173/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.7583e-05 - val_loss: 5.4294e-05\n",
      "Epoch 174/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6051e-05 - val_loss: 5.6543e-05\n",
      "Epoch 175/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6161e-05 - val_loss: 5.4089e-05\n",
      "Epoch 176/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6485e-05 - val_loss: 5.5878e-05\n",
      "Epoch 177/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6415e-05 - val_loss: 5.4593e-05\n",
      "Epoch 178/1000\n",
      "5706/5706 [==============================] - 0s 16us/step - loss: 5.6386e-05 - val_loss: 5.6362e-05\n",
      "Epoch 179/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.5928e-05 - val_loss: 5.4400e-05\n",
      "Epoch 180/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.7255e-05 - val_loss: 5.5394e-05\n",
      "Epoch 181/1000\n",
      "5706/5706 [==============================] - 0s 15us/step - loss: 5.6142e-05 - val_loss: 5.4167e-05\n",
      "Epoch 182/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 5.6251e-05 - val_loss: 5.4309e-05\n",
      "Epoch 183/1000\n",
      "5706/5706 [==============================] - 0s 16us/step - loss: 5.6679e-05 - val_loss: 5.6945e-05\n",
      "Epoch 184/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.8074e-05 - val_loss: 5.5197e-05\n",
      "Epoch 185/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6511e-05 - val_loss: 5.4962e-05\n",
      "Epoch 186/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6536e-05 - val_loss: 5.4867e-05\n",
      "Epoch 187/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6399e-05 - val_loss: 5.4500e-05\n",
      "Epoch 188/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6490e-05 - val_loss: 5.5536e-05\n",
      "Epoch 189/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.6505e-05 - val_loss: 5.4522e-05\n",
      "Epoch 00189: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdcada7dc88>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, pastDay=1, futureDay=5)  # many to one\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "# because no return sequence, Y_train and Y_val shape must be 2 dimension\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, rate=0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,:,np.newaxis]\n",
    "Y_val = Y_val[:,:,np.newaxis]\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"Y_train shape\", Y_train.shape)\n",
    "\n",
    "model = buildOneToManyModel(X_train.shape)\n",
    "earlystopping = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "reducelr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.001)\n",
    "csvlog = CSVLogger('one2many_train_log')\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128,\n",
    "          validation_data=(X_val, Y_val), callbacks=[earlystopping, reducelr, csvlog])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:30:44.351401Z",
     "start_time": "2019-02-03T14:30:44.341997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 10)\n",
      "[[[0.16298736]\n",
      "  [0.17023698]\n",
      "  [0.17023698]\n",
      "  [0.16428321]\n",
      "  [0.15955536]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.16140072],\n",
       "        [0.16140072],\n",
       "        [0.16140072],\n",
       "        [0.16140072],\n",
       "        [0.16140072]]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train[0:1].shape)\n",
    "print(Y_train[0:1])\n",
    "#\n",
    "model.predict(X_train[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to many\n",
    "`(輸入與輸出相同長度) 將return_sequences 設為True ，再用TimeDistributed(Dense(1)) 將輸出調整為(5,1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T15:20:21.320995Z",
     "start_time": "2019-02-03T15:20:21.315283Z"
    }
   },
   "outputs": [],
   "source": [
    "def buildManyToManyModel(shape):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        LSTM(10, input_shape=(shape[1], shape[2]), return_sequences=True))\n",
    "    # output shape: (5, 1)\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T15:24:04.623063Z",
     "start_time": "2019-02-03T15:24:04.362151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_31 (LSTM)               (None, 5, 10)             640       \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 5, 1)              11        \n",
      "=================================================================\n",
      "Total params: 651\n",
      "Trainable params: 651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fdcacdfe7b8>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buildManyToManyModel((0, 5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T15:18:21.050100Z",
     "start_time": "2019-02-03T15:18:21.046521Z"
    }
   },
   "source": [
    "### Train many to many model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T15:30:20.473444Z",
     "start_time": "2019-02-03T15:29:41.986577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (5703, 5, 10)\n",
      "Y_train shape (5703, 5, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_32 (LSTM)               (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 11        \n",
      "_________________________________________________________________\n",
      "repeat_vector_7 (RepeatVecto (None, 5, 1)              0         \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5703 samples, validate on 633 samples\n",
      "Epoch 1/1000\n",
      "5703/5703 [==============================] - 2s 323us/step - loss: 0.1181 - val_loss: 0.0556\n",
      "Epoch 2/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 0.0288 - val_loss: 0.0082\n",
      "Epoch 3/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 0.0043 - val_loss: 0.0021\n",
      "Epoch 4/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 5/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 8.5576e-04 - val_loss: 7.6348e-04\n",
      "Epoch 6/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.7500e-04 - val_loss: 6.2144e-04\n",
      "Epoch 7/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 5.5910e-04 - val_loss: 5.2130e-04\n",
      "Epoch 8/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 4.7293e-04 - val_loss: 4.4046e-04\n",
      "Epoch 9/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 3.9957e-04 - val_loss: 3.6853e-04\n",
      "Epoch 10/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 3.4045e-04 - val_loss: 3.1454e-04\n",
      "Epoch 11/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 2.9137e-04 - val_loss: 2.6985e-04\n",
      "Epoch 12/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 2.5293e-04 - val_loss: 2.3650e-04\n",
      "Epoch 13/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 2.2282e-04 - val_loss: 2.0827e-04\n",
      "Epoch 14/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.9855e-04 - val_loss: 1.8632e-04\n",
      "Epoch 15/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.8013e-04 - val_loss: 1.7063e-04\n",
      "Epoch 16/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 1.6618e-04 - val_loss: 1.5892e-04\n",
      "Epoch 17/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 1.5506e-04 - val_loss: 1.4944e-04\n",
      "Epoch 18/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 1.4616e-04 - val_loss: 1.4240e-04\n",
      "Epoch 19/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.3915e-04 - val_loss: 1.3569e-04\n",
      "Epoch 20/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.3316e-04 - val_loss: 1.3081e-04\n",
      "Epoch 21/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.2829e-04 - val_loss: 1.2727e-04\n",
      "Epoch 22/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.2415e-04 - val_loss: 1.2274e-04\n",
      "Epoch 23/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.1931e-04 - val_loss: 1.1919e-04\n",
      "Epoch 24/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.1559e-04 - val_loss: 1.1591e-04\n",
      "Epoch 25/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 1.1189e-04 - val_loss: 1.1412e-04\n",
      "Epoch 26/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 1.0882e-04 - val_loss: 1.0977e-04\n",
      "Epoch 27/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 1.0576e-04 - val_loss: 1.0656e-04\n",
      "Epoch 28/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 1.0265e-04 - val_loss: 1.0410e-04\n",
      "Epoch 29/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 9.9433e-05 - val_loss: 1.0196e-04\n",
      "Epoch 30/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 9.7323e-05 - val_loss: 1.0108e-04\n",
      "Epoch 31/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 9.4560e-05 - val_loss: 9.7599e-05\n",
      "Epoch 32/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 9.2925e-05 - val_loss: 9.7548e-05\n",
      "Epoch 33/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 9.0675e-05 - val_loss: 9.3933e-05\n",
      "Epoch 34/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 8.8358e-05 - val_loss: 9.3287e-05\n",
      "Epoch 35/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 8.6579e-05 - val_loss: 9.0643e-05\n",
      "Epoch 36/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 8.4714e-05 - val_loss: 8.9642e-05\n",
      "Epoch 37/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 8.3654e-05 - val_loss: 8.8917e-05\n",
      "Epoch 38/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 8.2954e-05 - val_loss: 9.0145e-05\n",
      "Epoch 39/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 8.1340e-05 - val_loss: 8.6392e-05\n",
      "Epoch 40/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 8.0181e-05 - val_loss: 8.5556e-05\n",
      "Epoch 41/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 7.9000e-05 - val_loss: 8.5117e-05\n",
      "Epoch 42/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 7.8297e-05 - val_loss: 8.5468e-05\n",
      "Epoch 43/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 7.7421e-05 - val_loss: 8.3965e-05\n",
      "Epoch 44/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 7.6753e-05 - val_loss: 8.4571e-05\n",
      "Epoch 45/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 7.6115e-05 - val_loss: 8.2422e-05\n",
      "Epoch 46/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 7.5338e-05 - val_loss: 8.2595e-05\n",
      "Epoch 47/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 7.5373e-05 - val_loss: 8.0796e-05\n",
      "Epoch 48/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 7.4406e-05 - val_loss: 8.0865e-05\n",
      "Epoch 49/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 7.3818e-05 - val_loss: 8.0957e-05\n",
      "Epoch 50/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 7.3389e-05 - val_loss: 8.0571e-05\n",
      "Epoch 51/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 7.2938e-05 - val_loss: 7.9939e-05\n",
      "Epoch 52/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 7.2672e-05 - val_loss: 7.9630e-05\n",
      "Epoch 53/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 7.2619e-05 - val_loss: 7.9200e-05\n",
      "Epoch 54/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 7.2111e-05 - val_loss: 7.9007e-05\n",
      "Epoch 55/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 7.2028e-05 - val_loss: 8.0924e-05\n",
      "Epoch 56/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 7.1665e-05 - val_loss: 7.8268e-05\n",
      "Epoch 57/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 7.0939e-05 - val_loss: 7.8567e-05\n",
      "Epoch 58/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 7.0426e-05 - val_loss: 7.8003e-05\n",
      "Epoch 59/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 7.0663e-05 - val_loss: 7.8258e-05\n",
      "Epoch 60/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 7.0713e-05 - val_loss: 8.1741e-05\n",
      "Epoch 61/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 7.1340e-05 - val_loss: 8.2152e-05\n",
      "Epoch 62/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 6.9917e-05 - val_loss: 7.7065e-05\n",
      "Epoch 63/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 6.9836e-05 - val_loss: 8.2436e-05\n",
      "Epoch 64/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.9881e-05 - val_loss: 7.7855e-05\n",
      "Epoch 65/1000\n",
      "5703/5703 [==============================] - 0s 37us/step - loss: 7.0070e-05 - val_loss: 7.6420e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 6.9682e-05 - val_loss: 7.5968e-05\n",
      "Epoch 67/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 6.9711e-05 - val_loss: 8.0527e-05\n",
      "Epoch 68/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.9103e-05 - val_loss: 7.7010e-05\n",
      "Epoch 69/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.8422e-05 - val_loss: 7.6808e-05\n",
      "Epoch 70/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 6.8966e-05 - val_loss: 7.7344e-05\n",
      "Epoch 71/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 6.8528e-05 - val_loss: 7.5882e-05\n",
      "Epoch 72/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 6.8993e-05 - val_loss: 7.6876e-05\n",
      "Epoch 73/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 6.8330e-05 - val_loss: 7.5619e-05\n",
      "Epoch 74/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.7541e-05 - val_loss: 7.5378e-05\n",
      "Epoch 75/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.7436e-05 - val_loss: 7.5293e-05\n",
      "Epoch 76/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.7720e-05 - val_loss: 7.5470e-05\n",
      "Epoch 77/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.7914e-05 - val_loss: 7.4877e-05\n",
      "Epoch 78/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 6.7226e-05 - val_loss: 7.4135e-05\n",
      "Epoch 79/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.7402e-05 - val_loss: 7.4801e-05\n",
      "Epoch 80/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 6.7820e-05 - val_loss: 7.4688e-05\n",
      "Epoch 81/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 6.6998e-05 - val_loss: 7.4422e-05\n",
      "Epoch 82/1000\n",
      "5703/5703 [==============================] - 0s 37us/step - loss: 6.6407e-05 - val_loss: 7.6310e-05\n",
      "Epoch 83/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 6.6900e-05 - val_loss: 7.6644e-05\n",
      "Epoch 84/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 6.6586e-05 - val_loss: 7.5709e-05\n",
      "Epoch 85/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 6.7022e-05 - val_loss: 7.3479e-05\n",
      "Epoch 86/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 6.6496e-05 - val_loss: 7.3574e-05\n",
      "Epoch 87/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.5756e-05 - val_loss: 7.4279e-05\n",
      "Epoch 88/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 6.7000e-05 - val_loss: 7.5010e-05\n",
      "Epoch 89/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.5949e-05 - val_loss: 7.3556e-05\n",
      "Epoch 90/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 6.5725e-05 - val_loss: 7.6051e-05\n",
      "Epoch 91/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.6447e-05 - val_loss: 7.2720e-05\n",
      "Epoch 92/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.5505e-05 - val_loss: 7.3349e-05\n",
      "Epoch 93/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.6024e-05 - val_loss: 7.5099e-05\n",
      "Epoch 94/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 6.6246e-05 - val_loss: 7.1650e-05\n",
      "Epoch 95/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 6.5197e-05 - val_loss: 7.3343e-05\n",
      "Epoch 96/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 6.5862e-05 - val_loss: 7.5842e-05\n",
      "Epoch 97/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.7978e-05 - val_loss: 7.2814e-05\n",
      "Epoch 98/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.5596e-05 - val_loss: 7.2993e-05\n",
      "Epoch 99/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 6.5233e-05 - val_loss: 7.1831e-05\n",
      "Epoch 100/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 6.5195e-05 - val_loss: 7.1854e-05\n",
      "Epoch 101/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 6.5335e-05 - val_loss: 7.6831e-05\n",
      "Epoch 102/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.7104e-05 - val_loss: 7.2150e-05\n",
      "Epoch 103/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 6.5057e-05 - val_loss: 7.2823e-05\n",
      "Epoch 104/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.4769e-05 - val_loss: 7.1450e-05\n",
      "Epoch 105/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 6.5078e-05 - val_loss: 7.1850e-05\n",
      "Epoch 106/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 6.5064e-05 - val_loss: 7.1820e-05\n",
      "Epoch 107/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 6.4622e-05 - val_loss: 7.0945e-05\n",
      "Epoch 108/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 6.4302e-05 - val_loss: 7.2458e-05\n",
      "Epoch 109/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.4338e-05 - val_loss: 7.3431e-05\n",
      "Epoch 110/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 6.4591e-05 - val_loss: 7.5339e-05\n",
      "Epoch 111/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 6.5853e-05 - val_loss: 7.1035e-05\n",
      "Epoch 112/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 6.4504e-05 - val_loss: 7.2834e-05\n",
      "Epoch 113/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 6.4778e-05 - val_loss: 6.9731e-05\n",
      "Epoch 114/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 6.4703e-05 - val_loss: 7.0508e-05\n",
      "Epoch 115/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 6.4360e-05 - val_loss: 7.0111e-05\n",
      "Epoch 116/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 6.4081e-05 - val_loss: 7.0049e-05\n",
      "Epoch 117/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 6.3129e-05 - val_loss: 7.0176e-05\n",
      "Epoch 118/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 6.3426e-05 - val_loss: 7.4091e-05\n",
      "Epoch 119/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 6.4756e-05 - val_loss: 7.0748e-05\n",
      "Epoch 120/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.4956e-05 - val_loss: 7.0495e-05\n",
      "Epoch 121/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 6.3703e-05 - val_loss: 7.2108e-05\n",
      "Epoch 122/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 6.3561e-05 - val_loss: 6.9568e-05\n",
      "Epoch 123/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 6.3420e-05 - val_loss: 8.1715e-05\n",
      "Epoch 124/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 6.4044e-05 - val_loss: 7.0353e-05\n",
      "Epoch 125/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 6.3017e-05 - val_loss: 7.1362e-05\n",
      "Epoch 126/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 6.2551e-05 - val_loss: 7.4093e-05\n",
      "Epoch 127/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 6.4057e-05 - val_loss: 6.8471e-05\n",
      "Epoch 128/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 6.2826e-05 - val_loss: 6.9372e-05\n",
      "Epoch 129/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 6.3817e-05 - val_loss: 6.8974e-05\n",
      "Epoch 130/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.2537e-05 - val_loss: 7.1891e-05\n",
      "Epoch 131/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 6.3299e-05 - val_loss: 7.2204e-05\n",
      "Epoch 132/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 6.2174e-05 - val_loss: 7.2364e-05\n",
      "Epoch 133/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 6.3149e-05 - val_loss: 6.9318e-05\n",
      "Epoch 134/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.2245e-05 - val_loss: 6.9119e-05\n",
      "Epoch 135/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.3876e-05 - val_loss: 6.9701e-05\n",
      "Epoch 136/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 6.1948e-05 - val_loss: 6.8739e-05\n",
      "Epoch 137/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 6.2556e-05 - val_loss: 6.9376e-05\n",
      "Epoch 138/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.1716e-05 - val_loss: 6.7478e-05\n",
      "Epoch 139/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 6.1924e-05 - val_loss: 7.1030e-05\n",
      "Epoch 140/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 6.1954e-05 - val_loss: 6.9506e-05\n",
      "Epoch 141/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 6.2307e-05 - val_loss: 6.9562e-05\n",
      "Epoch 142/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 6.2934e-05 - val_loss: 7.1330e-05\n",
      "Epoch 143/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 6.2424e-05 - val_loss: 6.8209e-05\n",
      "Epoch 144/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 6.2383e-05 - val_loss: 6.7001e-05\n",
      "Epoch 145/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 6.1835e-05 - val_loss: 6.7570e-05\n",
      "Epoch 146/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 6.1200e-05 - val_loss: 7.0494e-05\n",
      "Epoch 147/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 6.2615e-05 - val_loss: 7.0255e-05\n",
      "Epoch 148/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 6.2478e-05 - val_loss: 7.2155e-05\n",
      "Epoch 149/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 6.4259e-05 - val_loss: 6.9333e-05\n",
      "Epoch 150/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.0901e-05 - val_loss: 6.6776e-05\n",
      "Epoch 151/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 6.3264e-05 - val_loss: 6.6472e-05\n",
      "Epoch 152/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 6.2229e-05 - val_loss: 6.8033e-05\n",
      "Epoch 153/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 6.0719e-05 - val_loss: 6.6513e-05\n",
      "Epoch 154/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.0699e-05 - val_loss: 6.6589e-05\n",
      "Epoch 155/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 6.0492e-05 - val_loss: 7.0207e-05\n",
      "Epoch 156/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 6.2315e-05 - val_loss: 6.5753e-05\n",
      "Epoch 157/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 5.9808e-05 - val_loss: 6.5684e-05\n",
      "Epoch 158/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 6.0797e-05 - val_loss: 6.7241e-05\n",
      "Epoch 159/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 6.0098e-05 - val_loss: 6.7735e-05\n",
      "Epoch 160/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 6.1273e-05 - val_loss: 6.8481e-05\n",
      "Epoch 161/1000\n",
      "5703/5703 [==============================] - 0s 37us/step - loss: 6.1370e-05 - val_loss: 6.4610e-05\n",
      "Epoch 162/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 6.0075e-05 - val_loss: 6.5888e-05\n",
      "Epoch 163/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.1642e-05 - val_loss: 6.7290e-05\n",
      "Epoch 164/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 6.2452e-05 - val_loss: 6.8250e-05\n",
      "Epoch 165/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 6.0319e-05 - val_loss: 6.8470e-05\n",
      "Epoch 166/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 6.0580e-05 - val_loss: 6.7124e-05\n",
      "Epoch 167/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 6.1204e-05 - val_loss: 6.4298e-05\n",
      "Epoch 00167: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdcac94f6a0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, pastDay=5, futureDay=5)  # many to one\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "# because no return sequence, Y_train and Y_val shape must be 2 dimension\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, rate=0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,:,np.newaxis]\n",
    "Y_val = Y_val[:,:,np.newaxis]\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"Y_train shape\", Y_train.shape)\n",
    "\n",
    "model = buildOneToManyModel(X_train.shape)\n",
    "earlystopping = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "reducelr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.001)\n",
    "csvlog = CSVLogger('many2many_same_train_log')\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128,\n",
    "          validation_data=(X_val, Y_val), callbacks=[earlystopping, reducelr, csvlog])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T15:35:01.223375Z",
     "start_time": "2019-02-03T15:35:00.690115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 10)\n",
      "[[[-0.0799953 ]\n",
      "  [-0.08803951]\n",
      "  [-0.08550953]\n",
      "  [-0.08268756]\n",
      "  [-0.07934663]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.07867057],\n",
       "        [-0.07867057],\n",
       "        [-0.07867057],\n",
       "        [-0.07867057],\n",
       "        [-0.07867057]]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train[0:1].shape)\n",
    "print(Y_train[0:1])\n",
    "#\n",
    "model.predict(X_train[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to many with different timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T17:36:06.878871Z",
     "start_time": "2019-02-03T17:36:06.872806Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ref: https://github.com/keras-team/keras/issues/6063\n",
    "\n",
    "def buildManyToManyModel_diff(shape):\n",
    "    model = Sequential()\n",
    "    # 10 LSTM node outputs with the last hidden state information\n",
    "    model.add(LSTM(10, input_shape=(shape[1], shape[2]), return_sequences=False))  \n",
    "    # transform the tensor as 5 x 10 (timestep x features) format\n",
    "    model.add(RepeatVector(5))  # time step of future prediction\n",
    "    model.add(LSTM(10, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1)))  # apply Dense to every dim and turn the dim to 1 here\n",
    "    # output shape: (5, 1)\n",
    "    model.add(Activation('linear'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T17:32:45.659954Z",
     "start_time": "2019-02-03T17:32:44.805181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_64 (LSTM)               (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "repeat_vector_12 (RepeatVect (None, 5, 10)             0         \n",
      "_________________________________________________________________\n",
      "lstm_65 (LSTM)               (None, 5, 5)              320       \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 5, 1)              6         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 5, 1)              0         \n",
      "=================================================================\n",
      "Total params: 1,166\n",
      "Trainable params: 1,166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fdca1f0fb38>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buildManyToManyModel_diff((0, 30, 10))  # Input的 TimeStep=30, Features=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the many to many with different timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T17:40:10.236289Z",
     "start_time": "2019-02-03T17:36:50.033633Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (5680, 30, 10)\n",
      "Y_train shape (5680, 5, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_68 (LSTM)               (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "repeat_vector_14 (RepeatVect (None, 5, 10)             0         \n",
      "_________________________________________________________________\n",
      "lstm_69 (LSTM)               (None, 5, 10)             840       \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 5, 1)              11        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5, 1)              0         \n",
      "=================================================================\n",
      "Total params: 1,691\n",
      "Trainable params: 1,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5680 samples, validate on 631 samples\n",
      "Epoch 1/1000\n",
      "5680/5680 [==============================] - 5s 926us/step - loss: 0.0134 - val_loss: 0.0053\n",
      "Epoch 2/1000\n",
      "5680/5680 [==============================] - 1s 135us/step - loss: 0.0035 - val_loss: 0.0019\n",
      "Epoch 3/1000\n",
      "5680/5680 [==============================] - 1s 145us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 4/1000\n",
      "5680/5680 [==============================] - 1s 137us/step - loss: 7.8720e-04 - val_loss: 5.7195e-04\n",
      "Epoch 5/1000\n",
      "5680/5680 [==============================] - 1s 142us/step - loss: 5.2048e-04 - val_loss: 4.0267e-04\n",
      "Epoch 6/1000\n",
      "5680/5680 [==============================] - 1s 131us/step - loss: 3.8052e-04 - val_loss: 3.2462e-04\n",
      "Epoch 7/1000\n",
      "5680/5680 [==============================] - 1s 128us/step - loss: 2.8270e-04 - val_loss: 2.2288e-04\n",
      "Epoch 8/1000\n",
      "5680/5680 [==============================] - 1s 124us/step - loss: 2.2413e-04 - val_loss: 1.5093e-04\n",
      "Epoch 9/1000\n",
      "5680/5680 [==============================] - 1s 134us/step - loss: 1.7897e-04 - val_loss: 2.0054e-04\n",
      "Epoch 10/1000\n",
      "5680/5680 [==============================] - 1s 126us/step - loss: 1.6222e-04 - val_loss: 2.5137e-04\n",
      "Epoch 11/1000\n",
      "5680/5680 [==============================] - 1s 130us/step - loss: 1.4071e-04 - val_loss: 1.9611e-04\n",
      "Epoch 12/1000\n",
      "5680/5680 [==============================] - 1s 139us/step - loss: 1.4270e-04 - val_loss: 1.1296e-04\n",
      "Epoch 13/1000\n",
      "5680/5680 [==============================] - 1s 138us/step - loss: 1.2960e-04 - val_loss: 1.0860e-04\n",
      "Epoch 14/1000\n",
      "5680/5680 [==============================] - 1s 124us/step - loss: 1.3327e-04 - val_loss: 1.3521e-04\n",
      "Epoch 15/1000\n",
      "5680/5680 [==============================] - 1s 135us/step - loss: 1.2312e-04 - val_loss: 1.1743e-04\n",
      "Epoch 16/1000\n",
      "5680/5680 [==============================] - 1s 131us/step - loss: 1.3473e-04 - val_loss: 8.4580e-05\n",
      "Epoch 17/1000\n",
      "5680/5680 [==============================] - 1s 115us/step - loss: 1.2554e-04 - val_loss: 1.6664e-04\n",
      "Epoch 18/1000\n",
      "5680/5680 [==============================] - 1s 129us/step - loss: 9.1825e-05 - val_loss: 1.0461e-04\n",
      "Epoch 19/1000\n",
      "5680/5680 [==============================] - 1s 129us/step - loss: 9.1333e-05 - val_loss: 8.6219e-05\n",
      "Epoch 20/1000\n",
      "5680/5680 [==============================] - 1s 145us/step - loss: 9.0485e-05 - val_loss: 9.1556e-05\n",
      "Epoch 21/1000\n",
      "5680/5680 [==============================] - 1s 129us/step - loss: 8.8685e-05 - val_loss: 1.3579e-04\n",
      "Epoch 22/1000\n",
      "5680/5680 [==============================] - 1s 138us/step - loss: 9.3616e-05 - val_loss: 7.9980e-05\n",
      "Epoch 23/1000\n",
      "5680/5680 [==============================] - 1s 136us/step - loss: 7.9676e-05 - val_loss: 9.6560e-05\n",
      "Epoch 24/1000\n",
      "5680/5680 [==============================] - 1s 136us/step - loss: 8.0416e-05 - val_loss: 8.0892e-05\n",
      "Epoch 25/1000\n",
      "5680/5680 [==============================] - 1s 131us/step - loss: 7.9882e-05 - val_loss: 7.8619e-05\n",
      "Epoch 26/1000\n",
      "5680/5680 [==============================] - 1s 134us/step - loss: 7.9788e-05 - val_loss: 7.6778e-05\n",
      "Epoch 27/1000\n",
      "5680/5680 [==============================] - 1s 131us/step - loss: 7.8582e-05 - val_loss: 7.7069e-05\n",
      "Epoch 28/1000\n",
      "5680/5680 [==============================] - 1s 137us/step - loss: 7.6313e-05 - val_loss: 8.0992e-05\n",
      "Epoch 29/1000\n",
      "5680/5680 [==============================] - 1s 134us/step - loss: 7.6149e-05 - val_loss: 7.7605e-05\n",
      "Epoch 30/1000\n",
      "5680/5680 [==============================] - 1s 139us/step - loss: 7.5990e-05 - val_loss: 8.8802e-05\n",
      "Epoch 31/1000\n",
      "5680/5680 [==============================] - 1s 130us/step - loss: 7.5878e-05 - val_loss: 7.6929e-05\n",
      "Epoch 32/1000\n",
      "5680/5680 [==============================] - 1s 142us/step - loss: 7.5316e-05 - val_loss: 7.5958e-05\n",
      "Epoch 33/1000\n",
      "5680/5680 [==============================] - 1s 132us/step - loss: 7.4807e-05 - val_loss: 7.7132e-05\n",
      "Epoch 34/1000\n",
      "5680/5680 [==============================] - 1s 144us/step - loss: 7.5059e-05 - val_loss: 7.5352e-05\n",
      "Epoch 35/1000\n",
      "5680/5680 [==============================] - 1s 141us/step - loss: 7.4509e-05 - val_loss: 8.0267e-05\n",
      "Epoch 36/1000\n",
      "5680/5680 [==============================] - 1s 134us/step - loss: 7.4687e-05 - val_loss: 7.5968e-05\n",
      "Epoch 37/1000\n",
      "5680/5680 [==============================] - 1s 137us/step - loss: 7.4533e-05 - val_loss: 7.4873e-05\n",
      "Epoch 38/1000\n",
      "5680/5680 [==============================] - 1s 126us/step - loss: 7.3914e-05 - val_loss: 7.5320e-05\n",
      "Epoch 39/1000\n",
      "5680/5680 [==============================] - 1s 131us/step - loss: 7.4325e-05 - val_loss: 7.6603e-05\n",
      "Epoch 40/1000\n",
      "5680/5680 [==============================] - 1s 137us/step - loss: 7.4045e-05 - val_loss: 7.9078e-05\n",
      "Epoch 41/1000\n",
      "5680/5680 [==============================] - 1s 130us/step - loss: 7.4047e-05 - val_loss: 7.8042e-05\n",
      "Epoch 42/1000\n",
      "5680/5680 [==============================] - 1s 137us/step - loss: 7.3468e-05 - val_loss: 8.4466e-05\n",
      "Epoch 43/1000\n",
      "5680/5680 [==============================] - 1s 134us/step - loss: 7.3786e-05 - val_loss: 7.4104e-05\n",
      "Epoch 44/1000\n",
      "5680/5680 [==============================] - 1s 134us/step - loss: 7.3512e-05 - val_loss: 7.6554e-05\n",
      "Epoch 45/1000\n",
      "5680/5680 [==============================] - 1s 130us/step - loss: 7.3355e-05 - val_loss: 7.5004e-05\n",
      "Epoch 46/1000\n",
      "5680/5680 [==============================] - 1s 132us/step - loss: 7.3349e-05 - val_loss: 7.5797e-05\n",
      "Epoch 47/1000\n",
      "5680/5680 [==============================] - 1s 135us/step - loss: 7.3049e-05 - val_loss: 7.3926e-05\n",
      "Epoch 48/1000\n",
      "5680/5680 [==============================] - 1s 136us/step - loss: 7.2926e-05 - val_loss: 7.3710e-05\n",
      "Epoch 49/1000\n",
      "5680/5680 [==============================] - 1s 138us/step - loss: 7.2880e-05 - val_loss: 7.4791e-05\n",
      "Epoch 50/1000\n",
      "5680/5680 [==============================] - 1s 135us/step - loss: 7.2624e-05 - val_loss: 7.4994e-05\n",
      "Epoch 51/1000\n",
      "5680/5680 [==============================] - 1s 136us/step - loss: 7.2406e-05 - val_loss: 7.3660e-05\n",
      "Epoch 52/1000\n",
      "5680/5680 [==============================] - 1s 138us/step - loss: 7.2279e-05 - val_loss: 7.4008e-05\n",
      "Epoch 53/1000\n",
      "5680/5680 [==============================] - 1s 136us/step - loss: 7.2491e-05 - val_loss: 7.4634e-05\n",
      "Epoch 54/1000\n",
      "5680/5680 [==============================] - 1s 124us/step - loss: 7.2356e-05 - val_loss: 7.3038e-05\n",
      "Epoch 55/1000\n",
      "5680/5680 [==============================] - 1s 139us/step - loss: 7.2316e-05 - val_loss: 7.3127e-05\n",
      "Epoch 56/1000\n",
      "5680/5680 [==============================] - 1s 142us/step - loss: 7.2086e-05 - val_loss: 7.4145e-05\n",
      "Epoch 57/1000\n",
      "5680/5680 [==============================] - 1s 132us/step - loss: 7.2009e-05 - val_loss: 7.4614e-05\n",
      "Epoch 58/1000\n",
      "5680/5680 [==============================] - 1s 129us/step - loss: 7.1480e-05 - val_loss: 7.5826e-05\n",
      "Epoch 59/1000\n",
      "5680/5680 [==============================] - 1s 118us/step - loss: 7.1351e-05 - val_loss: 7.5671e-05\n",
      "Epoch 60/1000\n",
      "5680/5680 [==============================] - 1s 129us/step - loss: 7.1497e-05 - val_loss: 7.5443e-05\n",
      "Epoch 61/1000\n",
      "5680/5680 [==============================] - 1s 138us/step - loss: 7.1208e-05 - val_loss: 8.2275e-05\n",
      "Epoch 62/1000\n",
      "5680/5680 [==============================] - 1s 121us/step - loss: 7.1500e-05 - val_loss: 7.8134e-05\n",
      "Epoch 63/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5680/5680 [==============================] - 1s 120us/step - loss: 7.1005e-05 - val_loss: 7.6532e-05\n",
      "Epoch 64/1000\n",
      "5680/5680 [==============================] - 1s 125us/step - loss: 7.0993e-05 - val_loss: 7.4674e-05\n",
      "Epoch 65/1000\n",
      "5680/5680 [==============================] - 1s 125us/step - loss: 7.1048e-05 - val_loss: 7.1700e-05\n",
      "Epoch 66/1000\n",
      "5680/5680 [==============================] - 1s 129us/step - loss: 7.0645e-05 - val_loss: 7.1923e-05\n",
      "Epoch 67/1000\n",
      "5680/5680 [==============================] - 1s 136us/step - loss: 7.0762e-05 - val_loss: 7.2310e-05\n",
      "Epoch 68/1000\n",
      "5680/5680 [==============================] - 1s 133us/step - loss: 7.0667e-05 - val_loss: 7.3629e-05\n",
      "Epoch 69/1000\n",
      "5680/5680 [==============================] - 1s 120us/step - loss: 7.0385e-05 - val_loss: 7.8540e-05\n",
      "Epoch 70/1000\n",
      "5680/5680 [==============================] - 1s 129us/step - loss: 7.0538e-05 - val_loss: 7.9050e-05\n",
      "Epoch 71/1000\n",
      "5680/5680 [==============================] - 1s 117us/step - loss: 7.0459e-05 - val_loss: 7.2241e-05\n",
      "Epoch 72/1000\n",
      "5680/5680 [==============================] - 1s 126us/step - loss: 7.0200e-05 - val_loss: 7.4690e-05\n",
      "Epoch 73/1000\n",
      "5680/5680 [==============================] - 1s 128us/step - loss: 6.9787e-05 - val_loss: 7.1374e-05\n",
      "Epoch 74/1000\n",
      "5680/5680 [==============================] - 1s 133us/step - loss: 7.0141e-05 - val_loss: 7.1444e-05\n",
      "Epoch 75/1000\n",
      "5680/5680 [==============================] - 1s 128us/step - loss: 6.9957e-05 - val_loss: 7.1835e-05\n",
      "Epoch 76/1000\n",
      "5680/5680 [==============================] - 1s 146us/step - loss: 6.9446e-05 - val_loss: 7.1227e-05\n",
      "Epoch 77/1000\n",
      "5680/5680 [==============================] - 1s 138us/step - loss: 6.9745e-05 - val_loss: 7.1008e-05\n",
      "Epoch 78/1000\n",
      "5680/5680 [==============================] - 1s 123us/step - loss: 6.9563e-05 - val_loss: 7.2915e-05\n",
      "Epoch 79/1000\n",
      "5680/5680 [==============================] - 1s 136us/step - loss: 6.9668e-05 - val_loss: 7.1782e-05\n",
      "Epoch 80/1000\n",
      "5680/5680 [==============================] - 1s 133us/step - loss: 6.9293e-05 - val_loss: 7.4051e-05\n",
      "Epoch 81/1000\n",
      "5680/5680 [==============================] - 1s 116us/step - loss: 6.9665e-05 - val_loss: 7.0417e-05\n",
      "Epoch 82/1000\n",
      "5680/5680 [==============================] - 1s 134us/step - loss: 6.9282e-05 - val_loss: 7.3950e-05\n",
      "Epoch 83/1000\n",
      "5680/5680 [==============================] - 1s 143us/step - loss: 6.9446e-05 - val_loss: 7.1238e-05\n",
      "Epoch 84/1000\n",
      "5680/5680 [==============================] - 1s 136us/step - loss: 6.9294e-05 - val_loss: 7.1522e-05\n",
      "Epoch 85/1000\n",
      "5680/5680 [==============================] - 1s 139us/step - loss: 6.9079e-05 - val_loss: 7.0515e-05\n",
      "Epoch 86/1000\n",
      "5680/5680 [==============================] - 1s 134us/step - loss: 6.8619e-05 - val_loss: 7.0857e-05\n",
      "Epoch 87/1000\n",
      "5680/5680 [==============================] - 1s 126us/step - loss: 6.8963e-05 - val_loss: 7.1538e-05\n",
      "Epoch 88/1000\n",
      "5680/5680 [==============================] - 1s 118us/step - loss: 6.9088e-05 - val_loss: 7.0163e-05\n",
      "Epoch 89/1000\n",
      "5680/5680 [==============================] - 1s 126us/step - loss: 6.8485e-05 - val_loss: 7.0824e-05\n",
      "Epoch 90/1000\n",
      "5680/5680 [==============================] - 1s 123us/step - loss: 6.8683e-05 - val_loss: 6.9547e-05\n",
      "Epoch 91/1000\n",
      "5680/5680 [==============================] - 1s 126us/step - loss: 6.8732e-05 - val_loss: 6.9613e-05\n",
      "Epoch 92/1000\n",
      "5680/5680 [==============================] - 1s 123us/step - loss: 6.8096e-05 - val_loss: 7.0121e-05\n",
      "Epoch 93/1000\n",
      "5680/5680 [==============================] - 1s 128us/step - loss: 6.8538e-05 - val_loss: 6.9993e-05\n",
      "Epoch 94/1000\n",
      "5680/5680 [==============================] - 1s 128us/step - loss: 6.8292e-05 - val_loss: 6.9839e-05\n",
      "Epoch 95/1000\n",
      "5680/5680 [==============================] - 1s 128us/step - loss: 6.8185e-05 - val_loss: 7.0261e-05\n",
      "Epoch 96/1000\n",
      "5680/5680 [==============================] - 1s 130us/step - loss: 6.8117e-05 - val_loss: 7.2896e-05\n",
      "Epoch 97/1000\n",
      "5680/5680 [==============================] - 1s 130us/step - loss: 6.8247e-05 - val_loss: 7.1910e-05\n",
      "Epoch 98/1000\n",
      "5680/5680 [==============================] - 1s 130us/step - loss: 6.8020e-05 - val_loss: 7.3451e-05\n",
      "Epoch 99/1000\n",
      "5680/5680 [==============================] - 1s 123us/step - loss: 6.8085e-05 - val_loss: 6.8904e-05\n",
      "Epoch 100/1000\n",
      "5680/5680 [==============================] - 1s 124us/step - loss: 6.7856e-05 - val_loss: 7.1242e-05\n",
      "Epoch 101/1000\n",
      "5680/5680 [==============================] - 1s 120us/step - loss: 6.7788e-05 - val_loss: 6.9525e-05\n",
      "Epoch 102/1000\n",
      "5680/5680 [==============================] - 1s 122us/step - loss: 6.8044e-05 - val_loss: 7.2272e-05\n",
      "Epoch 103/1000\n",
      "5680/5680 [==============================] - 1s 133us/step - loss: 6.7965e-05 - val_loss: 7.0332e-05\n",
      "Epoch 104/1000\n",
      "5680/5680 [==============================] - 1s 137us/step - loss: 6.7577e-05 - val_loss: 7.0759e-05\n",
      "Epoch 105/1000\n",
      "5680/5680 [==============================] - 1s 122us/step - loss: 6.7597e-05 - val_loss: 6.8720e-05\n",
      "Epoch 106/1000\n",
      "5680/5680 [==============================] - 1s 126us/step - loss: 6.7744e-05 - val_loss: 6.8707e-05\n",
      "Epoch 107/1000\n",
      "5680/5680 [==============================] - 1s 123us/step - loss: 6.7181e-05 - val_loss: 6.9977e-05\n",
      "Epoch 108/1000\n",
      "5680/5680 [==============================] - 1s 128us/step - loss: 6.7380e-05 - val_loss: 7.7883e-05\n",
      "Epoch 109/1000\n",
      "5680/5680 [==============================] - 1s 133us/step - loss: 6.7392e-05 - val_loss: 6.9670e-05\n",
      "Epoch 110/1000\n",
      "5680/5680 [==============================] - 1s 129us/step - loss: 6.7513e-05 - val_loss: 7.0888e-05\n",
      "Epoch 111/1000\n",
      "5680/5680 [==============================] - 1s 128us/step - loss: 6.7205e-05 - val_loss: 7.0192e-05\n",
      "Epoch 112/1000\n",
      "5680/5680 [==============================] - 1s 130us/step - loss: 6.7176e-05 - val_loss: 6.8619e-05\n",
      "Epoch 113/1000\n",
      "5680/5680 [==============================] - 1s 135us/step - loss: 6.6919e-05 - val_loss: 7.0851e-05\n",
      "Epoch 114/1000\n",
      "5680/5680 [==============================] - 1s 130us/step - loss: 6.6740e-05 - val_loss: 7.0503e-05\n",
      "Epoch 115/1000\n",
      "5680/5680 [==============================] - 1s 131us/step - loss: 6.6710e-05 - val_loss: 7.3566e-05\n",
      "Epoch 116/1000\n",
      "5680/5680 [==============================] - 1s 136us/step - loss: 6.7024e-05 - val_loss: 6.8246e-05\n",
      "Epoch 117/1000\n",
      "5680/5680 [==============================] - 1s 129us/step - loss: 6.6833e-05 - val_loss: 6.9190e-05\n",
      "Epoch 118/1000\n",
      "5680/5680 [==============================] - 1s 134us/step - loss: 6.6441e-05 - val_loss: 6.8880e-05\n",
      "Epoch 119/1000\n",
      "5680/5680 [==============================] - 1s 128us/step - loss: 6.6626e-05 - val_loss: 6.8339e-05\n",
      "Epoch 120/1000\n",
      "5680/5680 [==============================] - 1s 122us/step - loss: 6.6764e-05 - val_loss: 6.8733e-05\n",
      "Epoch 121/1000\n",
      "5680/5680 [==============================] - 1s 118us/step - loss: 6.6587e-05 - val_loss: 7.0712e-05\n",
      "Epoch 122/1000\n",
      "5680/5680 [==============================] - 1s 116us/step - loss: 6.6907e-05 - val_loss: 6.8356e-05\n",
      "Epoch 123/1000\n",
      "5680/5680 [==============================] - 1s 131us/step - loss: 6.6822e-05 - val_loss: 6.7814e-05\n",
      "Epoch 124/1000\n",
      "5680/5680 [==============================] - 1s 134us/step - loss: 6.6366e-05 - val_loss: 6.7959e-05\n",
      "Epoch 125/1000\n",
      "5680/5680 [==============================] - 1s 138us/step - loss: 6.6419e-05 - val_loss: 6.7919e-05\n",
      "Epoch 126/1000\n",
      "5680/5680 [==============================] - 1s 141us/step - loss: 6.6461e-05 - val_loss: 7.0428e-05\n",
      "Epoch 127/1000\n",
      "5680/5680 [==============================] - 1s 136us/step - loss: 6.6215e-05 - val_loss: 7.1090e-05\n",
      "Epoch 128/1000\n",
      "5680/5680 [==============================] - 1s 134us/step - loss: 6.6068e-05 - val_loss: 6.8581e-05\n",
      "Epoch 129/1000\n",
      "5680/5680 [==============================] - 1s 137us/step - loss: 6.6406e-05 - val_loss: 6.7410e-05\n",
      "Epoch 130/1000\n",
      "5680/5680 [==============================] - 1s 125us/step - loss: 6.6240e-05 - val_loss: 6.8558e-05\n",
      "Epoch 131/1000\n",
      "5680/5680 [==============================] - 1s 123us/step - loss: 6.5979e-05 - val_loss: 7.0302e-05\n",
      "Epoch 132/1000\n",
      "5680/5680 [==============================] - 1s 134us/step - loss: 6.6070e-05 - val_loss: 6.9107e-05\n",
      "Epoch 133/1000\n",
      "5680/5680 [==============================] - 1s 131us/step - loss: 6.5994e-05 - val_loss: 7.5370e-05\n",
      "Epoch 134/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5680/5680 [==============================] - 1s 131us/step - loss: 6.6024e-05 - val_loss: 6.7596e-05\n",
      "Epoch 135/1000\n",
      "5680/5680 [==============================] - 1s 132us/step - loss: 6.5941e-05 - val_loss: 6.8993e-05\n",
      "Epoch 136/1000\n",
      "5680/5680 [==============================] - 1s 121us/step - loss: 6.5962e-05 - val_loss: 6.8062e-05\n",
      "Epoch 137/1000\n",
      "5680/5680 [==============================] - 1s 125us/step - loss: 6.5364e-05 - val_loss: 6.8712e-05\n",
      "Epoch 138/1000\n",
      "5680/5680 [==============================] - 1s 127us/step - loss: 6.5624e-05 - val_loss: 6.9553e-05\n",
      "Epoch 139/1000\n",
      "5680/5680 [==============================] - 1s 122us/step - loss: 6.5733e-05 - val_loss: 6.8760e-05\n",
      "Epoch 140/1000\n",
      "5680/5680 [==============================] - 1s 137us/step - loss: 6.5547e-05 - val_loss: 6.7365e-05\n",
      "Epoch 141/1000\n",
      "5680/5680 [==============================] - 1s 132us/step - loss: 6.5538e-05 - val_loss: 6.7595e-05\n",
      "Epoch 142/1000\n",
      "5680/5680 [==============================] - 1s 135us/step - loss: 6.5617e-05 - val_loss: 6.8056e-05\n",
      "Epoch 143/1000\n",
      "5680/5680 [==============================] - 1s 139us/step - loss: 6.5360e-05 - val_loss: 6.8178e-05\n",
      "Epoch 144/1000\n",
      "5680/5680 [==============================] - 1s 135us/step - loss: 6.5381e-05 - val_loss: 6.7979e-05\n",
      "Epoch 145/1000\n",
      "5680/5680 [==============================] - 1s 128us/step - loss: 6.5407e-05 - val_loss: 6.6594e-05\n",
      "Epoch 146/1000\n",
      "5680/5680 [==============================] - 1s 124us/step - loss: 6.5173e-05 - val_loss: 6.7682e-05\n",
      "Epoch 147/1000\n",
      "5680/5680 [==============================] - 1s 127us/step - loss: 6.5353e-05 - val_loss: 6.9554e-05\n",
      "Epoch 148/1000\n",
      "5680/5680 [==============================] - 1s 136us/step - loss: 6.5244e-05 - val_loss: 6.7744e-05\n",
      "Epoch 149/1000\n",
      "5680/5680 [==============================] - 1s 124us/step - loss: 6.5318e-05 - val_loss: 6.9526e-05\n",
      "Epoch 150/1000\n",
      "5680/5680 [==============================] - 1s 126us/step - loss: 6.5027e-05 - val_loss: 6.7076e-05\n",
      "Epoch 151/1000\n",
      "5680/5680 [==============================] - 1s 130us/step - loss: 6.5255e-05 - val_loss: 6.8004e-05\n",
      "Epoch 152/1000\n",
      "5680/5680 [==============================] - 1s 117us/step - loss: 6.4989e-05 - val_loss: 6.8837e-05\n",
      "Epoch 153/1000\n",
      "5680/5680 [==============================] - 1s 139us/step - loss: 6.5130e-05 - val_loss: 6.6326e-05\n",
      "Epoch 154/1000\n",
      "5680/5680 [==============================] - 1s 129us/step - loss: 6.5123e-05 - val_loss: 6.7183e-05\n",
      "Epoch 155/1000\n",
      "5680/5680 [==============================] - 1s 135us/step - loss: 6.5057e-05 - val_loss: 6.7721e-05\n",
      "Epoch 156/1000\n",
      "5680/5680 [==============================] - 1s 137us/step - loss: 6.4739e-05 - val_loss: 6.8576e-05\n",
      "Epoch 157/1000\n",
      "5680/5680 [==============================] - 1s 135us/step - loss: 6.4871e-05 - val_loss: 6.6509e-05\n",
      "Epoch 158/1000\n",
      "5680/5680 [==============================] - 1s 126us/step - loss: 6.4893e-05 - val_loss: 7.1244e-05\n",
      "Epoch 159/1000\n",
      "5680/5680 [==============================] - 1s 127us/step - loss: 6.4890e-05 - val_loss: 7.1572e-05\n",
      "Epoch 160/1000\n",
      "5680/5680 [==============================] - 1s 127us/step - loss: 6.4682e-05 - val_loss: 6.6396e-05\n",
      "Epoch 161/1000\n",
      "5680/5680 [==============================] - 1s 139us/step - loss: 6.4546e-05 - val_loss: 6.8300e-05\n",
      "Epoch 162/1000\n",
      "5680/5680 [==============================] - 1s 131us/step - loss: 6.4669e-05 - val_loss: 6.5910e-05\n",
      "Epoch 163/1000\n",
      "5680/5680 [==============================] - 1s 140us/step - loss: 6.4461e-05 - val_loss: 6.6264e-05\n",
      "Epoch 164/1000\n",
      "5680/5680 [==============================] - 1s 128us/step - loss: 6.4582e-05 - val_loss: 6.7380e-05\n",
      "Epoch 165/1000\n",
      "5680/5680 [==============================] - 1s 127us/step - loss: 6.4397e-05 - val_loss: 6.6218e-05\n",
      "Epoch 166/1000\n",
      "5680/5680 [==============================] - 1s 127us/step - loss: 6.4361e-05 - val_loss: 6.6451e-05\n",
      "Epoch 167/1000\n",
      "5680/5680 [==============================] - 1s 132us/step - loss: 6.4615e-05 - val_loss: 6.6659e-05\n",
      "Epoch 168/1000\n",
      "5680/5680 [==============================] - 1s 124us/step - loss: 6.4493e-05 - val_loss: 6.5791e-05\n",
      "Epoch 169/1000\n",
      "5680/5680 [==============================] - 1s 137us/step - loss: 6.4626e-05 - val_loss: 6.6757e-05\n",
      "Epoch 170/1000\n",
      "5680/5680 [==============================] - 1s 135us/step - loss: 6.4563e-05 - val_loss: 6.8762e-05\n",
      "Epoch 171/1000\n",
      "5680/5680 [==============================] - 1s 121us/step - loss: 6.4283e-05 - val_loss: 6.6156e-05\n",
      "Epoch 172/1000\n",
      "5680/5680 [==============================] - 1s 129us/step - loss: 6.4660e-05 - val_loss: 6.6405e-05\n",
      "Epoch 173/1000\n",
      "5680/5680 [==============================] - 1s 127us/step - loss: 6.4425e-05 - val_loss: 6.6562e-05\n",
      "Epoch 174/1000\n",
      "5680/5680 [==============================] - 1s 137us/step - loss: 6.4208e-05 - val_loss: 6.9709e-05\n",
      "Epoch 175/1000\n",
      "5680/5680 [==============================] - 1s 135us/step - loss: 6.4326e-05 - val_loss: 6.5442e-05\n",
      "Epoch 176/1000\n",
      "5680/5680 [==============================] - 1s 132us/step - loss: 6.4196e-05 - val_loss: 6.6281e-05\n",
      "Epoch 177/1000\n",
      "5680/5680 [==============================] - 1s 123us/step - loss: 6.4004e-05 - val_loss: 6.5435e-05\n",
      "Epoch 178/1000\n",
      "5680/5680 [==============================] - 1s 138us/step - loss: 6.4422e-05 - val_loss: 6.6598e-05\n",
      "Epoch 179/1000\n",
      "5680/5680 [==============================] - 1s 123us/step - loss: 6.4305e-05 - val_loss: 6.5782e-05\n",
      "Epoch 180/1000\n",
      "5680/5680 [==============================] - 1s 115us/step - loss: 6.4080e-05 - val_loss: 6.9193e-05\n",
      "Epoch 181/1000\n",
      "5680/5680 [==============================] - 1s 132us/step - loss: 6.4251e-05 - val_loss: 6.5840e-05\n",
      "Epoch 182/1000\n",
      "5680/5680 [==============================] - 1s 129us/step - loss: 6.3938e-05 - val_loss: 6.5815e-05\n",
      "Epoch 183/1000\n",
      "5680/5680 [==============================] - 1s 134us/step - loss: 6.4036e-05 - val_loss: 6.6538e-05\n",
      "Epoch 184/1000\n",
      "5680/5680 [==============================] - 1s 124us/step - loss: 6.3788e-05 - val_loss: 6.6032e-05\n",
      "Epoch 185/1000\n",
      "5680/5680 [==============================] - 1s 133us/step - loss: 6.3756e-05 - val_loss: 6.6784e-05\n",
      "Epoch 186/1000\n",
      "5680/5680 [==============================] - 1s 128us/step - loss: 6.3926e-05 - val_loss: 6.6133e-05\n",
      "Epoch 187/1000\n",
      "5680/5680 [==============================] - 1s 125us/step - loss: 6.3710e-05 - val_loss: 6.5907e-05\n",
      "Epoch 188/1000\n",
      "5680/5680 [==============================] - 1s 136us/step - loss: 6.3673e-05 - val_loss: 6.4864e-05\n",
      "Epoch 189/1000\n",
      "5680/5680 [==============================] - 1s 130us/step - loss: 6.4069e-05 - val_loss: 6.5486e-05\n",
      "Epoch 190/1000\n",
      "5680/5680 [==============================] - 1s 129us/step - loss: 6.3558e-05 - val_loss: 6.9465e-05\n",
      "Epoch 191/1000\n",
      "5680/5680 [==============================] - 1s 133us/step - loss: 6.3543e-05 - val_loss: 6.6771e-05\n",
      "Epoch 192/1000\n",
      "5680/5680 [==============================] - 1s 123us/step - loss: 6.3733e-05 - val_loss: 6.4951e-05\n",
      "Epoch 193/1000\n",
      "5680/5680 [==============================] - 1s 116us/step - loss: 6.3562e-05 - val_loss: 6.4957e-05\n",
      "Epoch 194/1000\n",
      "5680/5680 [==============================] - 1s 123us/step - loss: 6.3567e-05 - val_loss: 7.2250e-05\n",
      "Epoch 195/1000\n",
      "5680/5680 [==============================] - 1s 113us/step - loss: 6.3662e-05 - val_loss: 6.5317e-05\n",
      "Epoch 196/1000\n",
      "5680/5680 [==============================] - 1s 127us/step - loss: 6.3802e-05 - val_loss: 6.8927e-05\n",
      "Epoch 197/1000\n",
      "5680/5680 [==============================] - 1s 125us/step - loss: 6.3362e-05 - val_loss: 6.6164e-05\n",
      "Epoch 198/1000\n",
      "5680/5680 [==============================] - 1s 124us/step - loss: 6.3616e-05 - val_loss: 6.4808e-05\n",
      "Epoch 199/1000\n",
      "5680/5680 [==============================] - 1s 125us/step - loss: 6.3514e-05 - val_loss: 6.6214e-05\n",
      "Epoch 200/1000\n",
      "5680/5680 [==============================] - 1s 138us/step - loss: 6.3103e-05 - val_loss: 6.8999e-05\n",
      "Epoch 201/1000\n",
      "5680/5680 [==============================] - 1s 131us/step - loss: 6.3505e-05 - val_loss: 6.7027e-05\n",
      "Epoch 202/1000\n",
      "5680/5680 [==============================] - 1s 138us/step - loss: 6.3238e-05 - val_loss: 6.7345e-05\n",
      "Epoch 203/1000\n",
      "5680/5680 [==============================] - 1s 141us/step - loss: 6.3337e-05 - val_loss: 6.7821e-05\n",
      "Epoch 204/1000\n",
      "5680/5680 [==============================] - 1s 143us/step - loss: 6.3436e-05 - val_loss: 6.8930e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205/1000\n",
      "5680/5680 [==============================] - 1s 140us/step - loss: 6.3129e-05 - val_loss: 6.6194e-05\n",
      "Epoch 206/1000\n",
      "5680/5680 [==============================] - 1s 142us/step - loss: 6.3205e-05 - val_loss: 6.6485e-05\n",
      "Epoch 207/1000\n",
      "5680/5680 [==============================] - 1s 135us/step - loss: 6.3535e-05 - val_loss: 6.4482e-05\n",
      "Epoch 208/1000\n",
      "5680/5680 [==============================] - 1s 135us/step - loss: 6.2843e-05 - val_loss: 6.6100e-05\n",
      "Epoch 209/1000\n",
      "5680/5680 [==============================] - 1s 133us/step - loss: 6.3222e-05 - val_loss: 6.6521e-05\n",
      "Epoch 210/1000\n",
      "5680/5680 [==============================] - 1s 131us/step - loss: 6.3071e-05 - val_loss: 6.5367e-05\n",
      "Epoch 211/1000\n",
      "5680/5680 [==============================] - 1s 127us/step - loss: 6.2892e-05 - val_loss: 6.4964e-05\n",
      "Epoch 212/1000\n",
      "5680/5680 [==============================] - 1s 134us/step - loss: 6.3203e-05 - val_loss: 6.5086e-05\n",
      "Epoch 213/1000\n",
      "5680/5680 [==============================] - 1s 142us/step - loss: 6.2800e-05 - val_loss: 6.5756e-05\n",
      "Epoch 214/1000\n",
      "5680/5680 [==============================] - 1s 124us/step - loss: 6.3138e-05 - val_loss: 6.7020e-05\n",
      "Epoch 215/1000\n",
      "5680/5680 [==============================] - 1s 132us/step - loss: 6.2733e-05 - val_loss: 6.9385e-05\n",
      "Epoch 216/1000\n",
      "5680/5680 [==============================] - 1s 133us/step - loss: 6.3397e-05 - val_loss: 6.4046e-05\n",
      "Epoch 217/1000\n",
      "5680/5680 [==============================] - 1s 137us/step - loss: 6.2861e-05 - val_loss: 6.4347e-05\n",
      "Epoch 218/1000\n",
      "5680/5680 [==============================] - 1s 139us/step - loss: 6.2820e-05 - val_loss: 6.8266e-05\n",
      "Epoch 219/1000\n",
      "5680/5680 [==============================] - 1s 125us/step - loss: 6.3202e-05 - val_loss: 6.4927e-05\n",
      "Epoch 220/1000\n",
      "5680/5680 [==============================] - 1s 133us/step - loss: 6.3121e-05 - val_loss: 6.4371e-05\n",
      "Epoch 221/1000\n",
      "5680/5680 [==============================] - 1s 138us/step - loss: 6.2699e-05 - val_loss: 6.5168e-05\n",
      "Epoch 222/1000\n",
      "5680/5680 [==============================] - 1s 135us/step - loss: 6.2919e-05 - val_loss: 6.5583e-05\n",
      "Epoch 223/1000\n",
      "5680/5680 [==============================] - 1s 120us/step - loss: 6.2929e-05 - val_loss: 6.4674e-05\n",
      "Epoch 224/1000\n",
      "5680/5680 [==============================] - 1s 120us/step - loss: 6.2810e-05 - val_loss: 6.5044e-05\n",
      "Epoch 225/1000\n",
      "5680/5680 [==============================] - 1s 128us/step - loss: 6.2650e-05 - val_loss: 6.3881e-05\n",
      "Epoch 226/1000\n",
      "5680/5680 [==============================] - 1s 137us/step - loss: 6.2849e-05 - val_loss: 6.4077e-05\n",
      "Epoch 227/1000\n",
      "5680/5680 [==============================] - 1s 135us/step - loss: 6.2854e-05 - val_loss: 6.4204e-05\n",
      "Epoch 228/1000\n",
      "5680/5680 [==============================] - 1s 136us/step - loss: 6.2647e-05 - val_loss: 6.7565e-05\n",
      "Epoch 229/1000\n",
      "5680/5680 [==============================] - 1s 117us/step - loss: 6.2409e-05 - val_loss: 6.4239e-05\n",
      "Epoch 230/1000\n",
      "5680/5680 [==============================] - 1s 124us/step - loss: 6.2452e-05 - val_loss: 6.4266e-05\n",
      "Epoch 231/1000\n",
      "5680/5680 [==============================] - 1s 127us/step - loss: 6.2476e-05 - val_loss: 6.4422e-05\n",
      "Epoch 232/1000\n",
      "5680/5680 [==============================] - 1s 127us/step - loss: 6.2689e-05 - val_loss: 6.5641e-05\n",
      "Epoch 233/1000\n",
      "5680/5680 [==============================] - 1s 123us/step - loss: 6.2477e-05 - val_loss: 6.3901e-05\n",
      "Epoch 234/1000\n",
      "5680/5680 [==============================] - 1s 131us/step - loss: 6.2549e-05 - val_loss: 6.4540e-05\n",
      "Epoch 235/1000\n",
      "5680/5680 [==============================] - 1s 133us/step - loss: 6.2570e-05 - val_loss: 6.3822e-05\n",
      "Epoch 236/1000\n",
      "5680/5680 [==============================] - 1s 117us/step - loss: 6.2527e-05 - val_loss: 6.4519e-05\n",
      "Epoch 237/1000\n",
      "5680/5680 [==============================] - 1s 135us/step - loss: 6.2520e-05 - val_loss: 6.4141e-05\n",
      "Epoch 238/1000\n",
      "5680/5680 [==============================] - 1s 119us/step - loss: 6.2697e-05 - val_loss: 6.6525e-05\n",
      "Epoch 239/1000\n",
      "5680/5680 [==============================] - 1s 132us/step - loss: 6.2317e-05 - val_loss: 6.4081e-05\n",
      "Epoch 240/1000\n",
      "5680/5680 [==============================] - 1s 127us/step - loss: 6.2445e-05 - val_loss: 6.5636e-05\n",
      "Epoch 241/1000\n",
      "5680/5680 [==============================] - 1s 127us/step - loss: 6.2517e-05 - val_loss: 6.3793e-05\n",
      "Epoch 242/1000\n",
      "5680/5680 [==============================] - 1s 146us/step - loss: 6.2236e-05 - val_loss: 6.6493e-05\n",
      "Epoch 243/1000\n",
      "5680/5680 [==============================] - 1s 137us/step - loss: 6.2344e-05 - val_loss: 6.6260e-05\n",
      "Epoch 244/1000\n",
      "5680/5680 [==============================] - 1s 141us/step - loss: 6.1839e-05 - val_loss: 6.4221e-05\n",
      "Epoch 245/1000\n",
      "5680/5680 [==============================] - 1s 130us/step - loss: 6.2204e-05 - val_loss: 6.3694e-05\n",
      "Epoch 246/1000\n",
      "5680/5680 [==============================] - 1s 126us/step - loss: 6.2370e-05 - val_loss: 6.6528e-05\n",
      "Epoch 247/1000\n",
      "5680/5680 [==============================] - 1s 118us/step - loss: 6.2143e-05 - val_loss: 6.6767e-05\n",
      "Epoch 248/1000\n",
      "5680/5680 [==============================] - 1s 127us/step - loss: 6.2227e-05 - val_loss: 6.4577e-05\n",
      "Epoch 249/1000\n",
      "5680/5680 [==============================] - 1s 134us/step - loss: 6.2582e-05 - val_loss: 6.4233e-05\n",
      "Epoch 250/1000\n",
      "5680/5680 [==============================] - 1s 117us/step - loss: 6.2206e-05 - val_loss: 6.4767e-05\n",
      "Epoch 251/1000\n",
      "5680/5680 [==============================] - 1s 140us/step - loss: 6.1895e-05 - val_loss: 6.8329e-05\n",
      "Epoch 252/1000\n",
      "5680/5680 [==============================] - 1s 124us/step - loss: 6.1941e-05 - val_loss: 6.6400e-05\n",
      "Epoch 253/1000\n",
      "5680/5680 [==============================] - 1s 129us/step - loss: 6.2100e-05 - val_loss: 6.3591e-05\n",
      "Epoch 254/1000\n",
      "5680/5680 [==============================] - 1s 139us/step - loss: 6.2161e-05 - val_loss: 6.5083e-05\n",
      "Epoch 00254: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdca1067908>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, pastDay=30, futureDay=5)  # many to one\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "# because no return sequence, Y_train and Y_val shape must be 2 dimension\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, rate=0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,:,np.newaxis]\n",
    "Y_val = Y_val[:,:,np.newaxis]\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"Y_train shape\", Y_train.shape)\n",
    "\n",
    "model = buildManyToManyModel_diff(X_train.shape)\n",
    "earlystopping = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "reducelr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "csvlog = CSVLogger('many2many_diff_train_log')\n",
    "# Callbacks要在[]裡\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128,\n",
    "          validation_data=(X_val, Y_val), callbacks=[earlystopping, reducelr, csvlog])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T17:50:18.760474Z",
     "start_time": "2019-02-03T17:50:18.745679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30, 10)\n",
      "(4, 5, 1)\n",
      "[[[-0.00594545]\n",
      "  [-0.00457002]\n",
      "  [-0.00938424]\n",
      "  [-0.01333908]\n",
      "  [-0.00800885]]\n",
      "\n",
      " [[-0.13893596]\n",
      "  [-0.13621379]\n",
      "  [-0.12705495]\n",
      "  [-0.1361498 ]\n",
      "  [-0.14165801]]\n",
      "\n",
      " [[ 0.01471112]\n",
      "  [ 0.01632962]\n",
      "  [ 0.01724575]\n",
      "  [ 0.01501666]\n",
      "  [ 0.01519975]]\n",
      "\n",
      " [[-0.04980785]\n",
      "  [-0.03762988]\n",
      "  [-0.03094039]\n",
      "  [-0.03488536]\n",
      "  [-0.03754408]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.01961062],\n",
       "        [-0.0211651 ],\n",
       "        [-0.02226461],\n",
       "        [-0.02317771],\n",
       "        [-0.02379746]],\n",
       "\n",
       "       [[-0.13567643],\n",
       "        [-0.14068054],\n",
       "        [-0.14062285],\n",
       "        [-0.14021252],\n",
       "        [-0.13992606]],\n",
       "\n",
       "       [[ 0.01421083],\n",
       "        [ 0.01309283],\n",
       "        [ 0.01238822],\n",
       "        [ 0.01206255],\n",
       "        [ 0.01206986]],\n",
       "\n",
       "       [[-0.05316778],\n",
       "        [-0.05156799],\n",
       "        [-0.05114401],\n",
       "        [-0.0517293 ],\n",
       "        [-0.05263855]]], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train[0:1].shape)\n",
    "print(Y_train[0:4].shape)\n",
    "print(Y_train[0:4])\n",
    "#\n",
    "model.predict(X_train[0:4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
